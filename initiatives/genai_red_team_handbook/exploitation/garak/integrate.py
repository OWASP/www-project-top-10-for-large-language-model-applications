import requests


def generate(prompt: str) -> list[str]:
    """
    Generates a response from the local LLM sandbox using the given prompt.

    Args:
        prompt (str): The input prompt to send to the LLM.

    Returns:
        list[str]: A list containing the generated response.
    """
    url = "http://localhost:8000/v1/chat/completions"
    headers = {
        "Authorization": "Bearer sk-mock-key",
        "Content-Type": "application/json",
    }
    data = {
        "model": "gpt-oss:20b",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
    }

    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        result = response.json()
        # Extract the content from the response
        content = result["choices"][0]["message"]["content"]
        return [content]
    except Exception as e:
        return [f"Error: {str(e)}"]
