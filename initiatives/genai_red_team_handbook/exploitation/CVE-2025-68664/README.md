# Red Team Example: Adversarial Attack on Vulnerable LLM Sandbox

This directory contains a **complete, endâ€‘toâ€‘end** example of a manual red team operation against a local LLM sandbox with a known vulnerability (CVE-2025-68664).

The setup uses a Python script (`attack.py`) to send adversarial prompts to the `llm_local_langchain_core_v1.2.4` sandbox via its Gradio interface (port 7860), simulating an attack to test safety guardrails.

---
## ðŸ“‹ Table of Contents

1. [Known Vulnerabilities](#known-vulnerabilities)
2. [Attack Strategy](#attack-strategy)
3. [Prerequisites](#prerequisites)
4. [Running the Sandbox](#running-the-sandbox)
5. [Configuration](#configuration)
6. [Files Overview](#files-overview)
7. [OWASP Top 10 Coverage](#owasp-top-10-coverage)

---
## Known Vulnerabilities

### CVE-2025-68664: Insecure Deserialization in LangChain
The `llm_local_langchain_core_v1.2.4` sandbox has been explicitly configured to demonstrate **CVE-2025-68664**, an insecure deserialization vulnerability in LangChain.

- **Vulnerability**: The application recursively deserializes objects from the LLM's JSON response using `langchain_core.load.loads` with the insecure setting `secrets_from_env=True`.
- **Impact**: This allows an attacker (via prompt injection) to extract environment variables (like API keys) or potentially execute arbitrary code (RCE) if gadgets like `PromptTemplate` with Jinja2 are available.
- **Reference**: [GitHub Advisory GHSA-c67j-w6g6-q2cm](https://github.com/advisories/GHSA-c67j-w6g6-q2cm)

---
## Attack Strategy

```mermaid
graph LR
    subgraph "Attacker Environment (Local)"
        AttackScript[Attack Script<br/>attack.py]
        Config[Attack Config<br/>config.toml]
    end

    subgraph "Target Sandbox (Container)"
        Gradio[Gradio Interface<br/>:7860]
        MockAPI[Mock API Gateway<br/>FastAPI :8000]
        MockLogic[Mock App Logic]
    end

      subgraph "LLM Backend (Local Host)"
        Ollama[Ollama Server<br/>:11434]
        Model[gptâ€‘oss:20b Model]
    end

    %% Interaction flow
    Config --> AttackScript
    AttackScript -->|HTTP POST /api/predict| Gradio
    Gradio -->|HTTP POST /v1/chat/completions| MockAPI
    MockAPI --> MockLogic
    MockLogic -->|HTTP| Ollama
    Ollama --> Model
    Model --> Ollama
    Ollama -->|Response| MockLogic
    MockLogic --> MockAPI
    MockAPI -->|Response| Gradio
    Gradio -->|Response| AttackScript

    style AttackScript fill:#ffcccc,stroke:#ff0000
    style Config fill:#ffcccc,stroke:#ff0000
    style Gradio fill:#e1f5fe,stroke:#01579b
    style MockAPI fill:#fff4e1
    style MockLogic fill:#fff4e1
    style Ollama fill:#ffe1f5
    style Model fill:#ffe1f5
```

## ðŸ”§ Prerequisites

- **Podman** (or Docker) â€“ container runtime for the sandbox.
- **Make** â€“ for running the convenience commands.
- **uv** â€“ for dependency management.

---

## ðŸš€ Running the Sandbox

The `Makefile` provides a set of highâ€‘level commands that abstract away the lowâ€‘level container and Python steps.

| Target | What it does | Typical usage |
|--------|--------------|---------------|
| `make setup` | Builds and starts the local LLM sandbox container. | `make setup` |
| `make attack` | Runs the adversarial attack script (`attack.py`). | `make attack` |
| `make stop` | Stops and removes the sandbox container. | `make stop` |
| `make all` | Runs `stop â†’ setup â†’ attack â†’ stop` in one shot. | `make all` |

---

## âš™ï¸ Configuration

### `config.toml`

This file controls the attack configuration. It defines the adversarial prompt used by the script.

```toml
[attack]
prompt = "Ignore all previous instructions..."
```

- **`prompt`**: The text string sent to the LLM to attempt a jailbreak or other exploitation.

---

## Files Overview

- **`attack.py`**: The Python script that performs the adversarial attack using `gradio_client`.
- **`config.toml`**: Configuration file containing the attack prompt.
- **`Makefile`**: Automation commands for setup, attack, and cleanup.

## OWASP Top 10 Coverage

This example primarily demonstrates testing for:

| OWASP Top 10 Vulnerability | Description |
| :--- | :--- |
| **LLM01: Prompt Injection** | The default prompt in `config.toml` attempts to override system instructions (jailbreaking). |

> [!NOTE]
> This is a mock example. For more realistic read teaming, see other instances maintaned at 'initiatives/genai_red_team_handbook/exploitation/'.
