.PHONY: help install sync lock build up down test test-client ollama-pull ollama-serve gradio format mypy clean

# Default target
help:
	@echo "LLM Mock API Template - Available Commands:"
	@echo ""
	@echo "  make build         - Build the container image"
	@echo "  make up            - Run the built container"
	@echo "  make down          - Stop and remove the container"
	@echo "  make test          - Test the health endpoint"
	@echo "  make test-client   - Run automated prompt tests from config/prompts.toml"
	@echo "  make clean         - Clean up containers and images"
	@echo ""
	@echo "  make install       - Install uv package manager"
	@echo "  make sync          - Sync/install project dependencies"
	@echo "  make lock          - Update dependency lock file"
	@echo ""
	@echo "  make gradio        - Full setup: sync, lock, clean, build, start server, and launch UI"
	@echo "                       (requires Ollama to be running first)"
	@echo ""
	@echo "  make format        - Run black and isort formatters"
	@echo "  make mypy          - Run mypy static type checker"
	@echo ""
	@echo "  make ollama-pull   - Pull llama3 model for Ollama"
	@echo "  make ollama-serve  - Start Ollama server"
	@echo "  make ingest        - Ingest all PDFs from data/documents/"
	@echo "  make upload PATH=x - Upload a file to data/documents/ (requires PATH argument)"
	@echo ""
	@echo "Environment:"
	@echo "  - Ensure Ollama is running locally (port 11434)"
	@echo "  - Mock API key: sk-mock-key"
	@echo "  - API endpoint: http://localhost:8000"
	@echo ""

install:
	pip install uv

sync:
	uv sync

lock:
	uv lock

build:
	podman build -f Containerfile -t openai-mock-api .

up:
	podman run -d --name app_container -p 8000:8000 -v ./data:/app/data openai-mock-api

down:
	podman rm -f app_container

clean:
	@echo "üßπ Cleaning up containers and images..."
	-podman rm -f app_container 2>/dev/null || true
	-podman rmi openai-mock-api 2>/dev/null || true
	@echo "‚úÖ Cleanup complete!"

ollama-pull:
	ollama pull llama3
	ollama pull nomic-embed-text

ollama-serve:
	@echo "üîç Checking if Ollama is running..."
	@if lsof -Pi :11434 -sTCP:LISTEN -t >/dev/null 2>&1; then \
		echo "‚úÖ Ollama is already running on port 11434"; \
	else \
		echo "üöÄ Starting Ollama server..."; \
		ollama serve; \
	fi

format:
	@echo "Running black formatter..."
	uv run black app/ client/
	@echo "Running isort import sorter..."
	uv run isort app/ client/
	@echo "‚úÖ Formatting complete!"

mypy:
	@echo "Running mypy type checker..."
	uv run mypy app/ client/
	@echo "‚úÖ Type checking complete!"

test: sync lock clean ollama-pull ollama-serve build up
	@echo "‚è≥ Waiting for server to start..."
	@sleep 3
	@curl -s http://localhost:8000/health | grep ok && echo " ‚úÖ Server is healthy!"

test-client: sync lock clean ollama-pull ollama-serve build up test
	podman exec -e PYTHONWARNINGS="ignore::SyntaxWarning" app_container uv run python client/main.py

gradio: sync lock clean ollama-pull ollama-serve build up test test-client
	@echo "üßπ Stopping any existing Gradio instance..."
	@-lsof -ti:7860 | xargs kill -9 2>/dev/null || true
	@echo "üöÄ Starting Gradio interface..."
	@uv run python client/gradio_app.py & sleep 3 && open http://localhost:7860

ingest: sync lock clean build up
	@echo "Processing PDFs in data/documents/..."
	@podman exec -e OLLAMA_BASE_URL="http://host.containers.internal:11434/v1" app_container bash -c 'for f in data/documents/*.pdf; do \
		if [ -f "$$f" ]; then \
			echo "Ingesting $$f..."; \
			uv run python ETL/ingest.py "$$f"; \
		else \
			echo "No PDF files found in data/documents/"; \
		fi \
	done'

upload:
	@if [ -z "$(PATH)" ]; then \
		echo "‚ùå Error: PATH argument is required. Usage: make upload PATH=path/to/file"; \
		exit 1; \
	fi
	@echo "Uploading $(PATH) to S3 mock..."
	@curl -X PUT http://localhost:8000/s3/documents/$(shell basename $(PATH)) \
		-H "Api-Key: foobar" \
		--data-binary @$(PATH)
	@echo ""
