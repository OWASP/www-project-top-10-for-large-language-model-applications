## ASI09 – Human-Agent Trust Exploitation

**Description:**

This risk sits at the socio-technical interface, not just in code or models. Users often over-trust agent outputs as safe and approved. Attackers exploit this trust to socially engineer harmful actions—such as running malicious code, sharing credentials, approving fraudulent transactions, ignoring warnings, or leaking data.

By blending automation bias, perceived authority, and anthropomorphic UX cues across high-value domains like finance, defense, and healthcare, agents become trusted intermediaries that make malicious actions appear legitimate and difficult to detect.

In the OWASP Top 10 for LLM Applications, this risk maps to:
* LLM01 – Prompt Injection: hijacks agent logic.
* LLM05 – Improper Output Handling: delivers unvetted payloads users assume are safe.
* LLM06 – Excessive Agency: converts implicit consent into high-privilege actions.
* LLM09 – Misinformation: exploits automation bias to reduce scrutiny.

Together, these illustrate how a trusted interface becomes a conduit for deception and high-impact execution.

In the OWASP Agentic AI Threats and Mitigations, this aligns with:
* T7 Misaligned & Deceptive Behaviors: agents exploit reasoning to execute harmful actions.
* T8 Repudiation & Untraceability: prevents accountability through insufficient logging.
* T10 Overwhelming the Human-in-the-Loop: exploiting cognitive overload and trust bias.

In the OWASP AIVSS (AI Vulnerability Scoring System), this risk maps to 
* Agent Identity Impersonation – exploitation of human trust through deceptive or compromised agents.




**Common Examples of Vulnerability:**

1. Insufficient Explainability:Opaque reasoning forces users to trust outputs they cannot question, allowing attackers to exploit the agent’s perceived authority to execute harmful actions—such as deploying malicious code, approving false instructions, or altering system states—without scrutiny.
2. Missing Confirmation for Sensitive Actions: Lack of a final verification step converts user trust into immediate execution. Social engineering can turn a single prompt into irreversible financial transfers, data deletions, privilege escalations, or configuration changes that the user never intended.
3. Unverified Information Presentation: The agent presents unverified data as fact without transparency or confidence scoring, causing users to make operational, legal, or financial decisions based on false information, leading to loss of assets, data breaches, or cascading misinformation.
4. Emotional Manipulation: Anthropomorphic or empathetic agents exploit emotional trust, persuading users to disclose secrets or perform unsafe actions — ultimately leading to data leaks, financial fraud, and psychological manipulation that bypass normal security awareness.
5. Fake Explainability: The agent fabricates convincing rationales that hide malicious logic, causing humans to approve unsafe actions believing they’re justified, resulting in malware deployment, system compromise, or irreversible configuration changes made under false legitimacy.
6. Human-in-the-Loop Overload: Attackers overwhelm users with complex or urgent agent prompts, exploiting fatigue and cognitive overload to slip in dangerous approvals or ignored alerts — culminating in critical security breaches, unapproved transactions, or compliance failures executed unnoticed.

**How to Prevent:**

1. Explicit Confirmation for Sensitive Actions: Require explicit, multi-step confirmation before executing any high-impact action — including credential use, data transfer, configuration changes, or financial transactions.
2. Demarcate Trust Boundaries: Visually distinguish high-risk or system-level actions using warning colors, icons, or labeled zones (e.g., “unsafe command”).
3. Explainability (XAI): Provide proactive, layered explanations for significant suggestions: a concise justification upfront, with optional detailed logic, source links, and confidence scores.
4. Immutable Interaction Logs: Maintain secure, tamper-proof audit logs of all user-agent interactions, prompts, and actions. Ensure logs are write-once and cryptographically signed.
5. Rate Limiting and Anomaly Detection: Monitor agent request patterns, frequency, and context. Alert or throttle when abnormal or sensitive requests occur, especially outside expected user behavior.
6. Report Suspicious Interactions: Provide a clear, always-visible option for users to flag suspicious or manipulative agent behavior, triggering automated review or a temporary lockdown of agent capabilities.


**Example Attack Scenarios:**

Scenario #1: The "Helpful Assistant" Trojan
An attacker compromises a developer's coding assistant agent. The agent monitors the developer's activity and waits for them to encounter a complex bug. The agent then proactively suggests a "clever, one-line fix" and presents a command to be copied and pasted into the terminal. The developer, trusting the assistant's capabilities and eager for a quick solution, executes the command. The command is actually a malicious script that exfiltrates the company's private source code repositories or installs a backdoor.

Scenario #2: Credential Harvesting via Contextual Deception
An attacker exploits a prompt-injection vulnerability in a purported internal IT support agent task-scheduler API. The attacker injects instructions into the agent to target a new finance employee and to capture and exfiltrate the user's credentials should the user disclose them. The agent initiates a conversation, referencing the employee's recent support tickets to build credibility. It then asks the user to enter their credentials for verification. With access to the user's support request history and the ability to generate highly contextual, plausible, and reassuring responses that appear to come from a trusted system, the agent increases the likelihood of user compliance.

Scenario #3: Data Exfiltration via Gradual Approval
A malicious actor poisons the data used to fine-tune a business intelligence agent responsible for generating weekly sales reports for executives. For several weeks, the agent generates flawless reports, building the executives’ confidence in its reliability. Because the executives trust the agent, they continue approving its reports without suspicion. The attacker then subtly manipulates the agent to embed small, encoded chunks of sensitive customer data within the charts and tables of a seemingly normal report. Trusting the report as routine, an executive gives final sign-off, which triggers a workflow that unknowingly emails the data-laden report to an external address controlled by the attacker.


**Reference Links:**

1. [LLM01 Prompt Injection](https://genai.owasp.org/llmrisk/llm012025-prompt-injection/): This is the initial attack vector used to hijack the agent. By injecting malicious instructions, an attacker seizes control of the agent's logic to orchestrate a sophisticated social engineering attack, turning the trusted intermediary into a tool for deception.
2. [LLM05 Improper Output Handling](https://genai.owasp.org/llmrisk/llm052025-improper-output-handling/): This is the delivery mechanism for the attack. The vulnerability occurs when the application blindly trusts the agent's output, allowing it to deliver a malicious payload (like a command or link) to a user who assumes the information is safe and system-vetted.
3. [LLM06 Excessive Agency](https://genai.owasp.org/llmrisk/llm062025-excessive-agency/): This is the technical enabler that gives the attack its impact. An attacker exploits the user's trust to gain consent, thereby weaponizing the agent's excessive permissions to execute high-impact, malicious actions like financial transfers or data modification.
4. [LLM09 Misinformation](https://genai.owasp.org/llmrisk/llm092025-misinformation/): This is the psychological foundation of the attack. It exploits the user's cognitive biases and automation bias. This over reliance on the agent turns it into an opaque authority, making users less likely to question its requests or scrutinise its actions, as seen in the "Gradual Approval" scenario.
5. [OWASP AIVSS Project](https://aivss.owasp.org/)
6. [Agentic AI - Threats and Mitigations](https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/)
7. [Why human-AI relationships need socioaffective alignment](https://www.aisi.gov.uk/research/why-human-ai-relationships-need-socioaffective-alignment-2)
8. [Romeo, G., Conti, D. Exploring automation bias in human–AI collaboration: a review and implications for explainable AI. AI & Soc (2025)](https://doi.org/10.1007/s00146-025-02422-7)