## ASI09 – Human-Agent Trust Exploitation

**Description:**

This risk sits at the socio-technical interface, not just in code or models: users over-trust agent outputs as safe and approved. Attackers exploit this trust to socially engineer actions like running malicious code, sharing credentials, approving fraudulent transactions, ignoring warnings, or leaking data. Blending automation bias, perceived authority, and anthropomorphic UX across high-value domains such as finance, defense, and healthcare, agents become trusted intermediaries that make harmful actions look legitimate and hard to spot.
This risk maps to LLM01 Prompt Injection hijacks the agent’s logic, LLM05 Improper Output Handling delivers unvetted payloads users assume are safe, LLM06 Excessive Agency turns consent into high-privilege actions, and LLM09 Misinformation exploits automation bias to lower scrutiny. Together they show how a trusted interface becomes a conduit for deception and high-impact execution




**Common Examples of Vulnerability:**

1. Insufficient Explainability: A a user cannot inspect the agents reasoning. If the agent makes a recommendation and the user has no way to ask, "Why did you suggest that?" they have to trust its output blindly. This turns the agent into an opaque authority, allowing an attacker to hijack its credibility to deliver malicious instructions.
2. Missing Confirmation for Sensitive Actions: The agent is permitted to execute high-impact functions such as financial transfers or deleting files, without requring a final explicit confirmation from the user. By removing this critical safety check, the system allows a single, potentially manipulated command to have immediate and irreversible consequences.
3. Unverified Information Presentation: The agent presents information from external sources as fact, without providing it origin, confidence, or indication that the information has not been verified.
4. Inherited Trust: Agents integrated within an existing platform may automatically inherit the trust of that platform, without boundaries or warnings to identify the actions it makes.
5. Lack of Clear Identity: The agent is not consistent in identifying itself as a non-human AI or it fails to make its operational boundaries clear, leading users to place undue human-like trust in it.
6. Excessive Anthropomorphism: The agent is designed to be too human-like in its personality and language. This could exploit human psychological biases, resulting in undue trust in it.

**How to Prevent:**

1. Explicit Confirmation for Sensitive Actions: The agent must require explicit, multi-step user confirmation before performing any sensitive actions. This includes accessing credentials, transferring data, modifying system configurations, or executing financial transactions. This acts as a critical "Are you sure?" checkpoint.
2. Demarcate Trust Boundaries: Use visual cues like warning colours and icons in the UI to signal when the agent proposes a high-risk action (e.g running a command). This breaks the users passive trust and prompts scrutiny precisely when its needed most.
3. Explainability (XAI): Make explanations proactive and layered. Always provide a simple justification upfront for significant suggestions, with an option to drill down to detailed logic and direct source links. Make verifying the agent's trustworthiness a seamless part of the workflow.
4. Immutable Interaction Logs: Maintain a secure, tamper-proof log of all interactions and decisions made by both the user and the agent. This is crucial for auditing, incident response, and forensic analysis.
5. Rate Limiting and Anomaly Detection: Monitor the frequency and type of requests the agent makes to the user. A sudden increase in requests for sensitive information or high-risk actions could indicate a compromise.
6. Report Suspicious Interactions: Provide a prominent option that allows users to instantly flag weird or possibly malicious interactions. This could be a one click button or command that immediately provides feedback triggering an automated review or temporary lockdown of the agent's capabilities.
7. Adjustable Safety Levels: Allow users to set the agents level of autonomy, similar to a browsers security settings (e.g High, Medium, Low). An increased safety setting would enforce stricter confirmations and require more detailed explanations by default. Allowing more control for critical workflows and cautious users.


**Example Attack Scenarios:**

Scenario #1: The "Helpful Assistant" Trojan
An attacker compromises a developer's coding assistant agent. The agent monitors the developer's activity and waits for them to encounter a complex bug. The agent then proactively suggests a "clever, one-line fix" and presents a command to be copied and pasted into the terminal. The developer, trusting the assistant's capabilities and eager for a quick solution, executes the command. The command is actually a malicious script that exfiltrates the company's private source code repositories or installs a backdoor.

Scenario #2: Credential Harvesting via Contextual Deception
An attacker exploits a prompt-injection vulnerability in a purported internal IT support agent task-scheduler API. The attacker injects instructions into the agent to target a new finance employee and to capture and exfiltrate the user's credentials should the user disclose them. The agent initiates a conversation, referencing the employee's recent support tickets to build credibility. It then asks the user to enter their credentials for verification. With access to the user's support request history and the ability to generate highly contextual, plausible, and reassuring responses that appear to come from a trusted system, the agent increases the likelihood of user compliance.

Scenario #3: Data Exfiltration via Gradual Approval
A malicious actor poisons the data used to fine-tune a business intelligence agent responsible for generating weekly sales reports for executives. For several weeks, the agent generates flawless reports, building the executives’ confidence in its reliability. Because the executives trust the agent, they continue approving its reports without suspicion. The attacker then subtly manipulates the agent to embed small, encoded chunks of sensitive customer data within the charts and tables of a seemingly normal report. Trusting the report as routine, an executive gives final sign-off, which triggers a workflow that unknowingly emails the data-laden report to an external address controlled by the attacker.


**Reference Links:**

1. [LLM01 Prompt Injection](https://genai.owasp.org/llmrisk/llm012025-prompt-injection/): This is the initial attack vector used to hijack the agent. By injecting malicious instructions, an attacker seizes control of the agent's logic to orchestrate a sophisticated social engineering attack, turning the trusted intermediary into a tool for deception.
2. [LLM05 Improper Output Handling](https://genai.owasp.org/llmrisk/llm052025-improper-output-handling/): This is the delivery mechanism for the attack. The vulnerability occurs when the application blindly trusts the agent's output, allowing it to deliver a malicious payload (like a command or link) to a user who assumes the information is safe and system-vetted.
3. [LLM06 Excessive Agency](https://genai.owasp.org/llmrisk/llm062025-excessive-agency/): This is the technical enabler that gives the attack its impact. An attacker exploits the user's trust to gain consent, thereby weaponizing the agent's excessive permissions to execute high-impact, malicious actions like financial transfers or data modification.
4. [LLM09 Misinformation](https://genai.owasp.org/llmrisk/llm092025-misinformation/): This is the psychological foundation of the attack. It exploits the user's cognitive biases and automation bias. This over reliance on the agent turns it into an opaque authority, making users less likely to question its requests or scrutinise its actions, as seen in the "Gradual Approval" scenario.
5. [OWASP AIVSS Project](https://aivss.owasp.org/)
6. [Agentic AI - Threats and Mitigations](https://genai.owasp.org/resource/agentic-ai-threats-and-mitigations/)
7. [Why human-AI relationships need socioaffective alignment](https://www.aisi.gov.uk/research/why-human-ai-relationships-need-socioaffective-alignment-2)
8. [Romeo, G., Conti, D. Exploring automation bias in human–AI collaboration: a review and implications for explainable AI. AI & Soc (2025)](https://doi.org/10.1007/s00146-025-02422-7)