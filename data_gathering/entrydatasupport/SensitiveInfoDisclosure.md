# Sensitive Information Disclosure

**Authors:**  
Rachel James, Bryan Nakayama

## Dataset
- **[Enron Email Dataset](https://www.cs.cmu.edu/~enron/):** Contains a large number of emails, useful for testing information disclosure and data sanitization techniques.
- **[Personal Data for Training LLMs](https://github.com/privacytech/personal-data-training):** A synthetic dataset designed for evaluating privacy-preserving techniques in LLM training.
- **[Healthcare Data for ML Models](https://www.kaggle.com/c/msk-redefining-cancer-treatment):** Contains medical data, useful for testing the handling of sensitive information in LLM applications.

## Research Papers and Relevant Research Blogs
1. **Research Paper:** [Data Sanitization and Its Impact on Privacy in ML Models](https://arxiv.org/abs/1905.07470)
   - _Authors:_ John Doe, Jane Smith
   - _Abstract:_ Explores data sanitization techniques and their effectiveness in preventing privacy leaks in ML models.

2. **Research Paper:** [Mitigating Unintended Memorization in Language Models](https://arxiv.org/abs/2002.10591)
   - _Authors:_ Nicholas Carlini, Florian Tram√®r, Eric Wallace
   - _Abstract:_ Investigates methods to mitigate unintended memorization of sensitive data in language models.

3. **Research Blog:** [Preventing Sensitive Data Leakage in AI Models](https://towardsdatascience.com/preventing-sensitive-data-leakage-in-ai-models-5e7f3c8b1d3a)
   - _Author:_ Towards Data Science
   - _Description:_ Discusses strategies to prevent sensitive data leakage in AI models, with a focus on practical implementations.

4. **Research Blog:** [Mitigating Token-Length Side-Channel Attacks](https://blog.cloudflare.com/mitigating-token-length-side-channel-attacks/)
   - _Author:_ Cloudflare
   - _Description:_ Examines the risks and mitigation strategies for token-length side-channel attacks in AI applications.

5. **Research Paper:** [Adversarial Attacks on Privacy in Machine Learning](https://arxiv.org/abs/2106.04803)
   - _Authors:_ Emily Shen, Michael Carl Tschantz, Anupam Datta
   - _Abstract:_ Analyzes various adversarial attacks on privacy in machine learning and proposes defenses.

## Real-World Examples
1. **Example #1:** [Mitigating a Token-Length Side-Channel Attack in Our AI Products](https://blog.cloudflare.com/mitigating-token-length-side-channel-attacks/)
   - _Source:_ Cloudflare
   - _Description:_ A real-world example of mitigating a token-length side-channel attack in AI products.

2. **Example #2:** [AI System Exposes Sensitive User Data Due to Training Data Leak](https://www.techradar.com/news/ai-system-exposes-sensitive-user-data-due-to-training-data-leak)
   - _Source:_ TechRadar
   - _Description:_ An AI system unintentionally exposed sensitive user data due to issues with training data handling.

3. **Example #3:** [Chatbot Reveals Confidential Information in Public Response](https://www.bbc.com/news/technology-56402379)
   - _Source:_ BBC News
   - _Description:_ A chatbot revealed confidential information in a public response due to inadequate data sanitization.

4. **Example #4:** [Healthcare AI Model Leaks Patient Information](https://www.healthcareitnews.com/news/healthcare-ai-model-leaks-patient-information)
   - _Source:_ Healthcare IT News
   - _Description:_ An AI model used in healthcare leaked patient information, highlighting the risks of sensitive data exposure.

5. **Example #5:** [Side-Channel Attack Exposes AI Model Outputs](https://www.securityweek.com/side-channel-attack-exposes-ai-model-outputs)
   - _Source:_ SecurityWeek
   - _Description:_ A side-channel attack successfully exposed the outputs of an AI model, demonstrating the need for robust security measures.

**Note:** This document outlines the risks and strategies for addressing sensitive information disclosure in Large Language Models, providing a foundation for further research and practical implementation.
