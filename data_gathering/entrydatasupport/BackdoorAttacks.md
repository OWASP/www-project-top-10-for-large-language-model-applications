# Backdoor Attacks in Large Language Models (LLMs)

**Authors:**  
Massimo Bozza, Matteo Meucci

## Dataset
- **[Hugging Face Model Hub](https://huggingface.co/models):** Repository of models for various use cases, useful for testing backdoor vulnerabilities.
- **[OpenAI Model Zoo](https://github.com/openai/gym):** Datasets and models provided by OpenAI for evaluating the integrity of models.
- **[TensorFlow Hub](https://www.tensorflow.org/hub):** A repository of pre-trained models, helpful for analyzing vulnerabilities in the supply chain.

## Research Papers and Relevant Research Blogs
1. **Research Paper:** [Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review](https://arxiv.org/abs/2007.10760)
   - _Authors:_ Various
   - _Abstract:_ This paper provides a comprehensive review of backdoor attacks and countermeasures in deep learning, including LLMs.

2. **Research Paper:** [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)
   - _Authors:_ Various
   - _Abstract:_ Discusses methods for training deceptive LLMs that can persist through safety training, highlighting the risks of backdoor attacks.

3. **Research Blog:** [A Survey on Backdoor Attack and Defense in Natural Language Processing](https://arxiv.org/abs/2211.11958)
   - _Authors:_ Various
   - _Abstract:_ Surveys the landscape of backdoor attacks and defenses in natural language processing, providing insights into current trends and challenges.

4. **Research Blog:** [The Threat of Backdoor Attacks in AI Models](https://towardsdatascience.com/the-threat-of-backdoor-attacks-in-ai-models-79fafd87f7d5)
   - _Author:_ Towards Data Science
   - _Description:_ Explores the threat of backdoor attacks in AI models and discusses strategies to mitigate these risks.

5. **Research Paper:** [Adversarial Attacks and Defenses in Machine Learning](https://arxiv.org/abs/1810.00069)
   - _Authors:_ Various
   - _Abstract:_ Analyzes various adversarial attacks and defenses in machine learning, with a focus on LLMs.

## Real-World Examples
1. **Example #1:** [Backdoor Attacks in AI Systems: A Comprehensive Review](https://arxiv.org/abs/2007.10760)
   - _Source:_ Arxiv
   - _Description:_ A comprehensive review of backdoor attacks in AI systems, including case studies and real-world examples.

2. **Example #2:** [Sleeper Agents: Training Deceptive LLMs](https://arxiv.org/abs/2401.05566)
   - _Source:_ Arxiv
   - _Description:_ Discusses the training of deceptive LLMs that persist through safety training, including real-world implications.

3. **Example #3:** [Backdoor Attacks and Defense in NLP](https://arxiv.org/abs/2211.11958)
   - _Source:_ Arxiv
   - _Description:_ Surveys backdoor attacks and defenses in natural language processing, with real-world examples.

4. **Example #4:** [The Threat of Backdoor Attacks in AI Models](https://towardsdatascience.com/the-threat-of-backdoor-attacks-in-ai-models-79fafd87f7d5)
   - _Source:_ Towards Data Science
   - _Description:_ Discusses the threat of backdoor attacks in AI models and provides real-world examples of such attacks.

5. **Example #5:** [Adversarial Attacks and Defenses in Machine Learning](https://arxiv.org/abs/1810.00069)
   - _Source:_ Arxiv
   - _Description:_ Analyzes adversarial attacks and defenses in machine learning, with a focus on real-world examples of backdoor attacks.

**Note:** This document outlines the risks and strategies for addressing backdoor attacks in Large Language Models, providing a foundation for further research and practical implementation.
