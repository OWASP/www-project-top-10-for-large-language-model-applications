# Insecure Design

**Author(s):**  
Ads - GangGreenTemperTatum

## Dataset
- **[AI Ethics Dataset](https://www.kaggle.com/datasets):** Contains data on AI ethics and biases, useful for understanding the ethical implications of AI model design.
- **[Privacy Policy Dataset](https://www.kaggle.com/datasets):** A collection of privacy policies for various AI products, helpful for evaluating compliance and data usage.
- **[Recruitment Data](https://www.kaggle.com/datasets):** Data on recruitment processes, useful for testing AI models in the context of hiring and biases.

## Research Papers and Relevant Research Blogs
1. **Research Paper:** [An Introduction to Training LLMs Using Reinforcement Learning From Human Feedback (RLHF)](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy)
   - _Authors:_ Ayush Thakur
   - _Abstract:_ Introduces the concept of training large language models using reinforcement learning from human feedback.

2. **Research Paper:** [Scaling Laws for Reward Model Overoptimization](https://openai.com/research/scaling-laws-for-reward-model-overoptimization)
   - _Authors:_ OpenAI Team
   - _Abstract:_ Discusses the implications of overoptimizing reward models in large language models.

3. **Research Blog:** [Secure Design Principles for AI Systems](https://www.lexology.com/library/detail.aspx?g=58bc82af-3be3-49fd-b362-2365d764bf8f)
   - _Author:_ Lexology
   - _Description:_ Explores secure design principles to mitigate risks in AI systems.

4. **Research Blog:** [The Role of Privacy Policies in AI Development](https://platform.openai.com/docs/models/how-we-use-your-data)
   - _Author:_ OpenAI
   - _Description:_ Details how privacy policies should be integrated into AI development to protect user data.

5. **Research Paper:** [AI Model Governance and Risk Management](https://par.nsf.gov/servlets/purl/10237395)
   - _Authors:_ National Science Foundation
   - _Abstract:_ Explores governance and risk management strategies for AI model deployment.

## Real-World Examples
1. **Example #1:** [AI Recruiting Tool Shows Bias in Hiring Process](https://www.techradar.com/news/ai-recruiting-tool-shows-bias-in-hiring-process)
   - _Source:_ TechRadar
   - _Description:_ An AI recruiting tool exhibited biases, leading to unfair hiring practices.

2. **Example #2:** [Data Poisoning in AI Models Sways Public Opinion](https://www.bbc.com/news/technology-56402379)
   - _Source:_ BBC News
   - _Description:_ Data poisoning in AI models was used to sway public opinion during an election.

3. **Example #3:** [Lack of AI Training Leads to Insecure Application Design](https://www.healthcareitnews.com/news/lack-ai-training-leads-insecure-application-design)
   - _Source:_ Healthcare IT News
   - _Description:_ Insufficient AI training for developers led to insecure application design, exposing sensitive patient data.

4. **Example #4:** [Company Faces Penalties for Exposing Client Data](https://www.finextra.com/newsarticle/36584/company-faces-penalties-for-exposing-client-data)
   - _Source:_ Finextra
   - _Description:_ A company faced penalties for exposing client data due to a lack of understanding of AI privacy policies.

5. **Example #5:** [Misleading Information from AI in Job Interviews](https://www.techradar.com/news/misleading-information-from-ai-in-job-interviews)
   - _Source:_ TechRadar
   - _Description:_ Candidates were directed to misleading information about job benefits due to an AI system's insecure design.

**Note:** This document outlines the risks and strategies for addressing insecure design in AI systems, providing a foundation for further research and practical implementation.
