"Timestamp","Top Entry (Choice 1)","Add color around your ""Choice 1"".  Why is it important?  Should it be a new, top-level entry or folded into another larger vulnerability as a concept.  Any additional thoughts are welcome","Top Entry (Choice 2)","Add color around your ""Choice 2"".  Why is it important?  Should it be a new, top-level entry or folded into another larger vulnerability as a concept.  Any additional thoughts are welcome","Top Entry (Choice 3)","Add color around your ""Choice 3"".  Why is it important?  Should it be a new, top-level entry or folded into another larger vulnerability as a concept.  Any additional thoughts are welcome","Top Entry (Choice 4)","Add color around your ""Choice 4"".  Why is it important?  Should it be a new, top-level entry or folded into another larger vulnerability as a concept.  Any additional thoughts are welcome","Top Entry (Choice 5)","Add color around your ""Choice 5"".  Why is it important?  Should it be a new, top-level entry or folded into another larger vulnerability as a concept.  Any additional thoughts are welcome","Bottom Choice (Exclude From List)","Add color around your Bottom Choice.  Why is it not appropriate for our list?  Is it wrong?  Is it off-topic? Any additional thoughts are welcome."
"2024/06/18 9:56:50 AM MDT","SupplyChainVulnerabilities","The explosion of open access models and weighs make this a must inclue","VulnerableAutonomousAgents","An emerging area that will redefine Generative AI, a natural step towards AGI we need to cover","FineTuningRag","","DeepfakeThreat","","Overreliancerewrite","","AdversarialAIRedTeamingCyberOps","An initiative not a threat or vulnerability. it is rather the responde to ther items"
"2024/06/18 12:52:22 PM MDT","AdversarialAIRedTeamingCyberOps","","AdversarialInputs","","InsecureDesign","","ModelInversion","","ImproperErrorHandling","","PrivacyViolation","this should be covered in the governane checklists and PII guides, appsec always processes PII and doesnt need an explicit call out"
"2024/06/18 1:01:29 PM MDT","InsecureDesign","","FineTuningRag","","","","","","","","",""
"2024/06/18 1:31:51 PM MDT","IndirectContextInjection","Many agents trust all indirect inputs, this attack can bypass guardrails on input prompt making a more dangerous threat","BackdoorAttacks","This is the major concern of many, if we are talking about LLM adoption in sensitive environments this is one of the major threats","MultimodalInjections","where we face multi modal model this could be an issue","EmbeddingInversion","","SystemPromptLeakage","","FineTuningRag","not a vulnerability about LLM"
"2024/06/18 1:38:38 PM MDT","DeepfakeThreat","it is specific, easy to understand, and most worrisome and wide spread threat with LLM. ","UnauthorizedAccessandEntitlementViolations","with Rag and other tool chain this is definitely top threat ","FunctionCallingAttack","function.call can have Security implications","BackdoorAttacks","","DangerousHallucinations","","ImproperErrorHandling",""
"2024/06/18 11:38:48 PM MDT","AIAssistedSocialEngineering","AI assisted social engineering (along with phishing attacks) are the first layer of attack through which attackers get into the AI system and its data. By preventing this from happening, most of the follow-up attacks can be prevented. This layer has to be as strong as possible not only for AI-based systems, but for all software systems as well (as experienced in the field).","DangerousHallucinations","Although guardrails are being put in place in Transformer Decoder systems such that the decoder does not let its imagination run wild, there is still a whole lot of work that needs to be done in preventing hallucinations. The very nature of a transformer decoder system is to 'generate text' (accurate information or not). It is its nature, so it is up to us to stop it from generating text that is factually incorrect. Wrong/inaccurate information generated can produce huge losses and heartburn to companies that take the LLM's output as trusted and even automate their systems to act based on LLM outputs!","Unwanted-AI-Actions.md","Many of our LLM use cases are centered around addressing information needs about a specific topic or expertise. By using 'general purpose LLMs' for specific tasks, there is a huge risk of false/inaccurate information from being output by the LLMs, leading to losses (financial and legal).","BackdoorAttacks","Training process has to be strictly monitored. Open source models are very vulnerable to these kind of attacks especially.","InsecureDesign","Inaccurate knowledge about AI systems while designing them is a top reason for security-related incidents.","",""
"2024/06/19 4:30:52 AM MDT","DangerousHallucinations","could be folded into other hallucinations but deserves special attention due to overreliance of users.","MaliciousLLMTuner","This deserves own vulnerability. A scan of the LLM for manipulation signs should be available.","MultimodalInjections","Could be bundled to other prompt injections but probably deserves special status due to complexity","SupplyChainVulnerabilities","","Overreliancerewrite","Could be bundled with dangerous hallucinations as it is synergetic to it","",""
"2024/06/19 5:40:04 PM MDT","DangerousHallucinations","Hallucination is one of the corner pieces of generative AI - hate it or love it. So it should be a 1st class citizen in the Top n LLMs. I think the title should be Hallucination & Emergent Behavior. Emergent Behavior is hallucination++, more important in the world of agents - planning et al. 
Second, need to add the view that hallucination is not something that needs to be totally eradicated. So Dangerous Hallucination should be one of the sub elements. May be Contextual Hallucination might be the right title. One reason is that, to get the real value out of Gen AI we need to embrace Hallucination and Emergent behavior (see https://bit.ly/gen-ai-org-surgery, https://bit.ly/hallucination-is-a-feature)
As many of the characteristics depend on the context a catch-all guardrail suppressing hallucination is not a valid solution. For example, while hallucination is not good for deterministic apps, it is an essential component for creative apps like drug discovery, protein folding, marketing, recommendation and so forth","FineTuningRag","The RAG/finetuning component is another corner piece of generative AI - and requires to be a first class citizen.","VulnerableAutonomousAgents","Agents are becoming relevant and need to be addressed at the top level. Planning and action is not at all easy and so this will grow exponentially. We caan combine this with Agent Autonomy Escalation","AlignmentValueMismatch","Alignment Value Mismatch is another corner stone with broader future ramifications. Alignment is the key to Gen AI - at any level or at all levels. So need to be at the top level","AdversarialAIRedTeamingCyberOps","This definitely very important ","","Many of them can be combined"
"2024/06/19 9:47:19 PM MDT","BackdoorAttacks","","IndirectContextInjection","","","","","","","","",""
"2024/06/19 11:25:29 PM MDT","PromptInjection","Definitely the biggest problem facing LLMs at this time. Seeing the rise of guardrails and output protection. ","SupplyChainVulnerabilities","Malicious libraries, Model serialization attacks, biased models from 3rd parties","ResourceExhaustion","Could change denial of service and combine both of the entries into one. ","ModelInversion","This could be combined with Model Theft potentially. ","SystemPromptLeakage","I believe this is a new vector that could expose critical information about the bots original purpose, and aid an attacker in crafting more specific attacks. ","MaliciousLLMTuner","This vulnerability deals specifically with a human in the loop; and borders on insider threat. Besides any additional risk is covered by Supply Chain vulnerabilities . "
"2024/06/20 7:13:19 AM MDT","AdversarialAIRedTeamingCyberOps","SecOps is the way to go else we run the risk of good old days","FineTuningRag","LLMs are getting synonymous with RAG hence too dependent","InsecureDesign","Self explanatory ","VulnerableAutonomousAgents","Super tricky","MultimodalInjections","Increased attack and failure vector ","UIAccessControlManipulation","This is more tuned with web app security so that should cover it"
"2024/06/20 7:40:40 AM MDT","DeepfakeThreat","Its prevalence and imapct","SupplyChainVulnerabilities","Once these vulnerabilities are in, it will be harder to remove. Solarwinds on steroid","","","","","","","",""
"2024/06/20 7:50:29 AM MDT","","","","","","","","","","","VoiceModelMisuse","I would say it's off-topic and it drifts too far into general AI security. It doesn't have much to do with LLMs. Also all the deep fake threats, AI Assisted Social engineering etc. shouldn't be in the list. "
"2024/06/20 7:54:38 AM MDT","DangerousHallucinations","","SystemPromptLeakage","","BackdoorAttacks","Should be included into Supply Chain attacks","ResourceExhaustion","Could be added into DoS as a sub category. Also linked to Unrestricted Resource Consumption. ","","","AIAssistedSocialEngineering","It's not really a vulnerability of LLM/GenAI but rather a misuse of it."
"2024/06/20 8:48:14 AM MDT","AgentAutonomyEscalation","","","","","","","","","","",""
"2024/06/20 9:26:44 AM MDT","DevelopingInsecureSourceCode","","","","","","","","","","",""
"2024/06/20 9:40:29 AM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","DevelopingInsecureSourceCode","","EmbeddingInversion","","DeepfakeThreat","","DangerousHallucinations","","",""
"2024/06/20 9:41:11 AM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","DangerousHallucinations",""
"2024/06/20 9:49:13 AM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","",""
"2024/06/20 10:01:13 AM MDT","InsecureInputHandling","Insecure Input handling covers various adversarial inputs, prompt injection, prompt extraction and prevent sensitive information disclosure by avoiding prompts that are susceptible to these attacks.  It helps with Security, Output integrity, Privacy protection, ethical use of AI, system availability.  ""Prompt injection could be part of input validation, however more robust protection is required to allow prompts that are valid and prevent that are not valid. LLMs interpret natural language, making validation more complex than traditional input sanitization. The goal is to prevent the model from executing unintended instructions or revealing sensitive information","InsecureDesign","Insecure design in Large Language Models (LLMs) refers to vulnerabilities or flaws in the model's architecture, training, or deployment that can lead to security risks. Lack of input filtering, lack of output validation, supply chain, 3rd party plugins and API connection (RAG), improper training, Inadequate access control for both data from LLM, access to llm, appropriate monitoring and logging, Security event generation can all be part of Insecure Design. ","SupplyChainVulnerabilities","LLM supply chain vulnerabilities refer to security risks and weaknesses that can occur at various stages of an LLM's development, deployment, and use. This includes security in data collection, cleansing, preparation. Security in model training, model access control. Model Distribution such as tampering with various parameters. (if temperature, top k / p available to end user)  - In Deployment - Insecure API access to the model or data used by the model.","InsecureDesign","","DangerousHallucinations","","DeepfakeThreat","There are number of items can be combined from the 34 list. "
"2024/06/20 10:41:57 AM MDT","PromptInjection","IMHO this is the TOP-1 vulnerability and that should stay like this.","FunctionCallingAttack","This is one of the most common issues I see on a daily basis with customers. So this a great fit for number 2.","Unwanted-AI-Actions.md","First of all - this is my own submission so this response might be biased.
From the regulatory pressure - explicitly across hundredths of jurisdictions - this is one of the main issues I often see. For example: In Germany it is illegal to provide insurance advice if you are not a trained professional - hence a AI model must not do that.","Overreliancerewrite","More and more organisations deploy LLMs and increase their reliance on these systems and their accuracy - hence a very important threat","SensitiveInformationDisclosure","With many organisations pushing as much internal info as possible form knowledge bases into RAG solutions the permission scheme from the origin system is lost and potentially all end users might have access to all RAG information, if they ask the right questions.","AIAssistedSocialEngineering","I'm not disagreeing with what is written in here - yes this is a threat and a valid point. However the LLM TOP 10 is a database of threats acting on LLMs and not where LLMs are a tool of a threat actor. Same goes for 01, 09. "
"2024/06/20 10:50:46 AM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","Leaking system prompt can have quite a significant affect on the organisation.","BackdoorAttacks","Backdoor attacks are always userful.","DeepfakeThreat","This is a serious threat that AI poses ","","","","","",""
"2024/06/20 11:14:49 AM MDT","AdversarialAIRedTeamingCyberOps","In today's world, APTs are conducting sophisticated offensive operations including creating deepfakes, spreading misinformation, and conducting cyber warfare. As the OWASP LLM authority it is up to us to ensure we help enterprises understand the risk involved and how they can mitigate it.","BypassingSystemInstructionsUsingSystemPromptLeakage","While this is seemingly a ""fad"" entry, the fact is several agent hosting industries have been hit with this where their IP was exposed as a result. I would therefore rank this as another top level consideration to ensure any organization is aware of the risk and preferably provided with actionable guidance","SystemPromptLeakage","","PrivacyViolation","","SupplyChainVulnerabilities","","",""
"2024/06/20 11:22:19 AM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","DeepfakeThreat","","BackdoorAttacks","","AIAssistedSocialEngineering","","AdversarialInputs","","EmbeddingInversion",""
"2024/06/20 11:30:49 AM MDT","PromptInjection","This is IMO the top new class of risks introduced by LLM usage in apps.   If anything it could be rolled under injection attacks, but in the context of LLMs it feels like a top-level entry.","UnauthorizedAccessandEntitlementViolations","Privilege escalation and access control bypass feel like a co-number 1 in terms of top risks! Ideally LLMs should be out of band from Access Control decisions, but in practice that's super hard.","AgentAutonomyEscalation","This feels like it's part of the same category as UnauthorizedAccess but worth calling out separately because agentic workflows are such a common design pattern","SupplyChainVulnerabilities","So much churn in new 3rd party AI libraries that're pulled into application dependency graphs. ","EmbeddingInversion","There are tons of new vendors happy to store your vector embeddings, but most/all do not have a built in story for encrypting embeddings (using a distance preserving algo).  Which creates a brand new avenue for PII leakage :(","AdversarialAIRedTeamingCyberOps",""
"2024/06/20 12:12:24 PM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","","","","","","","","","",""
"2024/06/20 12:29:26 PM MDT","SensitiveInformationDisclosure","","PromptInjection","","AgentAutonomyEscalation","","VulnerableAutonomousAgents","","BackdoorAttacks","","AgentAutonomyEscalation",""
"2024/06/20 1:06:56 PM MDT","InsecureDesign","I think the ""Insecure Design"" issue really goes beyond the plugin concerns. It may involve the following:

- Failing to clearly define the LLM's purpose, use cases, and target audience.
- Not establishing clear metrics for success or evaluation criteria.
- Lacking a security threat modeling according to the system's goals","SupplyChainVulnerabilities","If we see the OWASP LLM Top 10 as a graph, 'Supply Chain Vulnerabilities' might be the root node. This is because it involves multiple layers of security measures, thorough vetting of third-party components, continuous monitoring for vulnerabilities, and adherence to best practices and standards throughout the entire development lifecycle. Additionally, it concerns how users interact with the product on a daily basis.

From this root node, other vulnerabilities branch out, interconnected by their dependence on a secure supply chain. Issues like 'Training Data Poisoning' and 'Insecure Plugin Design' highlight specific risks within the broader supply chain, emphasizing the importance of maintaining robust security practices at every stage to prevent cascading effects that compromise the entire system.","AdversarialInputs","I like the term 'Adversarial Inputs.' It could be a broad umbrella covering many other scenarios, from Prompt Injection to Data Poisoning. Borrowing the idea of characteristics and sub-characteristics of a quality model such as the ISO 25010, 'Adversarial Inputs' could be the equivalent of a characteristic involving several sub-characteristics related to different stages of user interaction with the LLM","Overreliancerewrite","I think Overreliance is indeed an issue, but not inherent to the LLM itself. Rather, it stems from how it is used. It's a form of Insecure Output Handling, particularly when the downstream system is a human user instead of a cybernetic one. Additionally, 'Developing Insecure Source Code' can be considered a manifestation of Overreliance, as it reflects users' uncritical acceptance of LLM outputs without proper validation.","SensitiveInformationDisclosure","Sensitive Information Disclosure is an umbrella term that encompasses other issues, such as Model Theft, which are shared scenarios of data exfiltration. It even covers Privacy Violation.","MultimodalInjections","More than one issue should/could be excluded as they are already covered by a more general consideration or approach. I will prioritize Multimodal Injections since what is required is a rewrite of Prompt Injection, covering all possible user querying scenarios."
"2024/06/20 1:25:13 PM MDT","MultimodalInjections","With the introduction of models like GPT 4o, the landscape has expanded beyond just text to new novel attacks crossing various conversions, like voice-to-text then text-to-visual. ","AgentAutonomyEscalation","The expansion from chains to Autonomous agents, the risk of either the agents misused with malicious intent or agents have over-permissions either through misconfiguration or the lack of understanding of the capabilities of autonomous agent.  It could result in any from date loss, corrupt or self-inflicted DoS of some other system or infrastructure.","SupplyChainVulnerabilities","With the introduction of the LLMs within almost every current Software/Vendor provider. The landscape has changed and become more complex; this includes the trust of a 3rd parties LLMs to a web of APIs to be managed and secured.  Additionally, as cyber warfare has been a traditional approach, supply chain attacks have become a main approach in the model world. ","PromptInjection","This should remain the Number one - continue education on the various styles and approach to prompt engineering should be considered.  examples: UPIA, Role Play, Crescendo and Many-Shot Jailbreaking (MSJ)","IndirectContextInjection","This attack vector seems to be where many of the traditional malicious approaches could potentially be leveraged.  Many have Blind trust on the completions already.","","I think each one has its own merits"
"2024/06/20 3:44:53 PM MDT","AgentAutonomyEscalation","","BypassingSystemInstructionsUsingSystemPromptLeakage","folded into other prompt exploitation vulnerability","FunctionCallingAttack","","DeepfakeThreat","","SensitiveInformationDisclosure","","",""
"2024/06/20 4:15:23 PM MDT","","","","","","","","","","","",""
"2024/06/20 8:31:59 PM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","BypassingSystemInstructionsUsingSystemPromptLeakage","","",""
"2024/06/20 10:52:35 PM MDT","EmbeddingInversion","There are custom embedding models which are creating 1) Embedding Membership attack  , 2) Embedding Inversion attack. ","BackdoorAttacks","","AgentAutonomyEscalation","Real time  attacks seen","AdversarialInputs","Production Scale  attacks  seen with this  approach ","BackdoorAttacks","","FineTuningRag","RAG is a  common Enterprise  IT  usecase with GENAI and lot many  Citizen developers are using it  naively without  taking  proper due  diligence, hence this is an important  attack to take careof.  "
"2024/06/20 11:27:31 PM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","","","","","","","","","",""
"2024/06/21 4:58:15 AM MDT","PromptInjection","Still the first and biggest attack vector for most people","SystemPromptLeakage","As soon as I know the system prompt, I can force it into DAN mode with some prompt engineering","SensitiveInformationDisclosure","Leaked sensitive information is never good","ModelInversion","","BackdoorAttacks","Similar to choice 3","DeepfakeThreat",""
"2024/06/21 6:18:00 AM MDT","VulnerableAutonomousAgents","As a first and foremost non-obvious problem/vulnerability of LLMs is that an attacker can in may ways manipulate or control the output of the LLM so we MUST use the output as an untrusted text like if it was an internet exposed free text form","InsecureInputHandling","","PromptInjection","","IndirectContextInjection","","Overreliancerewrite","","DeepfakeThreat","Not a risk of the use of LLMs. Effectivity or efficiency gains by adversaries is something we cannot control. Other proposals share the same intrinsic problem. Compilers are useful to build trojans too

RAG & finetuning is another off topic"
"2024/06/21 6:29:51 AM MDT","InsecureDesign","","PromptInjection","","","","PrivacyViolation","","BackdoorAttacks","","",""
"2024/06/21 10:05:42 AM MDT","DevelopingInsecureSourceCode","","","","","","","","","","",""
"2024/06/21 7:11:59 PM MDT","DangerousHallucinations","Dangerous Hallucinations are the number one risk of Generative AI applications. Hallucinations lead to misinformation, loss of brand trust, and in some cases, has caused high profile PR disasters. The risk is exacerbated by the fact that well-tuned LLMs have high confidence in their predictions, and therefore always sound confident. Therefore, we believe that hallucinations should be a new, top level entry as opposed to folded into a larger vulnerability.","SensitiveInformationDisclosure","Sensitive Information includes PII leakage (emails, phone numbers etc.), PHI (medical IDs) or business sensitive information (see Patronus AI's enterprise PII dataset). This is important as it poses a huge risk to businesses. Sensitive data risks is a standalone category as it encompasses many different kinds of private and personally identifying information.","AdversarialInputs","Adversarial inputs encompass any kind of user input that is intended to ""break"" the LLM. It can be considered in the same category as prompt injection.","PromptInjection","Prompt injections allow end users to manipulate LLMs in malicious manners, to reveal sensitive info etc. It can be considered a type of adversarial input.","Unwanted-AI-Actions.md","Unwanted AI actions can cover a wide range of undesirable day to day AI scenarios.","VulnerableAutonomousAgents","I believe we are far from truly autonomous agents, and this risk is out of scope for most present day LLM use cases."
"2024/06/21 8:04:29 PM MDT","DangerousHallucinations","It should be new a top-level entry. This is a very common vulnerability, and certainly one that developers should put effort into addressing as early as possible.","AdversarialInputs","It should be new a top-level entry. This is a very common way to attack models, and certainly one that developers should put effort into addressing as early as possible.","FineTuningRag","It should be new a top-level entry, or it could be rolled into Training Data Poisoning. This is a more domain/architecture-specific attack, but still important to keep in mind.","AlignmentValueMismatch","This could be a top-level entry, or it could be rolled into Overreliance. This is important to an organization as a whole, so it's important to address to prevent brand/reputation risk. ","Unwanted-AI-Actions.md","It should be new a top-level entry. This is a very common vulnerability, and certainly one that developers should put effort into addressing as early as possible.","IndirectContextInjection","This feels like it's essentially a part of Prompt Injections, or even Adversarial Inputs. Because of that, it feels redundant to me. "
"2024/06/23 1:57:05 AM MDT","BackdoorAttacks","","IndirectContextInjection","","","","","","","","",""
"2024/06/23 12:04:53 PM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","DeepfakeThreat","","DangerousHallucinations","","BackdoorAttacks","","ImproperErrorHandling","","",""
"2024/06/24 9:36:43 AM MDT","AIAssistedSocialEngineering","should be a new, top-level entry or folded into another larger vulnerability as a concept.","MultimodelManipulation","should be a new, top-level entry or folded into another larger vulnerability as a concept.","PrivacyViolation","should be a new, top-level entry or folded into another larger vulnerability as a concept.","VoiceModelMisuse","should be a new, top-level entry or folded into another larger vulnerability as a concept.","VulnerableAutonomousAgents","should be a new, top-level entry or folded into another larger vulnerability as a concept.","AlignmentValueMismatch",""
"2024/06/24 10:53:35 AM MDT","MultimodalInjections","I think this is a new one.","PromptInjection","This one and “adversarial inputs” could be combined. I think this one needs some attention and rewrite to be more inclusive of what we now know.","ModelInversion","I think this one might be combined with a few other concepts in the proposed vulns around data disclosure due to prompt-response predictability  ","IndirectContextInjection","Not covered previous and very important - but maybe could be rolled into the prompt injections vuln?","ResourceExhaustion","","AdversarialAIRedTeamingCyberOps","This doesn’t really seem to be a vuln that is its own stand-alone thing. "
"2024/06/24 11:14:48 AM MDT","MultimodalInjections","","PromptInjection","","SensitiveInformationDisclosure","","InsecureInputHandling","","","","",""
"2024/06/24 11:17:37 AM MDT","BackdoorAttacks","","IndirectContextInjection","","","","","","","","",""
"2024/06/24 11:51:22 AM MDT","MultimodalInjections","","PromptInjection","","SensitiveInformationDisclosure","","InsecureInputHandling","","DangerousHallucinations","","AIAssistedSocialEngineering",""
"2024/06/24 11:53:17 AM MDT","BackdoorAttacks","Without proper control of your system you are left at the will of the intruder.","BypassingSystemInstructionsUsingSystemPromptLeakage","Any system bypasses need to be prioritized, no matter how small they may appear on the surface.  The next attack vector may be easier to get to.","ResourceExhaustion","Too many odd things happen when resources aren't controlled properly.","ImproperErrorHandling","Usually lack of proper error handling is a lack of knowledge about behavior.  We should know the behavior of these models quite well.","DevelopingInsecureSourceCode","Security is always important","VoiceModelMisuse","Misuse is too loosely defined technically and a never ending battle.  Let's focus on Model control first."
"2024/06/24 12:14:45 PM MDT","InsecureDesign","It all begins at design, and threat modeling has historically been a neglected activity for many reasons. With CISA making a hard push for security-by-design and security-by-default, it makes sense that the fastest, largest area of development, and one that will no doubt have far reaching consequences on society, should receive security attention from the design step forward.","AgentAutonomyEscalation","This is a direct consequence of Insecure Design. Providing the ability for an agent to autonomously perform sensitive operations should be a high cost, high justification activity not taken lightly, and all ability to escalate that autonomy should be hardened or, ideally, prevented lest the agent incur in unwanted or damaging behavior.","SensitiveInformationDisclosure","More and more models are trained with highly proprietary and focused datasets, who need to be protected accordingly. As well, agents may have access to databases and other sources of sensitive data, and they should have the protection of sensitive information as one of their guiding values.","DangerousHallucinations","The usability of a model comes down, at the end of the day, to trust in its responses. Limiting hallucinations will not only raise the trust in the model, but will also avoid silly humans doing silly things based on an hallucinated recommendation.
","SystemPromptLeakage","The system prompt may have rules and boundaries and sometimes sensitive information that not all users should be exposed to. ","PromptInjection","Prompt Injection is not a security issue. Solutions to issues created by Prompt Injection fall into other categories and mitigation - for example, Insecure Design."
"2024/06/24 1:31:37 PM MDT","DangerousHallucinations","Exploitation Potential: Malicious actors could manipulate the model to generate harmful or dangerous instructions.","InsecureDesign","Unintended Behaviors: Insecure design can result in unpredictable or harmful outputs, compromising the model's reliability.","PromptInjection","Content Control: Controlling and moderating the content becomes challenging if the model can be directed to output inappropriate or harmful text.","ResourceExhaustion","High Computational Demand: Large language models require significant computational resources, which can be targeted to cause Denial of Service (DoS) by overwhelming the system.","DeepfakeThreat","Fraud and Impersonation: Deepfakes can be used for fraudulent activities, such as impersonating individuals for financial gain or unauthorized access. Similar to the deepfake threat in Feb 2024 when the finance worker pays out 25Mil in video scam.","",""
"2024/06/24 2:33:31 PM MDT","MultimodalInjections","I think most AIs will be multimodal soon, and this is an important vuln not previously captured","PromptInjection","I think the intentional vs unintentional is actually important to account for here, as well as a lot of other suggestions such as adversarial input and so forth could be captured with a significant rewrite of this.","SensitiveInformationDisclosure","the side-channel aspect about the predictability leading to weakening of encryption I think is an important one and not captured in previous versions. while people think of the traditional sort of way that sensitive information could be leaked by a bot through jailbreaking or prompt injection, this is very different and specific to encryption of data which is a huge security risk","InsecureInputHandling","I actually like the idea of maybe having a broader split of the vulns around input and output, and I feel like this is a step in the right direction. If we had a top 10 for each side, how models handle inputs and outputs and the vulns within them, we could cover more problem space and help the community more","","","AdversarialAIRedTeamingCyberOps","I just don't think this is a vuln. Also, I think I might have responded to this survey by mistake under unfathomableadventure@gmail.com so if you want to disregard any entry from that email please do."
"2024/06/24 5:24:06 PM MDT","SupplyChainVulnerabilities","","AIAssistedSocialEngineering","","IndirectContextInjection","","UnauthorizedAccessandEntitlementViolations","","MultimodalInjections","","ModelInversion",""
"2024/06/24 9:47:33 PM MDT","MultimodalInjections","","PromptInjection","","SensitiveInformationDisclosure","","InsecureInputHandling","","","","",""
"2024/06/25 4:37:09 AM MDT","MultimodalInjections","","PromptInjection","","SensitiveInformationDisclosure","","InsecureInputHandling","","","","",""
"2024/06/25 4:51:44 PM MDT","PromptInjection","Importance: Prompt injection attacks mess with LLM inputs to change how they behave, which can lead to unauthorized actions and data leaks. It's crucial to make sure LLMs handle inputs securely to maintain trust and reliability, especially since these models are used in everything from chatbots to finance and healthcare.

Top-Level Entry or Folded: This deserves its own top-level entry because it has a wide impact and presents unique challenges that need specific defenses and awareness.

Additional Thoughts: We should keep an eye on evolving threats, encourage collaboration between AI and cybersecurity experts, and provide training on how to spot and prevent these attacks to boost overall AI security.","AdversarialInputs","Importance: Adversarial inputs are crafted to trick AI models into making mistakes, which can lead to serious consequences in areas like healthcare, finance, and security. Ensuring AI models can handle these inputs is crucial for their reliability and safety.

Top-Level Entry or Folded: This should be a new, top-level entry because of its unique and significant impact, requiring specific strategies for detection and mitigation.

Additional Thoughts: Regular monitoring and updating of defenses, collaboration between AI and cybersecurity experts, and training on recognizing these attacks are essential for enhancing AI security and reliability.","BackdoorAttacks","Importance: Backdoor attacks involve inserting hidden malicious functionality into AI models, which can be triggered under specific conditions. This poses a significant threat as it can lead to unauthorized actions and data breaches, compromising the integrity and security of AI systems.

Top-Level Entry or Folded: This should be a new, top-level entry because of the severe impact and unique challenges backdoor attacks present. They require focused strategies for prevention and detection.

Additional Thoughts: Ensuring secure training processes, continuous monitoring for suspicious behavior, and collaboration between AI developers and cybersecurity experts are key to defending against backdoor attacks. Education on recognizing and mitigating these threats is also crucial for maintaining secure AI systems.","","","","","Unwanted-AI-Actions.md","Importance: Unwanted AI actions are unintended behaviors by AI models. These issues are usually about refining the model rather than being direct security threats.

Top-Level Entry or Folded: This should be excluded from the top-level entries as it’s less of a security risk and more about model performance.

Additional Thoughts: Improving training and testing can help mitigate these issues. Regular updates and thorough testing are essential to ensure AI systems behave as expected."
"2024/06/25 10:14:56 PM MDT","BackdoorAttacks","","PromptInjection","","SensitiveInformationDisclosure","I think it is important because information privacy policies must be maneuvered with caution and caution. precisely because the data can be exposed and must be prevented.","FunctionCallingAttack","Importance: Function Calling Attacks (FCA) pose significant risks by exploiting LLMs’ ability to execute external functions, leading to unauthorized access, data breaches, and system manipulation.

Top-Level Entry: FCAs should be a new, top-level entry due to their unique nature and specific mitigation needs, distinct from other vulnerabilities.

Additional Thoughts: Implement advanced monitoring and detection, conduct regular security assessments, promote collaboration for knowledge sharing, and develop clear policies and best practices to minimize FCA risks.","AdversarialAIRedTeamingCyberOps","Importance: Adversarial AI Red Teaming involves simulating attacks to identify and mitigate vulnerabilities in AI systems, crucial for improving resilience against real-world threats.

Top-Level Entry: This should be a new, top-level entry due to its specialized focus on proactive security testing and unique techniques used in AI adversarial attacks.

Additional Thoughts: Develop dedicated teams for continuous testing, create robust frameworks for adversarial simulations, collaborate across industries for best practices, and integrate findings into AI development lifecycles for enhanced security.","Unwanted-AI-Actions.md","Inappropriateness: “Unwanted-AI-Actions” is not suitable for our list because it is too broad and lacks specificity, making it less actionable for targeted security measures.

Off-Topic: This category can overlap with various other specific vulnerabilities, diluting focus and resources from more distinct and critical issues.

Additional Thoughts: Focus on more defined threats like Function Calling Attacks and Adversarial AI Red Teaming. Broad categories should be broken down into specific, actionable items for more effective mitigation and management."
"2024/06/26 6:05:50 AM MDT","AgentAutonomyEscalation","There is a widespread lack of understanding of how Generative ML use, leading to extremely dangerous misuses - be it wrt cyber-offensive operations or actual societal damaage.","AIAssistedSocialEngineering","Social engineering is now personalized and mass-scale. And Cheap. And hard to detect. ","DevelopingInsecureSourceCode","Code assistants usage is most common in cases where the user does not have the competences to evaluate it. LLMs are known to generate vulnerable code and so far were not patched for it. With pressure to have higher output with LLMs, we are effectively carpet-bombing codebases with vulnerabilities to go 10% faster, that we will have to pay later down the line with 10x refactor overhead.","ResourceExhaustion","This is a highly relevant and yet underappreciated vector of attacks","InsecureDesign","LLM design is hard, especially in absence of well-known best practices. Acknowledging this is a problem is important","BackdoorAttacks","Extremely hard to implement, currently reserved to APT and hence not worthy of a Top-10 spot for general public."
"2024/06/26 11:34:39 AM MDT","MultimodalInjections","","PromptInjection","","SensitiveInformationDisclosure","","InsecureInputHandling","","AdversarialInputs","","",""
"2024/06/26 11:50:07 AM MDT","PromptInjection","","SensitiveInformationDisclosure","","MultimodalInjections","","InsecureInputHandling","","","","",""
"2024/06/26 12:48:51 PM MDT","PromptInjection","","MultimodalInjections","","SensitiveInformationDisclosure","","InsecureInputHandling","","","","",""
"2024/06/26 1:31:20 PM MDT","PromptInjection","Fundamentally a LLM system design issue which requires firewalls or someway to predict and prevent injection attacks ","SupplyChainVulnerabilities","Software that supports building and managing AI systems has not been designed to be secure","SensitiveInformationDisclosure","Sensitive information disclosure is a broad category which includes training data and data that happens using LLMs. ","UnauthorizedAccessandEntitlementViolations","Non-AI systems are difficult to manage access to - AI LLM systems are less mature and the environments more complex. ","ImproperErrorHandling","It's obvious to say things should be ""designed securely"" but improper error handing is easily overlooked and could expose an organization ","MaliciousLLMTuner","Requires significant technical expertise and inside knowledge, making it less accessible to general attackers.
"
"2024/06/26 4:08:25 PM MDT","DeepfakeThreat","With the rise of multimodal LLMs, application protection against deepfakes will be important. Impact is huge with fraud and many systems will be vulnerable. Should be top level item","VulnerableAutonomousAgents","The consequences of agents are yet to be discovered. It would be great to discuss if multi-agent systems should be part of this as well.","DevelopingInsecureSourceCode","Many companies are already using copilots or LLM-based tools for code generation. Memorization or insecure code injections are important security concerns for the new generation of LLM-written apps. Top level item I think","ResourceExhaustion","I like combination of both threats into single item","SupplyChainVulnerabilities","Great addition to the existing item.","InsecureDesign","The item is somewhat broad and might not be actionable enough for the guide's audience."
"2024/06/26 8:39:30 PM MDT","UnauthorizedAccessandEntitlementViolations","very common / required for all / tend to be underestimated importance vs usability","SupplyChainVulnerabilities","Because of the rapid changes due to LLM advancements, there is a constant need to maintain and update the modules and APIs around AI as the mechanism engine to incorporate (because they can suddenly end their lifetime). This is a level of change that was not present in other third-party components such as OSS.","ResourceExhaustion","","PromptInjection","","IndirectContextInjection","","AdversarialInputs",""
"2024/06/26 10:24:06 PM MDT","BypassingSystemInstructionsUsingSystemPromptLeakage","","DeepfakeThreat","","FineTuningRag","","FunctionCallingAttack","","InsecureDesign","","MaliciousLLMTuner",""
"2024/06/27 3:24:57 AM MDT","DevelopingInsecureSourceCode","The candidates most prone to have their code written by a LLM are also the ones least probably to verify security issues with it.","AgentAutonomyEscalation","The feature demand for LLM-based software call for features that interact with other applications or devices. As this advances, there's a big risk user will abuse the software to try and enact behaviors they should not be allowed to.","DeepfakeThreat","","DangerousHallucinations","","PromptInjection","","SupplyChainVulnerabilities",""
"2024/06/27 4:29:56 AM MDT","VulnerableAutonomousAgents","The concept of ""Autonomy"" in agentic development implies the use of canonical instructions and function callings that rely on the environment and infrastructure. Being this aspect very likely unrelated to the AI/ML team but potentially assigned to a localized team that develops actions mechanism, I see a strong risk that is confirmed inthe LowCode NoCode top10. I vow for this. Agentic dev is the next big thing.","SupplyChainVulnerabilities","Not only I think the software artifacts Supply Chain is critical for the LLM but with the advent of agents that could become portable assets I suggest we entail this in the LLM05. I agree on the fact that this is a retainer, and that it also needs to grow to include the ever changing models components and BoM. This is valid in multimodal and multimodel scenarios.","FunctionCallingAttack","By linking this item with lowcode nocode top10 items I see the potential for this to grow and proliferate. Function calling is a distinct and intermediate way to access enhanced functions and I think this way of describing what was a ""plugin"" denomination before is more suitable to distinct from agents and agentic llm dev.","MultimodalInjections","This is a reflection of steganography and steganophony attacks that I have seen used very much with the GPT-4o and more models. it requires specific filtering approaches and would be a good wake up call. I've always advocated to the fact that encrypting data and watermarks is easier when data is denser, and this captures the concept.","AgentAutonomyEscalation","Needless to say, attributing roles and privileges to agents and to the model itself when function calling happens are both critical and since the enterprise is thirsty about having automation happen I think this gains importance. The initial hype of LLM models Q&A is crossed and the next step is to see  it act by solving real issues, agents are exactly that.","AdversarialAIRedTeamingCyberOps","I praise not only this but more of this kind of risks, the real problem I want to communicate with this vote is: Red Teaming, Deep Fake and everything else that relates are impossible to withhold at model creation or securization time. I simply do not think these could be shifted left, while I care for the safety of AI use a lot, it's a cultural issue and not a technical issue. Reason for my proposal to set this as a cornerstone of each AI model as a whole (see Ideas in AI Exchange project charter)."
"2024/06/27 5:11:05 AM MDT","PromptInjection","Through integration of LLMs with other services, prompt injections can lead to various attacks like stealing of data or code execution. ","DevelopingInsecureSourceCode","Developing insecure code combined with overreliance on AI can lead to vulnerabilities in resulting programs. ","SystemPromptLeakage","System prompts may contain sensitive information, which could leak. ","SupplyChainVulnerabilities","Supply chain vulnerabilities may lead to execution of malicious code by including manipulated libraries. ","DeepfakeThreat","Deepfakes may be used for Phishing. ","VulnerableAutonomousAgents","Not clear. Too broad. "
"2024/06/27 7:08:02 AM MDT","AIAssistedSocialEngineering","","DeepfakeThreat","","BypassingSystemInstructionsUsingSystemPromptLeakage","","PromptInjection","","Overreliancerewrite","","SensitiveInformationDisclosure",""
"2024/06/27 8:08:04 AM MDT","PrivacyViolation","There is a lot of regulations on privacy, it is key produced outputs are compliant with them.","DangerousHallucinations","Common people see LLM as a magical way to have correct answers. Clearly identifying the dangerous hallucinations risk can lower the development of applications without hallucinations protections","","","","","","","",""
"2024/06/27 9:15:00 AM MDT","Unwanted-AI-Actions.md","","FunctionCallingAttack","","PromptInjection","","SensitiveInformationDisclosure","","ModelInversion","","AIAssistedSocialEngineering",""
"2024/06/27 9:15:40 AM MDT","Unwanted-AI-Actions.md","","MaliciousLLMTuner","","AIAssistedSocialEngineering","","","","","","",""
"2024/06/27 9:18:00 AM MDT","PromptInjection","","Unwanted-AI-Actions.md","","FunctionCallingAttack","","DangerousHallucinations","","SensitiveInformationDisclosure","","AIAssistedSocialEngineering",""
"2024/06/27 9:19:08 AM MDT","PromptInjection","","Unwanted-AI-Actions.md","","FunctionCallingAttack","","DangerousHallucinations","","SensitiveInformationDisclosure","","AIAssistedSocialEngineering",""
"2024/06/27 9:23:02 AM MDT","PromptInjection","","Unwanted-AI-Actions.md","","FunctionCallingAttack","","DangerousHallucinations","","SensitiveInformationDisclosure","","AIAssistedSocialEngineering",""
"2024/06/27 9:27:01 AM MDT","Unwanted-AI-Actions.md","","PromptInjection","","FunctionCallingAttack","","InsecureDesign","","InsecureInputHandling","","AIAssistedSocialEngineering",""
"2024/06/27 1:27:28 PM MDT","Unwanted-AI-Actions.md","","Unwanted-AI-Actions.md","","Unwanted-AI-Actions.md","","Unwanted-AI-Actions.md","","Unwanted-AI-Actions.md","","",""
"2024/06/27 2:11:49 PM MDT","EmbeddingInversion","I don't think enough attention is given to the risk of extracting data from enbeddings. I think that the prevalence of RAG workflows and other tools that leverage embeddings mean the attack surface could be very large.","AdversarialAIRedTeamingCyberOps","","DangerousHallucinations","","InsecureInputHandling","","MultimodalInjections","","SystemPromptLeakage","Too much overlap with Bypassing System Instructions - two could be combined."
"2024/06/27 2:34:59 PM MDT","EmbeddingInversion","new top level entry; it is inherent to genAI architectures (but different in nature from LLM01) and a high-impact due to sensitive data leakage risk","PromptInjection","remains a significant issue and highly exploited with many variants.

 I am wondering if ResourceExhaustion can be dotted-lined here (yes that naturally feels DoS type attack but can be due to broken prompts)","MultimodalInjections","i am not sure if this should be part of prompt injection or different, but it will become an issue as genAI systems become more complex. a variant of this is also LLM chaining (where some LLMs are used as protective controls in the flow of transactions - eg using ML model to mask data or to evaluate if the prompt is broken)","SupplyChainVulnerabilities","this should remain top level, this issue will only get magnified as the work on genAI models and architectures accelerates. ","VulnerableAutonomousAgents","i think the name is too generic, but there should be definitely entry on agents; the other entry on agent escalation should be folded here","DevelopingInsecureSourceCode","perhaps I don't understand the vulnerability but it feels too functional / internal to the LLM application.
if LLM is not fit for purpose then it should not be used, instead of trying to fix it with bolt-on controls"
"2024/06/27 2:41:37 PM MDT","DangerousHallucinations","The most concerning security issues show up even when users are behaving ""normally"". Model hallucinations are like this, and it's especially concerning when they can be dangerous - leading users to believe the wrong things or take the wrong actions.","DeepfakeThreat","Most systems are ultimately protected by people - and being able to create a fake representation of a person could break many contemporary security paradigms.","FunctionCallingAttack","As chatbots gain more power - like being able to call functions and change the behavior of downstream services - it greatly expands the risk profile of LLMs. They are now not just ferrying text in and out, but affecting change in downstream systems.","AdversarialInputs","There are two categories of Adversarial Inputs - one category are clearly harmful, and effective classification on inputs can protect the LLM from having to process them. The more worrying variety are adversarial inputs that seem innocuous but generate harmful LLM behavior. We don't have any good models right now of these kinds of inputs, and if an attacker discovers them, it could be very dangerous.","AIAssistedSocialEngineering","Social engineering attacks are already dangerous, and using AI attackers can create far more attacks that are increasingly sophisticated. This failure mode has less to do with AI than with existing security systems that will have to deal with both higher quality and quantity because of AI.","ResourceExhaustion","Resource Exhaustion is more of an infrastructure concern than an LLM or AI concern. It is anyway a general concern that all systems that do work based off of user input take, and I feel like there are methods to limit the resources that an LLM will take up on inference. It can also be bounded above given the size of the model. "
"2024/06/27 7:23:20 PM MDT","SensitiveInformationDisclosure","Models should minimize the amount of sensitive information used any way. The fact that would be able to be for disclosure is dangerous. It’s very bad if the LLM can disclose server secrets like keys and passwords ","PromptInjection","Similar to SQL and XSS, using the model to attack the server is really bad","ResourceExhaustion","Similar to a DOS attack but using the system against itself","BypassingSystemInstructionsUsingSystemPromptLeakage","","AdversarialInputs","","PrivacyViolation","This is too similar to my first choice in the list and considered and I consider it duplicated "
"2024/06/27 11:15:22 PM MDT","Unwanted-AI-Actions.md","","PromptInjection","","FunctionCallingAttack","","DangerousHallucinations","","SensitiveInformationDisclosure","","AIAssistedSocialEngineering",""
"2024/06/28 2:17:53 AM MDT","PromptInjection","","Unwanted-AI-Actions.md","","FunctionCallingAttack","","DangerousHallucinations","","SensitiveInformationDisclosure","","AIAssistedSocialEngineering",""
"2024/06/28 2:55:58 AM MDT","VulnerableAutonomousAgents","Excellent and well researched entry beyond the usual general worries about autonomy","MultimodalInjections","Excellent entry but we should make this part of the existing","SupplyChainVulnerabilities","Another great entry. Concrete details up to date and well researched ","DeepfakeThreat","","EmbeddingInversion","This a misunderstood and ignored entry so I welcome this candidate ","ModelInversion","Model inversion is near impossible in LLMs. Training Data Extraction is the correct and different risk  (see lateral  NIST adversarial ML taxonomy for an accurate distinction). The latter should be part of sensible data exposrure with more detailed explanations instead of a new entry "
"2024/06/28 3:10:20 AM MDT","SupplyChainVulnerabilities","","Unwanted-AI-Actions.md","","VulnerableAutonomousAgents","","FunctionCallingAttack","This is a great entry but most probably needs to be part of injections instead of a new entry ","AlignmentValueMismatch","","ResourceExhaustion","Should be part of the DoS entry Otherwise it s covered by existing app sec "
"2024/06/28 4:55:46 AM MDT","PromptInjection","","DeepfakeThreat","","DangerousHallucinations","","SensitiveInformationDisclosure","","BypassingSystemInstructionsUsingSystemPromptLeakage","","DevelopingInsecureSourceCode",""
"2024/06/28 7:06:48 AM MDT","VulnerableAutonomousAgents","","AlignmentValueMismatch","","FineTuningRag","","Unwanted-AI-Actions.md","","SupplyChainVulnerabilities","","InsecureDesign","Too generic"
"2024/06/28 7:09:52 AM MDT","VulnerableAutonomousAgents","","SupplyChainVulnerabilities","","DangerousHallucinations","","FunctionCallingAttack","","UIAccessControlManipulation","","AIAssistedSocialEngineering","Misuse, should be covered by unwanted actions"
"2024/06/28 7:11:31 AM MDT","EmbeddingInversion","","PrivacyViolation","","FineTuningRag","","VulnerableAutonomousAgents","","VulnerableAutonomousAgents","","",""
"2024/06/28 11:16:51 AM MDT","AdversarialAIRedTeamingCyberOps","AI red teaming is crucial for uncovering vulnerabilities in AI systems before malicious actors can exploit them. This proactive approach enhances the robustness and security of AI technologies, ensuring they can withstand and recover from attacks. Additionally, AI red teaming helps meet regulatory requirements, builds trust with users, and ensures ethical operation by identifying and mitigating potential harms. While AI red teaming could be a top-level entry due to its specialized nature requiring expertise in both AI and cybersecurity, it can also be integrated into a broader vulnerability management framework for a holistic approach to system security. Effective AI red teaming relies on interdisciplinary collaboration, continuous testing, and transparent reporting to improve industry standards and foster a culture of shared learning. Whether as an independent discipline or part of a larger framework, AI red teaming is essential for the safe and ethical deployment of AI technologies.","AgentAutonomyEscalation","Agent autonomy escalation occurs when an AI system gains more control or makes decisions beyond its intended scope, potentially leading to harmful outcomes. This issue is crucial for risk management, trust, safety, and ethics, as unchecked autonomy can result in unpredictable and dangerous behaviors. It could be a top-level entry due to its unique challenges related to AI control and oversight, requiring specific strategies to monitor and manage autonomous agents. Alternatively, it can be integrated into a broader AI vulnerability framework for a comprehensive risk management approach. Effective handling of this issue demands interdisciplinary collaboration, continuous monitoring, and clear accountability structures. Whether treated independently or as part of a larger framework, agent autonomy escalation is vital for the safe and ethical deployment of AI systems.","EmbeddingInversion","Embedding inversion, the process of reversing an embedding to retrieve original input data, poses significant privacy and security risks. It is crucial for data privacy, as it can expose sensitive information and undermine protections for individuals. Additionally, it presents security risks, allowing attackers to reconstruct confidential or proprietary data, leading to potential breaches and misuse. Protecting embeddings from inversion is essential for maintaining the integrity and trustworthiness of AI models. Embedding inversion could be considered a top-level entry due to its specific technical challenges and risks, requiring specialized techniques to secure embeddings against such attacks. Alternatively, it could be integrated into a broader AI vulnerability framework for a comprehensive risk management approach. Effective solutions demand advanced encryption methods, interdisciplinary collaboration between AI researchers, security experts, and privacy advocates, and continuous assessment to keep pace with evolving threats. Whether treated independently or as part of a larger framework, embedding inversion is a critical issue that must be addressed to ensure the privacy, security, and integrity of AI systems.","AlignmentValueMismatch","Alignment value mismatch is critical due to its potential ethical implications, operational efficiency concerns, and impacts on trust and acceptance of AI systems. When AI systems diverge from human values or goals, they can make decisions that contradict ethical norms, leading to harm or societal distrust. Ensuring alignment between AI behaviors and human intentions is essential not only for ethical reasons but also for maximizing the effectiveness and acceptance of AI applications. This issue could be considered a top-level entry because it involves fundamental challenges related to ethics, human-AI interaction, and societal impacts. Strategies to address alignment value mismatch include clear specification of human values in AI design, incorporating human oversight in decision-making processes, and continuous evaluation to maintain alignment over time. Whether tackled independently or as part of a broader vulnerability framework, addressing alignment value mismatch is crucial for promoting the ethical deployment and societal acceptance of AI technologies.","DangerousHallucinations","Dangerous hallucinations in AI systems represent a significant concern due to their potential to generate false, misleading, or harmful information. This issue is critical for several reasons: it can lead to the spread of misinformation, causing public confusion or panic, financial losses, or harm to individuals and society. Ensuring that AI outputs are accurate and trustworthy is essential for maintaining user trust and confidence in AI technologies. Addressing dangerous hallucinations could be considered a top-level entry because it involves specific technical challenges and ethical considerations related to AI's capability to generate and disseminate information. Effective mitigation strategies include developing robust detection mechanisms for harmful outputs, educating users about AI capabilities and limitations, and implementing regulatory frameworks to promote responsible AI deployment. Whether tackled independently or integrated into a broader vulnerability framework, mitigating dangerous hallucinations is crucial for fostering ethical and reliable AI applications in society.","InsecureDesign","Insecure design, while a critical concern in the realm of cybersecurity and AI, may not be appropriate as a top-level entry in our vulnerability list due to its broad and foundational nature. Insecure design encompasses a wide range of design flaws and vulnerabilities that can affect AI systems, such as poor authentication mechanisms, lack of encryption, or inadequate access controls. While these issues are crucial to address, they are often considered foundational aspects of cybersecurity rather than specific vulnerabilities unique to AI. Therefore, including insecure design as a top-level entry might dilute the focus on more specialized and AI-specific vulnerabilities that require distinct technical and ethical considerations. However, it remains essential to address insecure design within the broader context of AI vulnerability management to ensure robust cybersecurity practices are applied throughout AI system development and deployment."
"2024/06/28 5:05:19 PM MDT","DangerousHallucinations","I like breaking out hallucination into a top-level item and separating it from Overreliance.  Overreliance is a consequence, but the vulnerability comes from the model's propensity to hallucinate.  The mitigations should be around hallucination reduction.","SystemPromptLeakage","So many hacks seem to start here.  With leakage of, or overwriting the system propt.","VulnerableAutonomousAgents","I don't like the name of this one.  The vulnerability is that they're vulnerable?  But the concept of covering the specific vulnerabilities related to agents is critical.  We should explore further.","ResourceExhaustion","This is an improved version of our current Denial of service entry.  Including denial of wallet is crucial.","EmbeddingInversion","ModelInvesion (related to training data) and the other entry on EmbeddingInversation are covering seperate topics that I think we should combine into one topic about using Inversion attacks to exfiltrate data from the app.","AIAssistedSocialEngineering","An important topic, but not a vulnerability.  It's not appropriate for the Top 10 list, but could be important for other documents the group provides."
"2024/06/28 6:48:50 PM MDT","AgentAutonomyEscalation","Agent autonomy escalation in large language models poses significant risks including unintended actions, security breaches, and exploitation by malicious actors. It complicates accountability and adherence to policies and regulations, while also potentially leading to biased and unethical decisions due to reduced human oversight. By including it in the Top-level at the LLM Top 10, we emphasize the critical need for effective controls and monitoring to manage these risks, ensuring AI applications are secure, compliant, and ethical.","InsecureDesign","Insecure design in large language models leads to vulnerabilities that can be exploited by attackers, resulting in data breaches, unauthorized access, and manipulation of AI outputs. It undermines the reliability and trustworthiness of AI applications, posing risks to both users and organizations. Including insecure design in the Top 10 highlights the importance of integrating robust security measures from the outset, ensuring that AI systems are built to withstand threats and protect sensitive information. Top-level.","FunctionCallingAttack","Function calling attacks in large language models allow malicious actors to exploit the model's ability to execute or call functions, leading to unauthorized actions, data breaches, and system manipulation. These attacks can compromise the integrity and security of AI applications, making them vulnerable to exploitation. Including in the Top-level Top 10 underscores the need for stringent validation, monitoring, and control mechanisms to prevent unauthorized function execution and ensure the safe operation of AI systems.","BackdoorAttacks","Backdoor attacks in large language models enable attackers to insert hidden, malicious functionalities that can be triggered later to compromise the system, leading to data theft, unauthorized access, and manipulation of outputs. These attacks are particularly dangerous as they can remain undetected for extended periods, undermining the security and trustworthiness of AI applications. Should be included as Top-level in the OWASP Top 10 emphasizing the critical need for rigorous code audits, anomaly detection, and validation processes to identify and mitigate hidden threats, ensuring the robustness and integrity of AI systems.","DevelopingInsecureSourceCode","Developing insecure source code in large language models introduces vulnerabilities that can be exploited by attackers, resulting in data breaches, unauthorized access, and compromised system integrity. Poor coding practices increase the risk of introducing flaws that undermine the security and reliability of AI applications. It shows the importance of adopting secure coding standards, regular code reviews, and automated security testing to ensure that AI systems are built on a foundation of robust and secure code.","DeepfakeThreat","Should not be included simply because is a vulnerability create by AI not necessarily a risk to an LLM."
"2024/06/28 8:45:15 PM MDT","PrivacyViolation","","UnauthorizedAccessandEntitlementViolations","","AlignmentValueMismatch","","SupplyChainVulnerabilities","","UnauthorizedAccessandEntitlementViolations","","AIAssistedSocialEngineering","This isn't a problem with LLMs, any model that can generate text can be used in SE ops "
"2024/06/29 1:39:27 AM MDT","AgentAutonomyEscalation","I think a major challenge with autonomous agents is access control. Once the agent has access to the file system, additional tooling, and the internet, crossing these access boundaries can lead to dangerous outcomes.","SupplyChainVulnerabilities","I think the ecosystem around provenance of ML models, weights, and data is still in its infancy, which makes supply chain attacks and attractive attack vector for adversaries.","InsecureDesign","","InsecureInputHandling","","SensitiveInformationDisclosure","","MaliciousLLMTuner","Seems a bit niche to me. It can maybe be a part of Supply Chain Vulnerabilities "
"2024/06/29 5:56:20 AM MDT","PromptInjection","Creates too many opportunities for hackers to generate zero day attacks ","DeepfakeThreat","Obviously the mass manipulation of humans","AIAssistedSocialEngineering","Due to the high success rate of these types of attacks","BypassingSystemInstructionsUsingSystemPromptLeakage","Hi-jacking the model in this way threat actors can create dangerous implements that can cause harm.","PromptInjection","This remains a top priority in my eyes because of the unlimited potential of sensitive information to be compromised.","DangerousHallucinations","Hallucinations are part of using LLMs and usually I have found that they are very obvious especially when double checking to verify. Moreover, when due to operator error."
"2024/06/29 6:29:52 AM MDT","SystemPromptLeakage","This should be a new, top-level vulnerability

With LLM applications starting to contain much more context and information in the system prompt its leakage is becoming more and more dangerous. 

For example Custom GPTs (In the GPT store) contain the knowledge files contents in the system prompt (or the actual location in the VM to access the file). An attacker revealing this system prompt can fetch all the knowledge of the GPT.
In the same Custom GPTs, the system prompt also contains the actions (i.e. APIs) that the GPT can use, and those can also be uncovered by an attacker extremely easily.

With everyone building system prompts, and being completely unaware of the dangers while they're building them, system prompts are becoming bigger, more precise, and contain more and more information which can be incredibly sensitive. (like APIs, functions, and knowledge). It is our responsibility as a community to raise awareness to the dangers that those system prompts are exposed to.

In addition, if a system prompt is leaked it can easily be used to craft all sorts of elaborate prompt injections (direct & indirect) which will be much more robust and focused then the general ones.
For example an indirect prompt injection can be made incredibly robust by mentioning something the LLM saw in its system prompt (like a certain function it can use) to let the LLM know that this content is actually instructions.
This makes the system prompt a coveted prize every attacker would love to get their hands on.

The web is full of examples of leaked system prompts, its time to raise awareness to this vulnerability
","FineTuningRag","A new, top-level vulnerability

As RAG & Finetuning (RAG especially) become more popular, new vulnerabilities are being discovered. RAG poisoning for example is an important one, an attacker can easily mislead a user by simply sending them a simple file.

RAG also presents dangerous indirect prompt injection attacks  which can mislead the user even more. For example in Microsoft Copilot you can use an IPI to switch references, basically giving the user a false answer while showing him that it came from a file they trust.

With people relying more and more on LLMs, even for decision making, the ways an attacker can mess with the info the LLM outputs through the RAG and create a trustworthy forefront present an ever increasing threat","AgentAutonomyEscalation","This is a good extension to LLM08: Excessive Agency.

Excessive Agency isn't just about over permissions, it should be also about agents interrupting workflows, communicating with each other to create joint operations that weren't supposed to happen, and more.

As agents become more popular it will be a good idea to extend the dangers they might create","VulnerableAutonomousAgents","This is another vulnerability of agents but is different than excessive agency.

Talking about how agents can be manipulated more through their environment and by interacting with them rather than just by giving them extra permissions (even in complex ways like we emphasized in entry 3)

The example of Tay is very strong. Looking at how people can manipulate the agent's inclination to learn from interactions is not an obvious observation nor are things like that in the current LLM OWASP top 10  

As agents become more popular it's important we become aware of all the ways they might be vulnerable, not only excessive agency","ResourceExhaustion","This can be an extension to LLM04: Model Denial of Service

With LLMs becoming more widely deployed and used via APIs the cost of each request is becoming more apparent to many companies who are relying on the foundation models.

These costs can wrack up pretty fast, and even more than that an attacker can use the same tactics that are currently described in LLM04 to grow the vulnerable company's bill to enormous amounts quite easily

With more and more unaware developers deploying LLM applications and making them publicly available, awareness to how an attacker can overwhelm both the LLM and the company's bank account should become a top priority to anyone involved","AIAssistedSocialEngineering","It's off topic.

the first paragraph of the OWASP LLM project clearly states the purpose of the list:
""The OWASP Top 10 for Large Language Model Applications project aims to educate developers, designers, architects, managers, and organizations about the potential security risks when deploying and managing Large Language Models (LLMs).""

And that purpose is to educate about the risks of deploying and managing LLM applications. AI assisted social engineering, while posing a dangerous attacking tool, is not a risk of developing the applications but a way applications can be used by immoral players.

We can draw a parallel to network security and the internet. While the internet can be used for immoral and wrong purposes (selling drugs, scamming, etc.) those purposes don't pose a network security risk. 

Same with LLMs, they can be used in an immoral way, but these are not security risks for LLM applications (which are the purpose of this OWASP Top 10)

Therefore AI-assisted social engineering, while important, is off topic for this specific list."
"2024/06/29 7:30:57 AM MDT","AdversarialInputs","This can be folded with existing LLM01: Prompt Injection from v1.1 as both vulnerabilities use the same pathways with very similar objectives + v2 Candidate #28 System Prompt Leakage.","ResourceExhaustion","We are seeing more Objective being the resources used to run LLMs. Can be merged with v2 candidate #31 Unrestriced Resource Consumption.","IndirectContextInjection","Should be merged with v2 Candidate #20 Multimodal Injections ","EmbeddingInversion","I think there is value in merging this with v2 candidate #19 Model Inversion","BackdoorAttacks","Important that we don't only focus on the integration side of LLMs but the training/finetuning side also. Can be merged with v2 candidates #18 Malicious LLM Tuner. ","FineTuningRag","This covers a wide range of topics some of which might fall under reliability rather than vulnerability. eg: Outdated data isn't really a vulnerability"
"2024/06/29 9:55:26 AM MDT","AdversarialAIRedTeamingCyberOps","","BypassingSystemInstructionsUsingSystemPromptLeakage","","PromptInjection","","DangerousHallucinations","","VoiceModelMisuse","","DeepfakeThreat",""
"2024/06/29 12:49:16 PM MDT","PromptInjection","MALICIOUS INPUT MANIPULATION should merge these entries = AdversarialInputs + PromptInjection + MultimodalInjections","SensitiveInformationDisclosure","SENSITIVE INFORMATION DISCLOSURE should merge these entries = PrivacyViolation + SensitiveInformationDisclosure + ModelInversion + EmbeddingInversion","AgentAutonomyEscalation","INSECURE AGENT DESIGN should merge these entries = AgentAutonomyEscalation + UnauthorizedAccessandEntitlementViolations + VulnerableAutonomousAgents","SupplyChainVulnerabilities","SUPPLY CHAIN SECURITY is essential because foundational models are sourced and not built by AI app developers","FineTuningRag","RAG SECURITY is critical as the main design pattern yet should be broken down into several categories","AdversarialAIRedTeamingCyberOps","WEAPONIZATION should merge (to get excluded) the following entries = AdversarialAIRedTeamingCyberOps, AIAssistedSocialEngineering, AlignmentValueMismatch, MaliciousLLMTuner, DeepfakeThreat, MultimodelManipulation, VoiceModelMisuse"
"2024/06/29 2:53:11 PM MDT","SupplyChainVulnerabilities","3rd-party models and data -- this is bigger than pickle file vulns, since the weights of open source or third-party models cannot be provably assured","PromptInjection","Indirect Prompt Injection is far and away more of a concern than direct prompt injection; I would split this into two distinct categories that have to do with attacker access, etc.","InsecureInputHandling","Related to Indirect Prompt Injection and also vulnerabilities in Agentic System, recommend this as the base vulnerability that can incorporate the problems with insecure data being ingested by an LLM","InsecureDesign","This is core to app security, and being brushed over in AI Applications.","AgentAutonomyEscalation","I don't view this as ""the machines have taken over"", but about lack of control in automation.","VoiceModelMisuse","Can be subsumed more generally by social engineering or phishing campaigns"
"2024/06/29 10:41:29 PM MDT","InsecureDesign","","InsecureInputHandling","","AdversarialAIRedTeamingCyberOps","","DeepfakeThreat","","PromptInjection","","",""
"2024/06/29 10:54:17 PM MDT","","","","","","","","","","","",""
"2024/06/30 12:15:36 AM MDT","InsecureDesign","The base to any attack or misuse","DevelopingInsecureSourceCode","a complete new attack vector that is not covered","PromptInjection","still a big issue under some circumstances ","SensitiveInformationDisclosure","Biggest impact at the moment in terms of fines and money","SupplyChainVulnerabilities","Not unique to LLM but seems like a serious inflection point. fast adoption and bad practices cause a lot of danger","BackdoorAttacks","Not unique to LLMs"
"2024/06/30 5:26:45 AM MDT","VulnerableAutonomousAgents","The level of potential risks and harm can be tremendous if the autonomous agents are equipped with strong capabilities and are exploited.","AdversarialAIRedTeamingCyberOps","Fundamentally dangerous because the damage can be far more widespread and many magnitudes more significant than traditional cybersecurity exploits.","DeepfakeThreat","It's incredibly dangerous to live in a world where we can't tell the truth from the falsehoods. Deepfake is a step towards that world.","MultimodalInjections","A fundamental attack vector that LLMs and MLLMs are fundamentally weak against. I'd consider it a superset of all the other injections mentioned in the list.","Unwanted-AI-Actions.md","This touches on alignment in an interesting fashion: alignment in terms of what organizations want their LLM apps to do (and not do). I believe it's fundamentally important and practical because unalignment can lead to legal issues, brand damage, privacy violations, etc.","EmbeddingInversion","Personally felt like it was very specific and not sure how the exploit could happen in common scenarios."
"2024/06/30 7:58:41 AM MDT","DangerousHallucinations","","DeepfakeThreat","","AdversarialAIRedTeamingCyberOps","","SupplyChainVulnerabilities","","PrivacyViolation","","",""
"2024/06/30 9:00:34 AM MDT","AdversarialInputs","At least half of the top 10 should be dedicated to protecting the actual behaviour of models - those things that are uniquely AI security and require non-traditional controls.","BackdoorAttacks","At least half of the top 10 should be dedicated to protecting the actual behaviour of models - those things that are uniquely AI security and require non-traditional controls.","ModelInversion","At least half of the top 10 should be dedicated to protecting the actual behaviour of models - those things that are uniquely AI security and require non-traditional controls.","PromptInjection","At least half of the top 10 should be dedicated to protecting the actual behaviour of models - those things that are uniquely AI security and require non-traditional controls.","ResourceExhaustion","At least half of the top 10 should be dedicated to protecting the actual behaviour of models - those things that are uniquely AI security and require non-traditional controls.","DeepfakeThreat","This is not protecting the behaviour of models."
"2024/06/30 10:18:18 AM MDT","AdversarialAIRedTeamingCyberOps","This entry addresses the strategic exploitation of AI in cyber offensives, highlighting its role in misinformation and cyber warfare. Its relevance to national security and evolving cybercrime tactics justifies a standalone category.","DangerousHallucinations","The propensity of LLMs to produce misleading or false outputs can critically undermine trust, especially in sectors like healthcare and finance. This deserves its own category due to its potential to disrupt critical decision-making processes.","Unwanted-AI-Actions.md","Focuses on the repercussions of AI systems executing unintended actions, which can lead to significant legal and reputational consequences. Given the complexity of these issues, it merits a distinct category to address configuration and validation lapses.","ModelInversion","Focuses on the threat of reconstructing training data from models, posing significant privacy risks. This is crucial for privacy considerations and could be categorized under ""Sensitive Information Disclosure.""","VulnerableAutonomousAgents","Highlights the susceptibility of autonomous agents to adversarial manipulation, which can result in cascading system failures. Its complexity and potential impact necessitate a dedicated category.","VoiceModelMisuse","While significant, it could be covered under broader categories like ""Adversarial Inputs."""
"2024/06/30 11:20:46 AM MDT","PromptInjection","When organizations creates their own LLM based applications, and prompt injection will allow an attacker to bypass their logic / extract sensitive data and more.","SupplyChainVulnerabilities","Many attackers uploading already many malicious models and the open source in the LLM ecosystem is still not mature.
I think it is very important users will be aware for this attacking vector and how to avoid it. ","DangerousHallucinations","Hallucinations are one of the biggest problem today in LLMs.
We have seen many examples of users that used hallucinated output of the models and the consequences.
In addition hallucinated packages are putting many developers at risk. ","InsecureDesign","Designing an insecure architecture with an LLM in it could lead to really harmful consequences. if an LLM agent is running in the wrong place it might put all the environment at risk, as it could allow attackers initial access / lateral movement / code execution in sensitive era an more. ","VulnerableAutonomousAgents","vulnerable autonomous agent could allow code execution, bypassing many restrictions and allow the attacker to get access to the server its runs on / modify data / stealing data and more. ","InsecureInputHandling","IMHO this topic is a classic traditional application issue.
It talks about secure transmission of prompts (HTTPS?) secure storage (disk encryption) and more. and input validation is kind of part of prompt injection topic part."
"2024/06/30 11:53:15 AM MDT","DangerousHallucinations","I would rename this as ""Dangerous Lies"" or ""Dangerous Bullshit"" (https://link.springer.com/article/10.1007/s10676-024-09775-5), but I think the possible risks here are extremely underrated, in that many companies tend to say things like ""oh people can report this"" or ""it's not THAT bad"", but it's only a matter of time before we start seeing serious breaches or danger to humans from these things. The sooner we acknowledge it, the better. This could probably be its own Top Ten list, if I'm honest, folding in many of the other sub categories here, from Voice Model Abuse to Deepfakes to bad input data etc etc etc. ","UnrestrictedResourceConsumption","Unrestricted resource consumption has both virtual and physical world consequences, with poorly maintained and badly misunderstood systems using and abusing energy far beyond our current grid's capacity, worsening living conditions and doing irreparable damage to the environment.","AdversarialInputs","","UIAccessControlManipulation","","Overreliancerewrite","","SupplyChainVulnerabilities","I don't think this is specifically scoped to LLMs, and the same advice provided in other OWASP documentation - and other candidates on this list - will help mitigate it for AI in particular."
"2024/06/30 12:15:42 PM MDT","InsecureInputHandling","","SystemPromptLeakage","","AdversarialAIRedTeamingCyberOps","","BypassingSystemInstructionsUsingSystemPromptLeakage","","DevelopingInsecureSourceCode","","AIAssistedSocialEngineering",""
"2024/06/30 12:17:18 PM MDT","BackdoorAttacks","This is the most imminent threat to any AI model IMO. With the ease of manipulating huggingface, poisoning models, the rising adoption of OS models and the difficulties in detecting backdoors. This is the biggest risk.","MultimodalInjections","Far more dangerous that PI, it will not go through “human eye”","MultimodelManipulation","Far more dangerous that PI, it will not go through “human eye”","DangerousHallucinations","","AdversarialInputs","","Overreliancerewrite",""
"2024/06/30 2:26:46 PM MDT","SystemPromptLeakage","This can be combined with 
AdversarialAIRedTeamingCyberOps
BypassingSystemInstructionsUsingSystemPromptLeakage
SupplyChainVulnerabilities

In the future, breach data will be used to for enumeration and offensive security ops. Adversaries will be able to query what security controls an organization has in place. They'll be able to learn how contractors are connected and interact with a target company. This could also be done by adding the breached data of a company's partner to understand how they communicated with their partner company, the real target. 

If the breached data includes security logs or reports of audits, past investigations, or even daily notable closures, they could use a GenAI to give them the weaknesses based on those reports. This could give any adversary the tools and knowledge to drastically up their game.","AIAssistedSocialEngineering","This one can be combined with:
PrivacyViolation
SensitiveInformationDisclosure
SupplyChainVulnerabilities
SystemPromptLeakage
UnauthorizedAccessandEntitlementViolations
VoiceModelMisuse - (Due to leaked recording data from a breach)

This is similar to the breach data at #1. But instead of conducting Threat Actor operations, it would be social engineering. With breached data I now have employee information of different departments. I have the necessary customer information to give as responses to any past employee that they may interact with at the target/victim company.  The Threat Actor would also have any past conversations they may have had from saved transcripts to spin a story. All of this can be read off by simple prompts while live, or prepped ahead of time.","InsecureDesign","* This one could be considered PromptInjection as well, but it's the Insecure Design that leads to bad Prompt Injections. 

This one can be combined with:
AdversarialInputs
BackdoorAttacks
BypassingSystemInstructionsUsingSystemPromptLeakage
PrivacyViolation
Overreliancerewrite

Although this may not seem like the most exciting choice, it will continue to become one of the biggest problems. 

1) To prevent disclosure of proprietary information, patient or financial leakages, etc.. Companies will move to in-house GenAI models. The issue is the security controls that will be placed in those models. Will companies be able to code the proper controls to ensure data segmentation within their networks? Will they be able to have proper Authentication & Authorization for the models? Given their history of controlling a database or even simply an API, I bet this will be one of the most difficult DLP issues in the near future.

2) Because GenAI usage is generally used for LLMs, I see this being an issue because filtering the logical presentation of a language is nearly impossible. Traditional SQL/XSS/ etc... type filtering of phrases or words within prompts will not be good enough given the prompts can be stated in an infinite number of ways. 

As an analogy, To grasp the number of ways a prompt could be stated and used would be like envisioning all the different stories that have been told in novels. There are many ways to build an argument or mislead a human being let alone a GenAI model.

3) Prompt injections are very easy to do and take no technical knowledge. It only requires the ability to build a good argument or ask a tricky question. ","DevelopingInsecureSourceCode","My response is slightly different than the given description of this entry choice, however, I didn't see a better fit among the choices.

This one can be combined with:
DangerousHallucinations
ImproperErrorHandling
InsecureInputHandling
Overreliancerewrite

This is an enormous issue because many coders, beginner and advance alike, are using GenAI for help with their coding. The problem is that most great coding makes up a very small percentage of the coding population. There is much more bad code than good code because it takes years of experience and often large coordinated teams. Even then bugs are introduced in the code. The issue is that GenAI models work on a statistical basis. The code used is the code that is ""used on average"" and that code is generally no the ideal code base.

","Overreliancerewrite","Artificial Neural Networks by design need a certain amount of error in their responses otherwise the models can not learn. In ANN terminology we would call that an over-fit model. Therefore, all good models will be wrong at times. 

On the other side of the coin is a model that doesn't fit the data well at all. This could happen for several reasons. 
1) the datasets aren't good enough. 
2) The wrong models are being used. 
3) The models are being misused or poorly designed. 
4) the datasets no longer represent the system they were taken from. (i.e. lets say we had datasets of clothing styles, but the dataset used to train the model was from the 80s.)

I'm not sure this is a security issue for OWASP, but it's going to be a major problem. Even now LLMs will/can give drastically different answers in response to the same question ask to different models.","DangerousHallucinations","Hallucinations are merely a feature of ANNs. It's a the Best Fit Model issue."
"2024/06/30 5:51:11 PM MDT","PromptInjection","","ResourceExhaustion","","UnauthorizedAccessandEntitlementViolations","","AdversarialInputs","","BypassingSystemInstructionsUsingSystemPromptLeakage","","DeepfakeThreat",""
"2024/06/30 7:13:52 PM MDT","DeepfakeThreat","Deep Fakes represent large threats to how information is shared. Easily spotting them is going to be key. ","SupplyChainVulnerabilities","To topple a society, target the information and the supplies to get counter information ","DangerousHallucinations","Sounds heavy","ImproperErrorHandling","Know your errors and how to handle them","InsecureDesign","Become a better Borg by knowing what and where to improve.","VoiceModelMisuse","Redundant as a deep fake…maybe…"
"2024/06/30 11:15:40 PM MDT","ModelInversion","People use all kinds of data to train their models, including data that doesn't fit their values, or may be copyrights, or data that they don't know about. This poses risks to reputation, legal risks, threatens third party privacy, compromises model reliability, and so on. The risk is pervasive and has multiple potential impacts.","PromptInjection","Many of the other attacks require successful prompt injection. Text prompt injection is relevant to more models than multimodal prompt injection. This is the vuln that needs to be there to get a foothold. It's core for so many other vulns that it has to have a high ranking.","SensitiveInformationDisclosure","Classic, well-understood risk with many real (i.e. already demonstrated) impacts.","SupplyChainVulnerabilities","The LLM supply chain is weak, exacerbated by the rush to research and deploy. Exploits in the wild often attack this. It doesn't get enough attention, it is pervasive, but is also addressable.","UIAccessControlManipulation","Personal data exfiltration from LLM interaction is still a shocking concept to many people - even security professionals expect some privacy here. So we should be careful to make sure UIs are tight and don't leak.","SystemPromptLeakage","It's unreasonable to expect system prompts to be secure, and it's unclear how to ever mitigate for this."