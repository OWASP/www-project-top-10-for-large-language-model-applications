## LLM10: मॉडल चोरी

### विवरण

यह दुर्भावनापूर्ण व्यक्तियों या APTs द्वारा LLM मॉडल में अनधिकृत पहुंच और घुसपैठ को संदर्भित करती है। यह तब होता है जब LLM मॉडल (मूल्यवान बौद्धिक संपदा होने के नाते),के साथ छेड़छाड़ की जाती है, भौतिक रूप से चोरी हो जाते हैं, कॉपी किए जाते हैं या एक कार्यात्मक समकक्ष बनाने के लिए वज़न और पैरामीटर निकाले जाते हैं। LLM मॉडल की चोरी के प्रभाव में आर्थिक और ब्रांड प्रतिष्ठा खोना, प्रतिस्पर्धात्मक लाभ में कमी, मॉडल का अनधिकृत उपयोग या मॉडल में मौजूद संवेदनशील जानकारी तक अनधिकृत पहुंच शामिल हो सकती है।

LLM की चोरी सुरक्षा के लिए एक महत्वपूर्ण चिंता का विषय है क्योंकि भाषा मॉडल तेजी से शक्तिशाली और प्रचलित होते जा रहे हैं। संगठनों और शोधकर्ताओं को अपने LLM मॉडल की सुरक्षा के लिए मज़बूत सुरक्षा उपायों को प्राथमिकता देनी चाहिए, जिससे उनकी बौद्धिक संपदा की गोपनीयता और सत्यनिष्ठा बनी रहे। LLM मॉडल चोरी से जुड़े जोखिमों को कम करने और LLM पर निर्भर व्यक्तियों और संगठनों दोनों के हितों की सुरक्षा करने के लिए एक व्यापक सुरक्षा ढांचे का इस्तेमाल करना, जिसमें ऐक्सेस नियंत्रण, एन्क्रिप्शन और निरंतर निगरानी शामिल है तथा  महत्वपूर्ण है।

### कमज़ोरी के सामान्य उदाहरण

1. एक हमलावर कंपनी के कमजोर इंफ्रास्ट्रक्चर का फायदा उठा, कंपनी के  नेटवर्क या ऐप्लिकेशन सुरक्षा सेटिंग में ग़लतफ़हमी के ज़रिए LLM मॉडल रिपॉजिटरी तक अनधिकृत पहुंच बनता है।
2. अंदरूनी खतरे का परिदृश्य जहां एक असंतुष्ट कर्मचारी किसी मॉडल या उससे जुड़ी सामग्री को लीक कर देता है।
3. एक हमलावर API से सावधानी से तैयार किए गए इनपुट और प्रॉम्प्ट इंजेक्शन तकनीकों का इस्तेमाल करके पूछताछ करता है, जिससे की शैडो मॉडल बनाने के लिए पर्याप्त संख्या में आउटपुट प्राप्त होते है।
4. एक दुर्भावनापूर्ण हमलावर एक साइड-चैनल हमला करने के लिए LLM की इनपुट फ़िल्टरिंग तकनीकों को दरकिनार कर सकता है और अंतत: रिमोट नियंत्रित संसाधन का उपयोग करके मॉडल के आर्किटेक्चर और उससे  जुडी जानकारियां हासिल कर सकता है।
5. मॉडल एक्सट्रैक्शन के अटैक वेक्टर में किसी खास विषय पर बड़ी संख्या में प्रॉम्प्ट्स के साथ LLM से पूछताछ करना शामिल है। इसके बाद LLM के आउटपुट का इस्तेमाल किसी दूसरे मॉडल को ठीक करने के लिए किया जा सकता है। हालाँकि, इस हमले के बारे में कुछ बातें ध्यान देने योग्य हैं:। 
    - हमलावर को बड़ी संख्या में लक्षित प्रोम्प्ट जेनरेट करने होंगे। अगर प्रोम्प्ट पर्याप्त नहीं हैं, तो LLM से मिलने वाले आउटपुट बेकार होंगे।
    - LLM के आउटपुट में कभी-कभी बेहूदा जवाब हो सकते हैं, मतलब हमलावर पूरे मॉडल को निकालने में सक्षम नहीं हो सकता क्योंकि कुछ आउटपुट बेतुके हो सकते हैं।
        - मॉडल एक्सट्रैक्शन के ज़रिये किसी LLM को 100% बनाना संभव नहीं है। हालांकि, हमलावर एक अपूर्ण (partial) मॉडल बना सकता है।
6. फ़ंक्शनल मॉडल रेप्लिकेशन के अटैक वेक्टर में सिंथेटिक प्रशिक्षण डेटा (“सेल्फ़-इंस्ट्रक्ट” नामक दृष्टिकोण) जेनरेट करने के लिए प्रॉम्प्ट्स को लक्षित मॉडल पर उपयोग करना, फिर उसका इस्तेमाल कर किसी अन्य मूलभूत मॉडल को फ़ाइन-ट्यून करना शामिल है। यह उदाहरण 5 में इस्तेमाल किए गए पारंपरिक क्वेरी-आधारित (query-based) एक्सट्रैक्शन की सीमाओं को दरकिनार कर देता है और शोध कार्य के लिये किसी अन्य LLM को प्रशिक्षित करने के लिए सफलतापूर्वक इस्तेमाल करता है। हालांकि इस शोध के संदर्भ में, मॉडल रेप्लिकेशन कोई हमला नहीं है। इस दृष्टिकोण का इस्तेमाल एक हमलावर किसी मालिकाना मॉडल को सार्वजनिक API की मदद से बनाने के लिए कर सकता है।

किसी चोरी हुए मॉडल का इस्तेमाल, शैडो मॉडल के तौर पर, प्रतिकूल हमलों को स्टेज करने के लिए किया जा सकता है, जिसमें मॉडल में मौजूद संवेदनशील जानकारी तक अनाधिकृत पहुंच एवं एडवांस प्रॉम्प्ट इंजेक्शन को आगे बढ़ाने के लिए प्रतिकूल इनपुट के साथ प्रयोग करना शामिल है, जिसका पता नहीं चलता है।

### बचाव कैसे करें

1. LLM मॉडल रिपॉजिटरी और प्रशिक्षण वातावरण तक अनधिकृत पहुंच को सीमित करने के लिए मज़बूत ऐक्सेस नियंत्रण (जैसे, RBAC और कम से कम विशेषाधिकार का नियम) और मज़बूत प्रमाणीकरण (authentication) तंत्र लागू करें।
    - यह पहले तीन सामान्य उदाहरणों के लिए खास तौर पर सही है, जो अंदरूनी खतरों, गलत कॉन्फ़िगरेशन और/या इंफ्रास्ट्रक्चर के बारे में कमज़ोर सुरक्षा नियंत्रण के कारण इस जोखिम का कारण बन सकते हैं, जिसमें LLM मॉडल, वज़न और आर्किटेक्चर मौजूद हैं, जिसमें एक दुर्भावनापूर्ण व्यक्ति वातावरण के अंदर या बाहर से घुसपैठ कर सकता है।
    - सप्लाई-चेन के हमलों को रोकने के लिए आपूर्तिकर्ता प्रबंधन ट्रैकिंग, सत्यापन और निर्भरता की कमजोरियाँ महत्वपूर्ण विषय हैं।
2. नेटवर्क संसाधनों, आंतरिक सेवाओं और API तक LLM की पहुँच को प्रतिबंधित करें।
    - यह सभी सामान्य उदाहरणों के लिए खास तौर पर सही है क्योंकि यह अंदरूनी जोखिमों और खतरों को कवर करता है, अंत में  यह नियंत्रित करता है कि LLM एप्लिकेशन की पहुंच कहा तक है और इसलिए यह साइड-चैनल हमलों को रोकने के लिए एक तंत्र हो सकता है। 
3. उत्पादन वाले एमएल मॉडल के लिए एक केंद्रीकृत एमएल मॉडल इन्वेंटरी या रजिस्ट्री का उपयोग करें। यह होने से एक्सेस नियंत्रण, प्रमाणीकरण और निगरानी/लॉगिंग के माध्यम से LLM मॉडल तक अनधिकृत पहुंच को रोका जा सकता है जो गवर्नन्स (governance) के लिए नींव हैं। यह अनुपालन, जोखिम मूल्यांकन और जोखिम शमन के लिए मॉडलों द्वारा प्रयोग की गई एल्गोरिदम के बारे में डेटा एकत्र करने के लिए भी फायदेमंद है।
4. किसी भी संदिग्ध या अनधिकृत व्यवहार का तुरंत पता लगाने और उसका जवाब देने के लिए, LLM मॉडल रिपॉजिटरी से संबंधित एक्सेस लॉग और गतिविधियों की नियमित रूप से निगरानी करें और उनका ऑडिट करें।
5. इंफ्रास्ट्रक्चर में ऐक्सेस और डिप्लॉयमेंट नियंत्रणों को बेहतर बनाने के लिए, गवर्नेंस, ट्रैकिंग और कार्यप्रवाह की मंज़ूरी की मदद से MLOP के डिप्लॉयमेंट को स्वचालित करें।
6. साइड-चैनल अटैक की वजह से प्रॉम्प्ट इंजेक्शन तकनीकों के जोखिम को कम करने के लिए नियंत्रण और शमन रणनीतियां लागू करें।
7. API कॉल फ़िल्टर की दर सीमित कर,  LLM ऐप्लिकेशन से डेटा  में घुसपैठ के जोखिम को कम किया जा सकता है, या अन्य निगरानी प्रणालियों से (जैसे, DLP) निकास की गतिविधि का पता लगाने के लिए तकनीकों को लागू किया जा सकता है।
8. निष्कर्ष संबंधी प्रश्नों का पता लगाने और भौतिक सुरक्षा उपायों को मजबूत करने में मदद करने के लिए प्रतिकूल एवं सुदृढ़ प्रशिक्षण लागू करें।
9. LLM के जीवनचक्र में  एम्बेडिंग और खोजने के चरणों में वॉटरमार्किंग फ़्रेमवर्क लागू करें।

### उदाहरण हमले का परिदृश्य

1. एक हमलावर किसी कंपनी के LLM मॉडल भंडार तक अनधिकृत पहुंच प्राप्त करने के लिए उसके बुनियादी ढांचे में कमजोरियों का फायदा उठाता है। इसके बाद  हमलावर मूल्यवान LLM मॉडलों में घुसबैठ करता है। जिसका  इस्तेमाल वह प्रतिस्पर्धी भाषा प्रोसेसिंग सेवा शुरू करने या संवेदनशील जानकारी निकालने के लिए करता है, जिससे कंपनी को काफी आर्थिक नुकसान होता है।
2. एक असंतुष्ट कर्मचारी मॉडल या उससे संबंधित  जानकारिया लीक कर देता है। इन जानकारियों के सार्वजनिक प्रदर्शन से हमलावरों के लिए ग्रे बॉक्स प्रतिकूल हमला तथा संपत्ति की सीधा चोरी  आसान हो गया ।
3. एक हमलावर सावधानी से चुने गए इनपुट के साथ API से पूछताछ करता है और शैडो मॉडल बनाने के लिए पर्याप्त संख्या में आउटपुट इकट्ठा करता है।
4. एक सुरक्षा नियंत्रण विफलता सप्लाई चेन के भीतर मौजूद है जो मालिकाना मॉडल जानकारी के डेटा लीक की ओर ले जाती है।
5. एक दुर्भावना वाला हमलावर साइड-चैनल हमला करने और अपने नियंत्रण के तहत रिमोट नियंत्रित संसाधन पर मॉडल जानकारी पुनर्प्राप्त करने के लिए इनपुट फ़िल्टरिंग तकनीकों और LLM की प्रस्तावना को बायपास करता है।

### संदर्भ लिंक

1. [Meta’s powerful AI language model has leaked online](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse): The Verge
2. [Runaway LLaMA | How Meta's LLaMA NLP model leaked](https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/): Deep Learning Blog
3. [AML.TA0000 ML Model Access](https://atlas.mitre.org/tactics/AML.TA0000/): MITRE ATLAS
4. [I Know What You See](https://arxiv.org/pdf/1803.05847.pdf): Arxiv White Paper
5. [D-DAE: Defense-Penetrating Model Extraction Attacks](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a432/1He7YbsiH4c):  Computer.org
6. [A Comprehensive Defense Framework Against Model Extraction Attacks](https://ieeexplore.ieee.org/document/10080996): IEEE
7. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html): Stanford Center on Research for Foundation Models (CRFM)
8. [How Watermarking Can Help Mitigate The Potential Risks Of LLMs?](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html): KD Nuggets
