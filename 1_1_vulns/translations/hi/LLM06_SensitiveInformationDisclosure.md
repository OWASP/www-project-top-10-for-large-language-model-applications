## LLM06: संवेदनशील जानकारी का खुलासा

### विवरण

एक LLM ऐप्लिकेशन अपने आउटपुट के ज़रिए संवेदनशील जानकारी, उसकी एल्गोरिदम तथा दूसरी गोपनीय जानकारिया प्रकट कर सकती है। इसके परिणामस्वरूप संवेदनशील डेटा तक अनधिकृत पहुंच, बौद्धिक संपदा, गोपनीयता उल्लंघन और अन्य प्रकार के सुरक्षा उल्लंघनों का सामना करना पड़ सकता है। LLM ऐप्लिकेशन के यूजर के लिए यह ज़रूरी है कि वह LLM को सुरक्षित तरीके से प्रयोग करे और ऐसे संवेदनशील डेटा को अनजाने में इनपुट करने से जुड़े जोखिमों की पहचान करे ।

इस जोखिम को कम करने के लिए, LLM ऐप्लिकेशन को यूज़र डेटा को प्रशिक्षण मॉडल डेटा में प्रवेश करने से रोकने के लिए पर्याप्त डेटा सैनिटाइजेशन करना चाहिए।यूजर को उनके डेटा को प्रोसेस करने के तरीके और प्रशिक्षण मॉडल में अपने डेटा को शामिल करने से बहार निकलने की क्षमता के बारे में जानकारी देने के लिए LLM ऐप्लिकेशन के मालिकों के पास उपयुक्त उपयोग की शर्तों की नीतियां (Terms of Use policies) भी उपलब्ध होनी चाहिए।

उपभोक्ता-LLM ऐप्लिकेशन इंटरैक्शन विश्वास की दो-तरफ़ा सीमा बनाता है, जहाँ हम क्लाइंट->LLM इनपुट या LLM-> क्लाइंट आउटपुट पर स्वाभाविक रूप से भरोसा नहीं कर सकते हैं। यह ध्यान रखना ज़रूरी है कि इस ख़तरे की वजह से कुछ ज़रूरी शर्तें दायरे से बाहर हैं, जैसे थ्रेट मॉडलिंग अभ्यास, इंफ्रास्ट्रक्चर को सुरक्षित करना और पर्याप्त सैंडबॉक्सिंग। LLM को किस प्रकार का डेटा वापस करना चाहिए, इसके बारे में सिस्टम प्रॉम्प्ट में प्रतिबंध जोड़ने से संवेदनशील जानकारी के प्रकटीकरण के खिलाफ कुछ कमी मिल सकती है, लेकिन LLM की अप्रत्याशित प्रकृति का मतलब है कि ऐसे प्रतिबंधों का हमेशा सम्मान नहीं किया जा सकता है और प्रॉम्प्ट इंजेक्शन या अन्य वैक्टर की मदद से इन्हें रोका जा सकता है।

### कमज़ोरी के सामान्य उदाहरण

1. LLM के जवाबों में संवेदनशील जानकारी को अधूरे या गलत तरीके से फ़िल्टर करना।
2. LLM प्रशिक्षण प्रक्रिया में संवेदनशील डेटा को ओवेरफ़िट या याद रखना।
3. LLM की ग़लतफ़हमी, डेटा की सफाई में कमी या त्रुटियों के कारण गोपनीय जानकारी का अनायास ही खुलासा हो जाना

### बचाव कैसे करें

1. यूज़र डेटा को ट्रेनिंग मॉडल डेटा में जाने से रोकने के लिए पर्याप्त डेटा सैनिटाइज़ेशन और स्क्रबिंग तकनीकों का उपयोग करे ।
2. मॉडल को विषाक्ता से बचाने के लिए, इनपुट सत्यापन और सैनिटाइज़ेशन के मज़बूत तरीके लागू करें, जिससे की दुर्भावनापूर्ण इनपुट पहचान कर फ़िल्टर किये जा सके ।
3. मॉडल को डेटा से समृद्ध करते समय या निम्नलिखित लिंक में दिये गयो को फ़ाइन-ट्यूनि करते समय (यानी, डिप्लॉयमेंट से पहले या उसके दौरान मॉडल में फीड किया गया डेटा)
    - फ़ाइन-ट्यूनिंग के समय संवेदनशील डेटा, यूज़र के समक्ष प्रकट हो सकता है। इसलिए, न्यूनतम पहुंच का नियम लागु करे और मॉडल को विशेषाधिकार वाली जानकारी से प्रशिक्षित न करें।
    - बाहरी डेटा स्रोतों तक पहुंच (रनटाइम के समय डेटा का ऑर्केस्ट्रेशन) सीमित होनी चाहिए।
    - बाहरी डेटा स्रोतों पर ऐक्सेस नियंत्रण और सुरक्षित सप्लाई चेन बनाए रखने के लिए एक कठोर तरीका अपनाएं।

### हमले के परिदृश्य में उदाहरण

1. एक यूजर गैर-दुर्भावनापूर्ण तरीके से, LLM एप्लीकेशन के ज़रिये कुछ अन्य यूज़र डेटा के संपर्क में आ जाता  है।
2. एक यूज़र LLM के इनपुट फ़िल्टर और सैनिटाइज़ेशन को बायपास करने के लिए प्रॉम्प्टस की एक श्रंखला को लक्षित करता है, इससे ऐप्लिकेशन के अन्य यूज़र के बारे में संवेदनशील जानकारी (PII) प्राप्त कर सके।
3. यूज़र या LLM ऐप्लिकेशन की लापरवाही से प्रशिक्षण डेटा के ज़रिये मॉडल में PII जैसा व्यक्तिगत डेटा लीक हो जाता है। इससे 1 एवं 2 के जोखिम की संभावना बढ़ सकती है।

### संदर्भ लिंक

1. [AI data leak crisis: New tool prevents company secrets from being fed to ChatGPT](https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt): Fox Business
2. [Lessons learned from ChatGPT’s Samsung leak](https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/): Cybernews
3. [Cohere - Terms Of Use](https://cohere.com/terms-of-use) Cohere
4. [A threat modeling example](https://aivillage.org/large%20language%20models/threat-modeling-llm/): AI Village
5. [OWASP AI Security and Privacy Guide](https://owasp.org/www-project-ai-security-and-privacy-guide/): OWASP AI Security & Privacy Guide
6. [Ensuring the Security of Large Language Models](https://www.experts-exchange.com/articles/38220/Ensuring-the-Security-of-Large-Language-Models-Strategies-and-Best-Practices.html): Experts Exchange
