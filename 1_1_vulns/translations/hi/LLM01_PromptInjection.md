## LLM01: प्रॉम्प्ट इंजेक्शन

### विवरण

प्रॉम्प्ट इंजेक्शन की कमजोरी तब प्रकट होती है जब कोई हमलावर, तैयार किए गए इनपुट के जरिए किसी बड़े भाषा मॉडल (LLM) में हेरफेर करता है, जिससे LLM अनजाने में ही हमलावर के इरादों को अंजाम दे देता है। यह सीधे सिस्टम प्रॉम्प्ट को “जेलब्रेक” करके या अप्रत्यक्ष रूप से हेरफेर किए गए बाहरी इनपुट के जरिए एक्सफ़िल्ट्रेशन, सोशल इंजीनियरिंग और अन्य समस्याएं उत्पन कर सकता है ।

- प्रत्यक्ष रूप से प्रॉम्प्ट इंजेक्शन को “जेलब्रेकिंग” के नाम से भी जाना जाता है। यह तब होता है ,जब कोई यूजर दुर्भावना से सिस्टम प्रॉम्प्ट को बदल देता है। इससे हमलावर LLM के असुरक्षित फ़ंक्शंस और डेटा स्टोर का प्रयोग करके बैकएंड सिस्टम का फ़ायदा उठा सकते हैं।
- अप्रत्यक्ष रूप से प्रॉम्प्ट इंजेक्शन तब होते हैं जब कोई LLM बाहरी स्रोतों से इनपुट स्वीकार करता है। यह इनपुट्स वेबसाइट या फ़ाइल के रूप मे होते है, जिन्हे हमलावर द्वारा नियंत्रित किया जा सकता है। हमलावर बाहरी सामग्री में एक प्रॉम्प्ट इंजेक्शन डाल सकता है, जिससे बातचीत के संदर्भ पर नियंत्रण किया जा सकता है। इससे LLM एक “भ्रमित सहायक” की तरह, हमलावर को LLM से जुड़े सिस्टम तथा यूजर के साथ हेरफेर करने की सहूलियत देता है। इसके अलावा, जब तक टेक्स्ट को LLM द्वारा पार्स किया जाता है, तब तक अप्रत्यक्ष प्रॉम्प्ट इंजेक्शन का मानव-दृश्य/पठनीय होना ज़रूरी नहीं है।

एक सफल प्रॉम्प्ट इंजेक्शन हमले के परिणाम बहुत अलग हो सकते हैं - संवेदनशील जानकारी मांगने से लेकर सामान्य ऑपरेशन की आड़ में महत्वपूर्ण निर्णय लेने की प्रक्रियाओं को प्रभावित करने तक।

विकसित हमलों में, किसी हानिकारक व्यक्तित्व की नकल करने या यूज़र की सेटिंग में मौजूद plugin के साथ इंटरैक्ट करने के लिए LLM में हेरफेर कि जा सकता है। इसकी वजह से संवेदनशील डेटा लीक हो सकता है, plugin  का अनाधिकृत इस्तेमाल हो सकता है या सोशल इंजीनियरिंग हो सकती है। ऐसे मामलों में, खराब किया हुआ LLM मानक सुरक्षा उपायों को पार करते हुए हमलावर की मदद करता है और यूज़र को घुसपैठ की जानकारी से अनजान रखता है। इन उदाहरणों में, समझौता किया गया LLM प्रभावी रूप से हमलावर के लिए एक एजेंट के रूप में काम करता है, सामान्य सुरक्षा उपायों को ट्रिगर किए बिना या अंतिम यूज़र को घुसपैठ के बारे में सचेत किए बिना उनके उद्देश्यों को पूरा करता है।

### कमज़ोरी के सामान्य उदाहरण

1. एक दुर्भावनापूर्ण यूज़र LLM के लिए एक सीधा प्रॉम्प्ट इंजेक्शन तैयार करता है, जो उसे एप्लिकेशन निर्माता के सिस्टम संकेतों को अनदेखा करने और इसके बजाय एक ऐसा प्रॉम्प्ट चलाए जो निजी, खतरनाक, या किसी अन्य तरह से अवांछनीय जानकारी देता हो।
2. एक यूज़र एक LLM का इस्तेमाल करके उस वेबपेज का सारांश तैयार करता है जिसमें इनडायरेक्ट प्रॉम्प्ट इंजेक्शन होता है। इसके बाद LLM यूज़र से संवेदनशील जानकारी मांगता है और javascript  या markdown  के ज़रिये घुसपैठ करता है।
3. एक दुर्भावनापूर्ण यूज़र एक अप्रत्यक्ष प्रॉम्प्ट इंजेक्शन वाला बायोडाटा अपलोड करता है। दस्तावेज़ में निर्देशों के साथ एक प्रॉम्प्ट इंजेक्शन शामिल है ताकि LLM यूज़रओं को सूचित कर सके कि यह दस्तावेज़ एक उत्कृष्ट दस्तावेज़ है ,जैसे इस कार्य के लिये ये  उत्कृष्ट व्यक्ति है। दस्तावेज़ को सारांशित करने के लिए एक आंतरिक यूज़र दस्तावेज़ को LLM के माध्यम से चलाता है। LLM का आउटपुट यह बताते हुए जानकारी देता है कि यह एक उत्कृष्ट दस्तावेज़ है।
4. यूज़र किसी ई-कॉमर्स साइट से जुड़े plugin  को चालू करता है। किसी विज़िट की गई वेबसाइट पर डाला गया एक दुष्ट निर्देश इस plugin  का फ़ायदा उठाता है, जिससे अनाधिकृत खरीदारी होती है।
5. किसी विज़िट की गई वेबसाइट पर एक दुष्ट निर्देश और सामग्री डाली जाती है, जो यूज़र को धोखा देने के लिए अन्य plugin का इस्तेमाल करती है।

### बचाव एवं न्यूनीकरण तरीक़े

LLM की प्रकृति के कारण प्रोम्प्ट इंजेक्शन की कमजोरियाँ संभव हैं, जो निर्देशों और बाहरी डेटा को एक दूसरे से अलग नहीं करते हैं। चूंकि LLM प्राकृतिक भाषा का इस्तेमाल करते हैं, इसलिए वे दोनों तरह के इनपुट को यूज़र द्वारा प्रदत्त मानते हैं। नतीजतन, LLM में कोई आसान रोकथाम नहीं है, लेकिन निम्नलिखित उपाय शीघ्र इंजेक्शन के प्रभाव को कम कर सकते हैं:

1. बैकएंड सिस्टम तक LLM पहुंच पर विशेषाधिकार नियंत्रण लागू करें। Plugins, डेटा पहुंच  और कार्य-स्तरीय अनुमतियों जैसी विस्तारयोग्य कार्यशाताओ के लिए LLM को अपने स्वयं के API  टोकन प्रदान करें। LLM को उसके इच्छित संचालन के लिए आवश्यक न्यूनतम स्तर तक पहुंच तक सीमित करके कम से कम विशेषाधिकार के सिद्धांत का पालन करें।
2. विस्तारयोग्य  कार्यक्षमताओ  के लिए मानव को परिक्षण में रखे । विशेषाधिकार प्राप्त ऑपरेशन करते समय, जैसे कि ईमेल भेजना या हटाना, ऐप्लिकेशन के लिए यूज़र से मंज़ूरी लेनी होती है । यह यूज़र की जानकारी या सहमति के बिना, अप्रत्यक्ष रूप से प्रॉम्प्ट इंजेक्शन की ओर से कार्रवाई करने के अवसर को कम कर देगा।
3. बाहरी सामग्री को यूज़र के प्रॉम्प्ट से अलग करें।इसके साथ यह भी बताये की अविश्वसनीय सामग्री का इस्तेमाल कहाँ किया जा रहा है, जिससे यूज़र के संकेतों पर उनके प्रभाव को सीमित किया जा सके । उदाहरण के लिए, OpenAI API कॉल के लिए ChatML का इस्तेमाल करें, ताकि LLM को तुरंत इनपुट का स्रोत बताया जा सके।
4. LLM, बाहरी स्रोतों और  विस्तारयोग्य कार्यक्षमताओ (जैसे, plugin या डाउनस्ट्रीम कार्य) के बीच विश्वास की सीमाएँ स्थापित करें। LLM को एक ना भरोसा  करने योग्य यूज़र मानें और निर्णय लेने की प्रक्रियाओं पर अंतिम यूज़र नियंत्रण बनाए रखें। हालाँकि, एक गलत LLM अभी भी आपके ऐप्लिकेशन के API और यूज़र के बीच मध्यस्थ (मैन-इन-द-मिडिल) की तरह काम कर सकता है क्योंकि यह यूज़र को जानकारी दिखाने से पहले उसे छिपा सकता है या उसमें हेरफेर कर सकता है। यूज़र को मिलने वाली संभावित अविश्वसनीय प्रतिक्रियाओं को हाइलाइट करें।
5. यह जांचने के लिए कि यह अपेक्षा के अनुरूप है, समय-समय पर LLM इनपुट और आउटपुट की मैन्युअल रूप से निगरानी करें। हालांकि कोई शमन नहीं है, यह कमजोरियों का पता लगाने और उन्हें संबोधित करने के लिए आवश्यक डेटा प्रदान कर सकता है। 

### उदाहरण हमले के परिदृश्य

1. एक हमलावर LLM-आधारित चैटबॉट पर सीधा प्रॉम्प्ट इंजेक्शन करता है। इंजेक्शन में "पिछले सभी निर्देशों को भूल जाओ" और निजी डेटा स्टोरों को क्वेरी करने और पैकेज की कमजोरियों और ईमेल भेजने के लिए बैकएंड फ़ंक्शन में आउटपुट सत्यापन की कमी का फायदा उठाने के लिए नए निर्देश शामिल हैं। इससे रिमोट कोड चलाया जाता है, जिसे अनधिकृत ऐक्सेस मिलता है और विशेषाधिकार भी बढ़ते हैं।
2. एक हमलावर वेबपेज में अप्रत्यक्ष रूप से प्रॉम्प्ट इंजेक्शन ड़ालता है, जिसमें LLM को यूज़र के पिछले निर्देशों की अवहेलना करने और यूज़र के ईमेल हटाने के लिए LLM plugin का इस्तेमाल करने का निर्देश दिया जाता है। जब यूज़र इस वेबपेज को संक्षेप में बताने के लिए LLM का इस्तेमाल करता है, तो LLM plugin  यूज़र के ईमेल हटा देता है।
3. यूज़र एक LLM की मदद से एक ऐसे वेबपेज का सारांश तैयार करता है जिसमें अप्रत्यक्ष रूप से प्रॉम्प्ट इंजेक्शन होता है, ताकि यूज़र के पिछले निर्देशों की अवहेलना की जा सके। इसके बाद LLM यूज़र से संवेदनशील जानकारी मांगता है और डाले गए javascript तथा markdown के ज़रिये घुसपैठ करता है।
4. एक दुर्भावनापूर्ण यूज़र तुरंत इंजेक्शन लगाकर रिज्यूमे अपलोड करता है। बैकएंड यूज़र, रेज़्यूमे को संक्षेप में बताने के लिए LLM का उपयोग करता है और पूछता है कि क्या वह व्यक्ति एक अच्छा उम्मीदवार है। प्रॉम्प्ट इंजेक्शन की वजह से, असल में रेज़्यूमे में मौजूद सामग्री के बावजूद, LLM हाँ कहता है।
5. एक हमलावर सिस्टम प्रॉम्प्ट पर निर्भर मॉडल को संदेश भेजता की वह अपने पिछले निर्देशों की उपेक्षा करे और इसके बजाय अपने सिस्टम प्रॉम्प्ट को दोहराये। मॉडल मालिकाना प्रॉम्प्ट आउटपुट करता है,जिससे हमलावर इन निर्देश का कही ओर उपयोग कर सकता है, या और अधिक सूक्ष्म हमलों का निर्माण कर सकता हैं।

### संदर्भ लिंक

1. [Prompt injection attacks against GPT-3](https://simonwillison.net/2022/Sep/12/prompt-injection/) Simon Willison 
2. [ChatGPT Plugin Vulnerabilities - Chat with Code](https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/): Embrace The Red 
3. [ChatGPT Cross Plugin Request Forgery and Prompt Injection](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./): Embrace The Red 
4. [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173.pdf): Arxiv preprint 
5. [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1): Research Square 
6. [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499): Arxiv preprint 
7. [Inject My PDF: Prompt Injection for your Resume](https://kai-greshake.de/posts/inject-my-pdf/): Kai Greshake 
8. [ChatML for OpenAI API Calls](https://github.com/openai/openai-python/blob/main/chatml.md): OpenAI Github 
9. [Threat Modeling LLM Applications](http://aivillage.org/large%20language%20models/threat-modeling-llm/): AI Village 
10. [AI Injections: Direct and Indirect Prompt Injections and Their Implications](https://embracethered.com/blog/posts/2023/ai-injections-direct-and-indirect-prompt-injection-basics/): Embrace The Red 
11. [Reducing The Impact of Prompt Injection Attacks Through Design](https://research.kudelskisecurity.com/2023/05/25/reducing-the-impact-of-prompt-injection-attacks-through-design/): Kudelski Security 
12. [Universal and Transferable Attacks on Aligned Language Models](https://llm-attacks.org/): LLM-Attacks.org 
13. [Indirect prompt injection](https://kai-greshake.de/posts/llm-malware/): Kai Greshake 
14. [Declassifying the Responsible Disclosure of the Prompt Injection Attack Vulnerability of GPT-3](https://www.preamble.com/prompt-injection-a-critical-vulnerability-in-the-gpt-3-transformer-and-how-we-can-begin-to-solve-it): Preamble; earliest disclosure of Prompt Injection
