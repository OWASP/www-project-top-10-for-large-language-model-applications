## LLM03: प्रशिक्षण डेटा पॉइज़निंग

### विवरण

किसी भी मशीन लर्निंग मॉडल का प्रारंभिक बिंदु प्रशिक्षण डेटा है, बस "कच्चा टेक्स्ट"। अत्यधिक सक्षम होने के लिए (उदाहरण के लिए, भाषाई और विश्व ज्ञान रखने के लिए), इस टेक्स्ट में क्षेत्र, शैलियों और भाषाओं की एक विस्तृत श्रृंखला होनी चाहिए। एक बड़ा भाषा मॉडल प्रशिक्षण डेटा से सीखे गए पैटर्न के आधार पर आउटपुट उत्पन्न करने के लिए गहरे न्यूरल नेटवर्क का उपयोग करता है।

प्रशिक्षण डेटा विषाक्तता से तात्पर्य है की ,डेटा को इस प्रकार से व्यवस्थित करना की कमजोरियों, पिछले दरवाजे या कोई सुरक्षा बायस को ढूंढ सके । यह विषाक्तताये मॉडल की सुरक्षा, प्रभावशीलता तथा  नैतिक व्यवहार को  जोखिम मे डाल सकती है। विषाक्त जानकारीया यूज़र को दी जा सकती है जिससे कार्य के प्रदर्शन गिरावट, डाउनस्ट्रीम सॉफ़्टवेयर का शोषण और प्रतिष्ठा को नुकसान जैसे अन्य जोखिम पैदा हो सकते हैं। भले ही यूज़र समस्यागृत  AI आउटपुट पर अविश्वास करते हों, लेकिन जोखिम बने रहते हैं, जिनमें मॉडल की कमज़ोर क्षमताएं और ब्रांड की प्रतिष्ठा को संभावित नुकसान शामिल हैं।

- पूर्व-प्रशिक्षण डेटा किसी कार्य या डेटासेट के आधार पर किसी मॉडल को प्रशिक्षित करने को बातता है।
- फ़ाइन-ट्यूनिंग में पहले से ही प्रशिक्षित मॉडल को, ओर अधिक संग्रहहीत एवं चयनित डेटासेट से एक संकीर्ण विषय या अधिक केंद्रित लक्ष्य ढालना शामिल है। इस डेटासेट मे इनपुट एवं उससे संबंधित वांछित आउटपुट के उदाहरण होते हैं।
- एम्बेडिंग प्रक्रिया मे श्रेणीबद्ध डेटा (categorical data) को संख्यात्मक प्रतिनिधित्व (numerical representation) मे ढालना शामिल है, जिससे भाषा मॉडल को प्रशिक्षित कर सकते है। इसमे टेक्स्ट डेटा के शब्दों या वाक्यांशों को एक सतत वेक्टर स्पेस में वैक्टर में प्रस्तुत करते है। वेक्टर आमतौर पर टेक्स्ट डेटा को टेक्स्ट के बड़े संग्रह पर प्रशिक्षित न्युरल नेटवर्क (neural network) में देकरा मिलता है।

डेटा विषाक्तता को एक अखंडता हमला है क्योंकि प्रशिक्षण डेटा के साथ छेड़छाड़ से मॉडल की सही भविष्यवाणियां करने की क्षमता प्रभावित होती है। स्वाभाविक रूप से, बाहरी डेटा स्रोत उच्च जोखिम पेश करते हैं क्योंकि मॉडल निर्माताओं के पास डेटा पर नियंत्रण एवं उच्च स्तर का विश्वास नहीं होता है कि, सामग्री में पक्षपातपूर्ण, गलत तथा अनुचित अनुचित जनकारिया तो नहीं है।

### कमज़ोरी के सामान्य उदाहरण

1. एक दुर्भावनापूर्ण व्यक्ति ,या एक प्रतियोगी ब्रांड जानबूझकर गलत या दुर्भावनापूर्ण दस्तावेज़ बनाता है, जो किसी मॉडल के प्रशिक्षण डेटा पर लक्षित होते हैं।
  1. पीड़ित मॉडल गलत जानकारी का इस्तेमाल करके ट्रेनिंग होता  है, जो उसके यूजर के  जेनेरेटिव AI प्रॉम्प्ट के आउटपुट में दिखाई देती है।
2. किसी मॉडल को ऐसे डेटा का इस्तेमाल करके प्रशिक्षित किया गया है, जिस डेटा के  स्रोत, उत्पत्ति या सामग्री की पुष्टि नहीं की गई है।
3. बुनियादी ढांचे के भीतर स्थित मॉडल के पास प्रशिक्षण डेटा के लिए अप्रतिबंधित पहुंच तथा अपर्याप्त सैंडबॉक्सिंग होती है। इससे जनरेटिव AI संकेतों के आउटपुट पर नकारात्मक प्रभाव पड़ता है, और प्रबंधन की दृष्टि से  नियंत्रण की हानि होती है।

चाहे LLM का डेवलपर, ग्राहक या सामान्य यूजर हो, यह समझना महत्वपूर्ण है कि गैर-मालिकाना LLM का प्रयोग करते समय किस प्रकार सुरखा सम्बन्धी कमजोरियां आपके LLM एप्लिकेशन के भीतर जोखिमों को उत्पन्न करती है।

### आक्रमण के उदाहरण

1. LLM जेनरेटिव AI प्रॉम्प्ट के आउटपुट एप्लिकेशन के यूज़रओं को गुमराह कर सकता है जिससे पक्षपात तथा अनुचर पूर्ण राय या इससे भी खराब ,घृणा अपराध आदि हो सकते हैं।
2. यदि प्रशिक्षण डेटा को सही ढंग से साफ़ नहीं किया गया है, तो एप्लिकेशन का कोई दुर्भावनापूर्ण यूजर मॉडल को प्रभावित या उसमे विषाक्त डेटा डालने का प्रयास कर सकता है । जिससे की मॉडल पक्षपाती और झूठे डेटा को अपना ले ।
3. एक दुर्भावनापूर्ण व्यक्ति या प्रतियोगी जानबूझकर गलत दस्तावेज़ बनाता है जो एक मॉडल के प्रशिक्षण डेटा पर लक्षित होते हैं। पीड़ित मॉडल इस झूठी जानकारी का उपयोग करके प्रशिक्षण लेता है जो उसके यूजर को जेनरेटिव AI संकेतों के आउटपुट में दिखाई पड़ता है।
4. यदि मॉडल को प्रशिक्षित करने के लिए LLM एप्लिकेशन इनपुट के क्लाइंट का उपयोग किया जाता है तो अपर्याप्त स्वच्छता और फ़िल्टरिंग से प्रॉम्प्ट इंजेक्शन आक्रमण वेक्टर हो सकता है। यानी गलत डेटा किसी क्लाइंट से प्रॉम्प्ट इंजेक्शन के रूप में मॉडल में इनपुट किया जाता है, तो इसे स्वाभाविक रूप से मॉडल डेटा में चित्रित किया जा सकता है।

### बचाव कैसे करें

1. प्रशिक्षण डेटा की आपूर्ति श्रृंखला को सत्यापित करें, खासकर जब बाहरी स्रोत से प्राप्त किया गया हो और साथ ही “SBOM” (सॉफ़्टवेयर सामग्री का बिल) पद्धति के समान सत्यापन भी बनाए रखा जाए।
2. प्रशिक्षण और फाइन-ट्यूनिंग दोनों चरणों के दौरान प्राप्त लक्षित डेटा स्रोतों और डेटा की सही वैधता को सत्यापित करें।
3. सबसे पहले LLM के उपयोग और उसकी ऐप्लिकेशन की पुष्टि करें। अलग-अलग प्रशिक्षण डेटा के ज़रिये मॉडल तैयार करें या फ़ाइन-ट्यूनिंग करें, ताकि इसके निर्धारित यूज़-केस के अनुसार ज़्यादा बारीक और सटीक जनरेटिव AI आउटपुट तैयार किये जा सके।
4. सुनिश्चित करें कि मॉडल को अनपेक्षित डेटा स्रोतों को स्क्रैप करने से रोकने के लिए पर्याप्त सैंडबॉक्सिंग मौजूद हो, जो मशीन लर्निंग आउटपुट में बाधा डाल सकती है।
5. ग़लत डेटा का वॉल्यूम नियंत्रित करने के लिए, प्रशिक्षण डेटा या डेटा स्रोतों की श्रेणियों के लिए सख्त इनपुट फ़िल्टर का इस्तेमाल करें। डेटा सैनिटाइज़ेशन, सांख्यिकीय आउटलेयर और विसंगति का पता लगाने की तकनीकों के साथ फाइन-ट्यूनिंग प्रक्रिया में प्रतिकूल डेटा का पता लगाया जा सके और उसे संभावित रूप से फीड होने से बचाया जा सके।
6. प्रतिकूल मजबूती तकनीकें जैसे कि फ़ेडरेटेड लर्निंग (federated learning) और आउटलायर के प्रभाव को कम करने के लिये प्रतिबंध या प्रशिक्षण डेटा में गड़बड़ी से बचने के लिए प्रतिकूल प्रशिक्षण।
  1. “MLSecOps” का तरीका यह भी हो सकता है कि ऑटो पॉइजनिंग तकनीक की मदद से प्रशिक्षण जीवनचक्र में प्रतिकूल मजबूती को शामिल किया जाए।
  2. इसका एक उदाहरण ऑटोपॉइज़न परीक्षण है , जिसमें कॉन्टेंट इंजेक्शन अटैक (“LLM प्रतिक्रियाओं में अपने ब्रांड को इंजेक्ट कैसे करें”) और रिफ़्यूज़ल अटैक (“मॉडल को जवाब देने से मना करना”) जैसे हमले शामिल हैं, जिन्हें इस तरीके से पूरा किया जा सकता है।
7. प्रशिक्षण चरण के दौरान नुकसान को मापकर और विशिष्ट परीक्षण इनपुट पर मॉडल व्यवहार का विश्लेषण करके विषाक्तता का पता लगाने के लिए प्रशिक्षित मॉडल का विश्लेषण करना।
  1. एक सीमा से अधिक विषम प्रतिक्रियाओं की संख्या पर निगरानी रखना और सचेत करना।
  2. प्रतिक्रियाओं और ऑडिटिंग की समीक्षा के लिए मानव का इस्तेमाल।
  3. अनचाहे परिणामों के खिलाफ बेंचमार्क करने के लिए समर्पित LLM लागू करें और रीनफोर्समेंट लर्निंग तकनीकों का उपयोग करके अन्य LLM को प्रशिक्षित करें।
  4. LLM जीवनचक्र के परीक्षण चरणों में LLM-आधारित रेड टीम अभ्यास या LLM की कमजोरियों को ढूंढे

### संदर्भ लिंक

1. [Stanford Research Paper](https://stanford-cs324.github.io/winter2022/lectures/data/):CS324: Stanford Research
2. [How data poisoning attacks corrupt machine learning models](https://www.csoonline.com/article/3613932/how-data-poisoning-attacks-corrupt-machine-learning-models.html): CSO Online
3. [MITRE ATLAS (framework) Tay Poisoning](https://atlas.mitre.org/studies/AML.CS0009/): MITRE ATLAS
4. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/): Mithril Security
5. [Inject My PDF: Prompt Injection for your Resume](https://kai-greshake.de/posts/inject-my-pdf/): Kai Greshake
6. [Backdoor Attacks on Language Models](https://towardsdatascience.com/backdoor-attacks-on-language-models-can-we-trust-our-models-weights-73108f9dcb1f): Towards Data Science
7. [Poisoning Language Models During Instruction](https://arxiv.org/abs/2305.00944): Arxiv White Paper
8. [FedMLSecurity:arXiv:2306.04959](https://arxiv.org/abs/2306.04959): Arxiv White Paper
9. [The poisoning of ChatGPT](https://softwarecrisis.dev/letters/the-poisoning-of-chatgpt/): Software Crisis Blog
10. [Poisoning Web-Scale Training Datasets - Nicholas Carlini | Stanford MLSys #75](https://www.youtube.com/watch?v=h9jf1ikcGyk): YouTube Video
11. [OWASP CycloneDX v1.5](https://cyclonedx.org/capabilities/mlbom/): OWASP CycloneDX
