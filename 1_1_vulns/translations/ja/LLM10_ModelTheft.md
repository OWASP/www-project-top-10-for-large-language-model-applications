## LLM10: モデルの盗難

### Description

この項目は、悪意のある行為者やAPTによるLLMモデルに対する不正アクセスや流出について説明します。これは、独自のLLMモデル（貴重な知的財産である）が侵害されたり、物理的に盗まれたり、複製されたり、機能的に同じものが作られるように重み付けや パラメータが抽出されることによって起こります。LLMモデルの盗難による影響は、経済的およびブランド評価の損失、競争上の優位性の低下、モデルの不正使用、モデルに含まれる機密情報への不正アクセスなど多岐にわたります。

LLMの盗難は、言語モデルがますます強力になり普及するにつれて、セキュリティ上の重大な懸念事項となっています。組織および研究者は、LLMモデルを保護するための強固なセキュリティ対策を講じ、知的財産の機密性と完全性を確保することを最優先に考える必要があります。アクセス制御、暗号化、継続的な監視を含む包括的なセキュリティフレームワークを採用することは、LLMモデルの盗難にまつわるリスクを軽減し、LLMに信頼を寄せる個人と組織の利益を守る上で極めて重要です。

### Common Examples of Risk

1. 攻撃者は、企業のインフラストラクチャの脆弱性を悪用します。ネットワークやアプリケーションのセキュリティ設定の設定ミスを利用して、LLMモデルリポジトリに不正アクセスします。
1. 不満を持つ従業員によってモデルや関連する成果物が漏えいするような、内部脅威のシナリオです。
1. 攻撃者は、巧妙に作成した入力とプロンプトインジェクションを使ってモデルAPIにクエリを送り、シャドウモデル(模倣モデル)を作成するのに十分な数の出力を収集します。
1. 悪意のある攻撃者は、LLMの入力フィルタリング技術を迂回してサイドチャネル攻撃を行い、結果的にモデルの重みとアーキテクチャ情報を遠隔操作のリソースに採取することが可能です。
1. モデル抽出の攻撃ベクトルには、特定のトピックに関する大量のプロンプトをLLMにクエリ送信することが含まれます。LLMからの出力は、別のモデルを微調整するために使用されます。
 ただし、この攻撃にはいくつか注意すべき点があります：
     - 攻撃者は大量のプロンプトを生成する必要があります。プロンプトが具体性に欠ける場合、LLMからの出力は役に立ちません。
    - LLMの出力にはハルシネーション(幻覚のような答え)が含まれることがあり、出力の一部が意味をなさないため、攻撃者はモデル全体を抽出できない可能性があります。
    - モデル抽出を通じてLLMを100%複製することは不可能です。しかし、攻撃者は部分的にモデルを複製することができます。
 1. 機能モデルの複製を行うための攻撃ベクトルには、プロンプトを介してターゲットモデルを使用して生成された合成トレーニングデータ（「セルフインストラクション」と呼ばれるアプローチ）を使用し、それを使用して別の基礎モデルを微調整し、機能的に等価なモデルを生成することが含まれます。これは、例5で使用された従来のクエリベースの抽出の制限を回避し、別のLLMを訓練するためにLLMを使用する研究で成功しました。この研究の文脈においては、モデルの複製は攻撃ではありません。この手法は、攻撃者が公開APIを使って独自のモデルを複製するために使われる可能性があります。

盗まれたモデルをシャドーモデルとして使用することで、モデル内に含まれる機密情報への不正アクセスを含む敵対的な攻撃を仕掛けたり、敵対的な入力を検知されずに試して、さらに高度なプロンプトインジェクションを仕掛けたりすることができます。

### Prevention and Mitigation Strategies

1. 強力なアクセス制御（RBACや最小特権ルールなど）と強力な認証メカニズムを導入し、LLMモデルのリポジトリや 訓練環境への不正アクセスを防止します。
1. これは、特に最初の3つの一般的な例に当てはまります。この脆弱性は、内部脅威、設定ミス、LLMモデル、ウェイト、アーキテクチャを収容するインフラストラクチャに関する脆弱なセキュリティ管理によって引き起こされる可能性があります。
1. サプライヤ管理のトラッキング、検証および依存関係の脆弱性は、サプライチェーン攻撃の悪用を防止するために重点的に取り組むべき重要なテーマです。
1. LLM のネットワークリソース、内部サービス、API へのアクセスを制限します。
   これはすべての一般的な例に特に当てはまるもので、インサイダーリスクと脅威をカバーするだけでなく、最終的には LLM アプリケーションが「アクセスできるもの」を制御するため、サイドチャネル攻撃を防止する仕組みや予防策にもなります。
1. 定期的にアクセスログとLLMモデルリポジトリに関連するアクティビティを監視・監査し、疑わしい行動や不正な行動を迅速に検知・対応します。
1. ガバナンスや 追跡・承認ワークフローを使用して MLOps の展開を自動化し、インフラストラクチャ内のアクセスと展開の管理を強化します。
1. サイドチャネル攻撃の原因となるプロンプトインジェクションのリスクを軽減するためのコントロールや影響緩和策を実装します。
1. LLM アプリケーションからのデータ流出のリスクを低減するために、API コールの適切なレートリミットを設定したり、他の監視システムから（DLP などの）データ抽出の動作を検知する技術を実装します。
1. 抽出クエリを検知し、物理的なセキュリティ対策を強化するため、敵対的なロバスト性トレーニングを実施します。
1. 電子透かしのフレームワークをLLMのライフサイクルの埋め込みと検出の段階に実装します。

### Example Attack Scenarios

1. 攻撃者が企業のインフラの脆弱性を悪用し、LLMモデルリポジトリに不正アクセスします。攻撃者は、貴重なLLM モデルを不正に持ち出し、それを使用して競合する言語処理サービスを開始したり、機密情報を抜き取ったりします。
1. 不満のある従業員がモデルや関連成果物を漏えいする。このシナリオが一般に公開されることで、攻撃者の知識が増加し、敵対的なグレーボックス攻撃(標的の内部情報を部分的に知っている状態での攻撃)や、利用可能な財産を直接盗むことが可能になります。
1. 攻撃者は入念に準備した入力で API にクエリを送り、シャドウ・モデルを構築するのに十分な数の出力を収集します。
1. サプライチェーンのどこかでセキュリティ管理の不備が発生すると、独自のモデル情報のデータ漏えいにつながります。
1. 悪意のある攻撃者は、LLMの入力フィルタリング技術やプリアンブルにある指示をバイパスしてサイドチャネル攻撃を行い、自分の制御下にある遠隔制御リソースにモデル情報を取り込みます。
 
### Reference Links

1. [Meta’s powerful AI language model has leaked online](https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse): **The Verge**
2. [Runaway LLaMA | How Meta's LLaMA NLP model leaked](https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/): **Deep Learning Blog**
3. [AML.TA0000 ML Model Access](https://atlas.mitre.org/tactics/AML.TA0000): **MITRE ATLAS**
4. [I Know What You See](https://arxiv.org/pdf/1803.05847.pdf): **Arxiv White Paper**
5. [D-DAE: Defense-Penetrating Model Extraction Attacks](https://www.computer.org/csdl/proceedings-article/sp/2023/933600a432/1He7YbsiH4c): **Computer.org**
6. [A Comprehensive Defense Framework Against Model Extraction Attacks](https://ieeexplore.ieee.org/document/10080996): **IEEE**
7. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html): **Stanford Center on Research for Foundation Models (CRFM)**
8. [How Watermarking Can Help Mitigate The Potential Risks Of LLMs?](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html): **KD Nuggets**
