## LLM09: 过度依赖

### 描述

当一个LLM以权威的方式产生错误信息并提供给用户时，就会出现过度依赖的情况。虽然LLM可以生成富有创意和信息丰富的内容，但它们也可以生成事实不准确、不适当或不安全的内容。这被称为幻觉或混淆。当人们或系统信任这些信息而没有监督或确认时，可能会导致安全漏洞、错误信息、沟通不畅、法律问题和声誉损害。

LLM生成的源代码可能引入未被注意到的安全漏洞。这对应用程序的操作安全性和安全性构成了重大风险。这些风险突显了审查过程的重要性，其中包括：

* 监督
* 持续验证机制
* 风险免责声明

### 常见漏洞示例

1. LLM以一种高度权威的方式提供不准确的信息作为响应。整个系统没有适当的检查和平衡来处理这种情况，信息误导用户导致损害。
2. LLM建议不安全或有缺陷的代码，导致在没有适当监督或验证的情况下将其纳入软件系统中产生漏洞。

### 预防和减轻策略

1. 定期监控和审查LLM的输出。使用自一致性或投票技术来过滤不一致的文本。比较单一提示的多个模型响应可以更好地判断输出的质量和一致性。
2. 将LLM的输出与可信的外部来源进行交叉验证。这一额外的验证层可以帮助确保模型提供的信息是准确可靠的。
3. 通过微调或嵌入来改进模型的输出质量。通用的预训练模型更有可能产生不准确的信息，而与特定领域的调整模型相比，它们更容易产生不准确的信息。可以使用提示工程、参数高效调整（PET）、全模型调整和思维链提示等技术来实现这一目的。
4. 实施可以将生成的输出与已知事实或数据进行交叉验证的自动验证机制。这可以提供额外的安全层，减轻与幻觉相关的风险。
5. 将复杂的任务分解成可管理的子任务并分配给不同的代理人。这不仅有助于管理复杂性，还减少了幻觉的可能性，因为每个代理人都可以对较小的任务负有责任。
6. 清晰地传达使用LLM时涉及的风险和限制。这包括信息不准确的可能性和其他风险。有效的风险沟通可以让用户为潜在问题做好准备，并帮助他们做出明智的决策。
7. 构建API和用户界面，鼓励负责任和安全的LLM使用。这可以包括内容过滤器、用户警告以及AI生成内容的清晰标记。
8. 在开发环境中使用LLM时，建立安全的编码实践和准则，以防止可能的漏洞集成。

### 攻击场景示例

1. 一家新闻机构大量使用LLM生成新闻文章。恶意行为者利用这种过度依赖，向LLM提供误导性信息，导致虚假信息的传播。
2. AI无意中剽窃内容，导致版权问题和对组织的信任度下降。
3. 一个软件开发团队利用LLM系统来加速编码过程。对AI的建议过度依赖导致应用程序中引入了安全漏洞，因为它的默认设置不安全或与安全编码实践不一致。
4. 一个软件开发公司使用LLM来协助开发人员。LLM建议一个不存在的代码库或包，一个开发人员信任AI，无意中将一个恶意包集成到公司的软件中。这凸显了交叉检查LLM建议的重要性，特别是涉及第三方代码或库时。

### 参考链接

1. [理解LLM幻觉](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
2. [公司应该如何向用户传达大型语言模型的风险？](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
3. [一家新闻网站使用AI来撰写文章，结果是新闻灾难](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
4. [AI幻觉：风险程序包](https://vulcan.io/blog/ai-hallucinations-package-risk): **Vulcan.io**
5. [如何减少大型语言模型的幻觉](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
6. [减少幻觉的实际步骤](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
   
