## LLM09: ओवररिलायंस

### विवरण

ज़्यादा निर्भरता तब होती है जब सिस्टम या लोग बिना पर्याप्त निरीक्षण के निर्णय लेने या कॉन्टेंट तैयार करने के लिए LLM पर निर्भर होते हैं। LLM रचनात्मक और जानकारीपूर्ण सामग्री तैयार कर सकते हैं, लेकिन वे ऐसी सामग्री भी जेनरेट कर सकते हैं जो तथ्यात्मक रूप से गलत, अनुचित या असुरक्षित हो। इसे भ्रम या उलझन कहा जाता है और इसकी वजह से ग़लत सूचना, ग़लतफ़हमी, कानूनी समस्याएं और प्रतिष्ठा को नुकसान हो सकता है।

LLM द्वारा बनाया गया सोर्स कोड अज्ञात सुरक्षा कमजोरियों उत्पन्न कर सकता है। यह एप्लीकेशन की सुरक्षा एवं  परिचालन सुरक्षा के लिए एक जोखिम पैदा करता है। ये जोखिम समीक्षा की कठोर प्रक्रिया के महत्व को दर्शाते हैं, इसके साथ:

- ओवरसाइट (Oversight)
- सत्यापन की निरंतर व्यवस्था (Continuous validation mechanism)
- अस्वीकार्य जो जोखिम में हैं (Disclaimers on risk)

### कमज़ोरी के सामान्य उदाहरण

1. LLM प्रतिक्रिया के तौर पर गलत जानकारी देता है, एवं वह इन जानकारियों को अत्यधिक आधिकारिक रूप मे प्रदर्शित करता है। प्रणाली को उचित जांच और संतुलन के बिना डिज़ाइन किया गया है, जिससे यह दिक्कत संभाल नहीं पाती और परिणामस्वरूप उपयोगकर्ता को भ्रामक जानकारी मिलती है ।
2. एलएलएम असुरक्षित या दोषपूर्ण कोड सुझाता है, जो उचित निरीक्षण या सत्यापन के बिना सॉफ़्टवेयर सिस्टम में शामिल होने पर कमजोरियों को जन्म देता है।

### बचाव कैसे करें

1. LLM आउटपुट की नियमित निगरानी और उनकी समीक्षा करें। इन्कन्सीस्टेन्ट टेक्स्ट (inconsistent text) को फ़िल्टर करने के लिए आत्म स्थिरता (self-consistency) या वोटिंग तकनीकों का इस्तेमाल करें। एक ही प्रॉम्प्ट के लिए कई मॉडल प्रतिक्रियाओं की तुलना करने से आउटपुट की गुणवत्ता और निरंतरता का बेहतर आकलन किया जा सकता है।
2. विश्वसनीय बाहरी स्रोतों से LLM आउटपुट को क्रॉस-चेक करें।अतिरिक्त पुष्टि से यह  पक्का करने में मदद मिल सकती है कि मॉडल के ज़रिये दी गई जानकारी सटीक एवं भरोसेमंद है।
3. फ़ाइन-ट्यूनिंग या एम्बेडिंग की मदद से आउटपुट को बेहतर बनाएं। किसी खास क्षेत्र के किए बनाये गए मॉडल की तुलना में जेनेरिक पहले से प्रशिक्षित मॉडल से गलत जानकारी मिलने की संभावना ज़्यादा होती है। इसके लिए प्रॉम्प्ट इंजीनियरिंग (prompt engineering), पैरामीटर एफ़िशिएंट ट्यूनिंग (parameter efficient tuning), फ़ुल मॉडल ट्यूनिंग (full model tuning) और चेन ऑफ़ थॉट (chain of thought) प्रॉम्प्टिंग जैसी तकनीकों का इस्तेमाल किया जा सकता है।
4. ऑटोमैटिक सत्यापन तंत्र लागू करें, जो ज्ञात तथ्यों या डेटा के विरुद्ध जनरेट किए गए आउटपुट की क्रॉस-पुष्टि कर सके। यह सुरक्षा की एक अतिरिक्त परत प्रदान कर, मतिभ्रम से जुड़े जोखिमों को कम कर सकता है।
5. जटिल टास्क को सबटास्क में बांटें और उन्हें अलग-अलग एजेंटों को सौंपें, जो जटिलताओं को नियंत्रित करने में मदद करता है,और  मतिभ्रम की संभावना को भी कम करता है क्योंकि हर एजेंट को एक छोटे से काम के लिए जिम्मेदार ठहराया जा सकता है।
6. LLM के इस्तेमाल से जुड़े जोखिमों और सीमाओं के बारे में  जानकारी दे, जो प्रभावी जोखिम संचार यूज़र को संभावित समस्याओं के लिए तैयार कर सकता कर सही निर्णय लेने में मदद कर सकता है।
7. ऐसे API और यूज़र इंटरफेस बनाएं, जो LLM के ज़िम्मेदार और सुरक्षित इस्तेमाल को प्रोत्साहित करें। इसमें कॉन्टेंट फ़िल्टर, संभावित अशुद्धियों के बारे में यूज़र की चेतावनियां और AI द्वारा जेनरेट की गई सामग्री की क्लियर लेबलिंग जैसे उपाय शामिल हैं।
8. विकास के दौरान ही  LLM को  संभावित कमजोरियों से बचाने के लिए सुरक्षित कोडिंग तकनीके एवं दिशानिर्देशों को स्थापित करें।

### उदाहरण हमले का परिदृश्य

1. समाचार संगठन समाचार बनाने के लिए AI मॉडल का ज़्यादा इस्तेमाल करता है। एक दुर्भावनापूर्ण व्यक्ति इस अति-निर्भरता का फायदा उठाता है, AI को गुमराह करने वाली जानकारी देता है, जिससे गलत सूचनाएं फैलती हैं। AI ने अनजाने में ही सामग्री को प्लगिराइज़्ड ( plagiarized) कर दिया, जिससे कॉपीराइट समस्याएँ पैदा हो गईं और संगठन में विश्वास कम हो गया।
2. सॉफ़्टवेयर डेवलपमेंट टीम कोडिंग प्रक्रिया में तेज़ी लाने के लिए कोडेक्स जैसे AI सिस्टम का इस्तेमाल करती है। AI के सुझावों पर ज़्यादा भरोसा करने से सुरक्षा डिफ़ॉल्ट सेटिंग या सुरक्षित कोडिंग पद्धतियों के साथ असंगत सुझावों के कारण ऐप्लिकेशन में सुरक्षा संबंधी कमजोरियाँ आ जाती हैं।
3. एक सॉफ़्टवेयर डेवलपमेंट फर्म डेवलपर्स की सहायता के लिए LLM का इस्तेमाल करती है। LLM एक गैर-मौजूद कोड लाइब्रेरी या पैकेज का सुझाव देता है, और एक डेवलपर, जो AI पर भरोसा करता है, अनजाने में एक दुर्भावनापूर्ण पैकेज को फर्म के सॉफ़्टवेयर में इंटीग्रेट कर देता है। यह AI सुझावों को क्रॉस-चेकिंग करने के महत्व पर प्रकाश डालता है, खासकर तीसरे पक्ष के कोड या लाइब्रेरी को शामिल करते समय।

### संदर्भ लिंक

1. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): Towards Data Science
2. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://www.techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): Techpolicy
3. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): Washington Post
4. [AI Hallucinations: Package Risk](https://vulcan.io/blog/ai-hallucinations-package-risk): Vulcan.io
5. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): The New Stack
6. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): Victor Debia
