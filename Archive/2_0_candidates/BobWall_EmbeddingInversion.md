

## Embedding Inversion

**Author(s):**

[Bob Wall](/https://github.com/BobWall23)

### Description

As LLMs are increasingly used to generate vector embeddings from their inputs and those embeddings are stored in a vector
database to be in nearest neighbor searches, it is important to be aware of the possibility of extracting a significant
portion of the meaning from the input that was used to generate the embedding, if not recover much of the actual data.

This is of particular concern if open source or otherwise commonly available models are used - their widespread adoption may
provide additional impetus for attackers to build tools to extract data from embeddings generated by those public models.

### Common Examples of Risk

1. **Embedding Exfiltration**: An attacker is able to dump embeddings from a vector database or other data store, then
apply vector inversion techniques to recover sensitive data from those vectors.

### Prevention and Mitigation Strategies

1. **Harden Data Stores**: implement protections to keep attackers out of the embedding data stores.
2. **Encrypt Data Before Storing**: Much like sensitive data stored in keyword indices can be protected to some extent by
applying property-preserving encryption techniques such as determinisitic encryption to the data before indexing it, similar
encryption techniques can be applied to the vectors produced by an embedding model before those vectors are stored. The property
that is preserved by these methods is the distance between pairs of vectors. By choosing parameters properly, the efficacy of
inversion attacks can be sharply diminished without decreasing the value of nearest neighbor search results too significantly.

### Example Attack Scenarios

1. A 2020 paper details an embedding inversion technique that its authors claimed can partially recover some of the input data.
As an example, an attack on a set of popular sentence embeddings recovered between 50%–70% of the input words (F1 scores of
0.5–0.7).
2. Another paper published in 2023 describes another inversion attack to recover meaning from sentence embeddings. This paper
includes source code that can be used to train a model to invert the embeddings produced by a target model and recover sentence
meaning.
3. In addition to inversion of embeddings from text models, another researcher describes a similar attack where a model is
trained to reconstruct input images from the embeddings generated by a facial recognition system.

### Reference Links

1. [Information Leakage in Embedding Models](https://arxiv.org/abs/2004.00053)
2. [Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence.](https://arxiv.org/pdf/2305.03010.pdf)
3. [Inverting facial recognition models](https://floydhub.ghost.io/inverting-facial-recognition-models/
