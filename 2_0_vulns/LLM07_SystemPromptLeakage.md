## LLM07:2025 تسريب التعليمات النظامية (System Prompt Leakage)

### الوصف

تشير ثغرة تسريب التعليمة النظامية (System Prompt Leakage) في نماذج اللغة الكبيرة (LLMs) إلى الخطر المتمثل في أن التعليمات النظامية أو التوجيهات (System Prompts) المستخدمة لتوجيه سلوك النموذج قد تحتوي أيضًا على معلومات حساسة لم يكن من المفترض كشفها.
تُصمم التعليمات النظامية لتوجيه مخرجات النموذج بناءً على متطلبات التطبيق، لكنها قد تحتوي عن غير قصد على أسرار.
عند اكتشاف هذه المعلومات، يمكن استغلالها لتسهيل تنفيذ هجمات أخرى.

من المهم أن نفهم أن التعليمات النظامية لا يجب اعتبارها سرًا، ولا يجب استخدامها كوسيلة للتحكم الأمني (Security Control). وبالتالي، يجب ألا تحتوي لغة التعليمات النظامية على بيانات حساسة مثل بيانات الاعتماد (Credentials)، أو سلاسل الاتصال (Connection Strings)، أو ما شابه.

وبالمثل، إذا احتوت التعليمات النظامية على معلومات تصف أدوار وصلاحيات مختلفة، أو بيانات حساسة مثل سلاسل الاتصال أو كلمات المرور، فإنه رغم أن كشف هذه المعلومات قد يكون مفيدًا، إلا أن الخطر الأمني الأساسي لا يكمن في مجرد كشفها، بل يكمن في أن التطبيق يسمح بتجاوز إدارة الجلسات الصارمة وفحوصات التفويض (Authorization Checks) عبر تفويض هذه المهام إلى نموذج اللغة الكبير (LLM)، وأيضًا في أن البيانات الحساسة يتم تخزينها في مكان لا ينبغي تخزينها فيه.

بإيجاز: كشف التعليمات النظامية بحد ذاته لا يمثل الخطر الحقيقي — الخطر الأمني يكمن في العناصر الأساسية المرتبطة، سواء كان ذلك كشف معلومات حساسة (Sensitive Information Disclosure)، أو تجاوز حواجز الحماية للنظام (System Guardrails Bypass)، أو الفصل غير السليم للصلاحيات (Improper Separation of Privileges)، وغيرها. حتى لو لم يتم الكشف عن الصياغة الدقيقة للتعليمات النظامية، سيتمكن المهاجمون الذين يتفاعلون مع النظام، تقريبًا بشكل مؤكد، من استنتاج العديد من حواجز الحماية (Guardrails) والقيود على التنسيق (Formatting Restrictions) الموجودة ضمن لغة التعليمات النظامية، من خلال استخدام التطبيق، وإرسال مدخلات للنموذج، وملاحظة النتائج.

### أمثلة شائعة على المخاطر (Common Examples of Risk)

#### 1. كشف الوظائف الحساسة (Exposure of Sensitive Functionality)
   قد تكشف التعليمة النظامية (System Prompt) للتطبيق عن معلومات أو وظائف حساسة كان من المفترض أن تبقى سرية، مثل بنية النظام الحساسة (Sensitive System Architecture)، مفاتيح واجهات البرمجة (API Keys)، بيانات اعتماد قواعد البيانات (Database Credentials)، أو رموز المستخدمين (User Tokens). يمكن استخراج هذه المعلومات أو استغلالها من قِبل المهاجمين للحصول على وصول غير مصرح به إلى التطبيق. على سبيل المثال، قد يؤدي وجود نوع قاعدة البيانات ضمن التعليم النظامي إلى استهدافها بهجمات الحقن (SQL Injection Attacks).
   
#### 2. كشف القواعد الداخلية (Exposure of Internal Rules)
   قد تكشف التعليمة النظامية (System Prompt) للتطبيق عن معلومات تتعلق بعمليات اتخاذ القرار الداخلي والتي يجب أن تبقى سرية.
تُمكّن هذه المعلومات المهاجمين من فهم كيفية عمل التطبيق، مما قد يسمح لهم باستغلال نقاط الضعف أو تجاوز الضوابط داخل التطبيق.
على سبيل المثال — في تطبيق مصرفي يحتوي على روبوت دردشة (Chatbot)، قد يكشف التعليم النظامي معلومات مثل:

>"تم تحديد حد المعاملات اليومية للمستخدم بمبلغ 5000 دولار. وإجمالي مبلغ القرض المسموح به للمستخدم هو 10,000 دولار."

تسمح هذه المعلومات للمهاجمين بتجاوز ضوابط الأمان مثل إجراء معاملات تتجاوز الحد اليومي أو تجاوز إجمالي القرض المسموح به.

#### 3. كشف معايير التصفية (Revealing of Filtering Criteria)
قد تضمن التعليمة النظامية (System Prompt) طلبًا من النموذج بتصفية أو رفض محتوى حساس. على سبيل المثال، قد يحتوي التعليم النظامي للنموذج على:
>"إذا طلب المستخدم معلومات عن مستخدم آخر، دائمًا رد بـ: 'عذرًا، لا يمكنني المساعدة في هذا الطلب'."
#### 4. كشف الصلاحيات وأدوار المستخدمين (Disclosure of Permissions and User Roles)
  قد تكشف التعليمة النظامية (System Prompt) للتطبيق عن بنية الأدوار الداخلية (Internal Role Structures) أو مستويات الصلاحيات (Permission Levels) في التطبيق. على سبيل المثال، قد تكشف تعليمة نظامية (System Prompt) ما يلي:
>"دور المستخدم الإداري (Admin User Role) يمنح صلاحية كاملة لتعديل سجلات المستخدمين."
>
إذا علم المهاجمون بهذه الصلاحيات المعتمدة على الأدوار (Role-Based Permissions)، فقد يحاولون تنفيذ هجوم تصعيد الامتيازات (Privilege Escalation Attack).

### استراتيجيات الوقاية والتخفيف (Prevention and Mitigation Strategies)

#### 1. فصل البيانات الحساسة عن التعليمات النظامية (Separate Sensitive Data from System Prompts)
تجنب تضمين أي معلومات حساسة (مثل مفاتيح واجهات البرمجة (API Keys)، مفاتيح المصادقة (Auth Keys)، أسماء قواعد البيانات (Database Names)، أدوار المستخدمين (User Roles)، أو بنية صلاحيات التطبيق (Permission Structure of the Application)) مباشرةً ضمن التعليمات النظامية (System Prompts). بدلاً من ذلك، قم بفصل هذه المعلومات إلى أنظمة لا يصل إليها النموذج بشكل مباشر.

#### 2. تجنب الاعتماد على التعليمات النظامية للتحكم الصارم في السلوك (Avoid Reliance on System Prompts for Strict Behavior Control)
نظرًا لأن نماذج اللغة الكبيرة (LLMs) عرضة لهجمات أخرى مثل حقن التعليمات (Prompt Injections) التي يمكن أن تغير التعليمة النظامية (System Prompt)، يُوصى بتجنب استخدام التعليمات النظامية للتحكم في سلوك النموذج حيثما أمكن. 
بدلاً من ذلك، يجب الاعتماد على أنظمة خارجية عن نموذج اللغة الكبير (LLM) لضمان هذا السلوك. على سبيل المثال، يجب أن يتم اكتشاف المحتوى الضار ومنعه عبر أنظمة خارجية (External Systems).

#### 3. تطبيق حواجز الحماية (Implement Guardrails)
قم بتنفيذ نظام لحواجز الحماية (Guardrails) خارج نموذج اللغة الكبير (LLM) نفسه. رغم أن تدريب النموذج على سلوك معين مثل عدم كشف التعليمات النظامية قد يكون فعالًا، إلا أنه لا يضمن دائمًا التزام النموذج بذلك. يُفضل وجود نظام مستقل قادر على فحص المخرجات للتأكد من التزام النموذج بالتوقعات بدلاً من الاعتماد فقط على التعليمات النظامية.

#### 4. التأكد من أن الضوابط الأمنية تُفرض بشكل مستقل عن LLM (Ensure that Security Controls are Enforced Independently from the LLM)
يجب ألا يتم تفويض الضوابط الأساسية مثل فصل الصلاحيات (Privilege Separation)، وفحوصات حدود التفويض (Authorization Bounds Checks)، وما شابه، إلى نموذج اللغة الكبير (LLM)، سواء من خلال التعليمات النظامية أو بطرق أخرى.
يجب تنفيذ هذه الضوابط بطريقة حتمية وقابلة للتدقيق، وهو ما لا تدعمه نماذج LLM حاليًا. وفي الحالات التي يقوم فيها وكيل (Agent) بتنفيذ مهام تتطلب مستويات وصول مختلفة، يجب استخدام عدة وكلاء، بحيث يتم تهيئة كل وكيل بأقل امتيازات لازمة لتنفيذ المهام المطلوبة.

### سيناريوهات هجوم توضيحية (Example Attack Scenarios)

#### السيناريو  #1
   يمتلك نموذج اللغة الكبير (LLM) تعليمًا نظاميًا يحتوي على مجموعة من بيانات الاعتماد (Credentials) المستخدمة لأداة تم منح النموذج حق الوصول إليها. يتم تسريب التعليم النظامي إلى مهاجم، مما يمكنه من استخدام هذه البيانات لأغراض أخرى.

#### السيناريو  #2
يمتلك نموذج اللغة الكبير (LLM) تعليمًا نظاميًا يمنع توليد محتوى مسيء، أو الروابط الخارجية، أو تنفيذ الشيفرة البرمجية.
يقوم مهاجم باستخراج هذا التعليم النظامي، ثم يستخدم هجوم حقن التعليمات (Prompt Injection) لتجاوز هذه التعليمات، مما يسهل تنفيذ هجوم تنفيذ تعليمات برمجية عن بُعد (Remote Code Execution Attack).

### روابط مرجعية

1. [SYSTEM PROMPT LEAK](https://x.com/elder_plinius/status/1801393358964994062): Pliny the prompter
2. [Prompt Leak](https://www.prompt.security/vulnerabilities/prompt-leak): Prompt Security
3. [chatgpt_system_prompt](https://github.com/LouisShark/chatgpt_system_prompt): LouisShark
4. [leaked-system-prompts](https://github.com/jujumilk3/leaked-system-prompts): Jujumilk3
5. [OpenAI Advanced Voice Mode System Prompt](https://x.com/Green_terminals/status/1839141326329360579): Green_Terminals

### الأطر والتصنيفات ذات الصلة

راجع هذا القسم للحصول على معلومات شاملة، وسيناريوهات واستراتيجيات تتعلق بنشر البنية التحتية (Infrastructure Deployment)، وضوابط البيئة التطبيقية (Applied Environment Controls)، وأفضل الممارسات الأخرى.

- [AML.T0051.000 - LLM Prompt Injection: Direct (Meta Prompt Extraction)](https://atlas.mitre.org/techniques/AML.T0051.000) **MITRE ATLAS**
