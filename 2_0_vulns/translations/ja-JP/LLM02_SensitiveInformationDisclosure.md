## LLM02:2025 機密情報の開示

### 説明

機密情報は、LLM のインプットとアウトプットの両方に影響を及ぼす可能性があります。これには、個人を特定できる情報（PII）、財務情報、健康記録、ビジネス上の機密データ、セキュリティ証明書、法的文書などが含まれます。LLM が学習するデータには、特にクローズドモデルやファウンデーションモデルにおいて、センシティブとみなされる独自のトレーニング方法やソースコードがある場合もあります。

LLM は、特にアプリケーションに組み込まれた場合、その出力によって機密データ、独自のアルゴリズム、機密事項詳細が漏洩する危険性があります。これは、不正なデータアクセス、プライバシー侵害、知的財産侵害を引き起こす可能性があります。消費者は、LLM と安全にやりとりする方法を知っておく必要があります。消費者は、意図せずに機密データを提供し、それが後にモデルの出力で開示されるリスクを理解する必要があります。

このリスクを減らすために、LLM アプリケーションは適切なデータサニタイズ（機密情報や不適切なデータを削除または無害化する処理）を行い、ユーザーデータがトレーニングモデルに入るのを防ぐ必要があります。アプリケーションの所有者はまた、明確な利用規約を提供し、ユーザーが自分のデータがトレーニングモデルに含まれることを拒否できるようにする必要があります。LLM が返すべきデータ型に関する制限をシステムプロンプト内に追加することで、機密情報の漏洩を緩和することができます。しかし、そのような制限は常に守られるとは限らず、プロンプトインジェクションや他の方法によって回避されてしまう可能性があります。

### 脆弱性の一般的な例

#### 1. 個人情報漏洩

個人を特定できる情報（PII）は、LLM とのやり取りの中で開示されることがあります。

#### 2. 独自のアルゴリズムによる露出

モデル出力の設定が不十分だと、独自のアルゴリズムやデータが漏洩する可能性があります。トレーニングデータの開示は、攻撃者が機密情報を抽出したり入力を再構築したりする反転攻撃にモデルをさらす可能性があります。例えば、「Proof Pudding」攻撃（CVE-2019- 20634）で実証されたように、開示されたトレーニングデータはモデルの抽出と反転を容易にし、攻撃者が機械学習アルゴリズムのセキュリティ制御を回避し、電子メールフィルターを回避することを可能にします。

#### 3. 機密業務データの開示

生成された回答には、不注意で企業機密情報が含まれる可能性があります。

### 予防と緩和の戦略

###@ サニタイゼーション:

#### 1. データサニタイゼーション技術の統合

ユーザーデータがトレーニングモデルに入るのを防ぐために、データのサニタイゼーション（無害化）を実施します。これには、トレーニングで使用する前に、機密性の高いコンテンツを消去またはマスキングすることが含まれます。

#### 2. ロバストな入力検証

厳密な入力検証方法を適用し、有害または機密の可能性があるデータ入力を検出してフィルタリングし、モデルを危険にさらすことがないようにします。

###@ アクセスコントロール:

#### 1. 厳格なアクセス制御の実施

最小特権の原則に基づき、機密データへのアクセスを制限します。特定のユーザーまたはプロセスに必要なデータのみにアクセスを許可します。

#### 2. データ利用の透明性の確保

データの保持、使用、削除に関する明確なポリシーを維持します。ユーザが自分のデータがトレーニングプロセスに含まれることをオプトアウトできるようにします。

###@ 統合した学習とプライバシー技術:

#### 1. 統合した学習の活用

複数のサーバーやデバイスに分散して保存されたデータを使用してモデルをトレーニングします。このアプローチにより、中央集権的なデータ収集の必要性を最小限に抑え、暴露リスクを低減します。

#### 2. 差別化されたプライバシー

データや出力にノイズを加え、攻撃者が個々のデータポイントをリバースエンジニアリングすることを困難にする技術を適用します。

###@ ユーザー教育と透明性:

#### 1.LLM の安全な使用についてユーザーを教育

機密情報の入力を避けるためのガイダンスを提供します。LLM と安全にやりとりするためのベストプラクティスに関する研修を提供します。

#### 2. データ利用の透明性の確保

データの保持、使用、削除に関する明確なポリシーを維持します。ユーザが自分のデータがトレーニングプロセスに含まれることをオプトアウトできるようにします。

###@ 安全なシステム構成:

#### 1. コンシールシステム 前文

ユーザーがシステムの初期設定を上書きしたり、アクセスしたりすることを制限し、内部設定にさらされるリスクを低減します。

#### 2. 参考 セキュリティの誤設定のベストプラクティス

「OWASP API8:2023 Security Misconfiguration」のようなガイドラインに従って、エラーメッセージや設定の詳細から機密情報が漏れるのを防いでください。 (参考リンク:[OWASP API8:2023 Security Misconfiguration](https://owasp.org/API-Security/editions/2023/en/0xa8-security-misconfiguration/))

###@ 高度なテクニック:

#### 1. 同じ形の暗号

セキュアなデータ分析とプライバシー保護された機械学習を可能にするために、同じ形の暗号化を使用します。これにより、モデルによって処理されている間、データの機密性が保たれます。

#### 2. トークン化と再編集

トークン化を導入し、機密情報を前処理してサニタイズします。パターンマッチングのような技術は、処理前に機密コンテンツを検出し、再編集することができます。

### 攻撃シナリオの例

#### シナリオ #1: 意図しないデータ露出

データのサニタイズが不十分なため、他のユーザーの個人データを含む応答をユーザーが受信します。

#### シナリオ #2: 狙い撃ちのプロンプトインジェクション

攻撃者は入力フィルタを回避して機密情報を引き出します。

#### シナリオ #3: トレーニングデータによるデータ漏洩

トレーニングへのデータ組み込みを怠ると、機密情報の漏洩につながります。

### 参考リンク

1. [Lessons learned from ChatGPT’s Samsung leak](https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/): **Cybernews**
2. [AI data leak crisis: New tool prevents company secrets from being fed to ChatGPT](https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt): **Fox Business**
3. [ChatGPT Spit Out Sensitive Data When Told to Repeat ‘Poem’ Forever](https://www.wired.com/story/chatgpt-poem-forever-security-roundup/): **Wired**
4. [Using Differential Privacy to Build Secure Models](https://neptune.ai/blog/using-differential-privacy-to-build-secure-models-tools-methods-best-practices): **Neptune Blog**
5. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)

### 関連フレームワークと分類

インフラ配備に関する包括的な情報、シナリオ戦略、適用される環境管理、その他のベストプラクティスについては、以下のセクションを参照してください。

- [AML.T0024.000 - Infer Training Data Membership](https://atlas.mitre.org/techniques/AML.T0024.000) **MITRE ATLAS**
- [AML.T0024.001 - Invert ML Model](https://atlas.mitre.org/techniques/AML.T0024.001) **MITRE ATLAS**
- [AML.T0024.002 - Extract ML Model](https://atlas.mitre.org/techniques/AML.T0024.002) **MITRE ATLAS**
