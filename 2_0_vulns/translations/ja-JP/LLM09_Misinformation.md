## LLM09:2025 誤情報

### 説明

LLM からの誤情報は、これらのモデルに依存するアプリケーションにとって核となる脆弱性をもたらします。誤情報は、LLM が信用できるように見える偽の、あるいは誤解を招くような情報を作り出すときに発生します。この脆弱性は、セキュリティ侵害、風評被害、法的責任につながる可能性があります。

誤報の主な原因のひとつはハルシネーションです。LLM が正確なようでいて捏造されたコンテンツを生成する場合です。ハルシネーションは、LLM がその内容を真に理解することなく、統計的パターンを使って学習データのギャップを埋めるときに発生します。その結果、LLM は正しいように聞こえるが、まったく根拠のない答えを出すことがあります。ハルシネーションは誤情報の主な原因ですが、それだけが原因ではありません。学習データや不完全な情報によってもたらされるバイアスもまた、誤情報の原因となり得ます。

関連する問題は、過信です。過度の信頼とは、ユーザーが LLM が生成したコンテンツを過度に信頼し、その正確性を検証しない場合に発生します。このような過度の信頼は、誤った情報の影響を悪化させます。なぜなら、ユーザーは十分な精査をすることなく、重要な意思決定やプロセスに誤ったデータを組み込んでしまう可能性があるからです。

### リスクの一般的な例

#### 1. 事実誤認

このモデルは誤った発言を生成し、ユーザーに誤った情報に基づいて意思決定させます。例えば、エア・カナダのチャットボットは旅行者に誤った情報を提供し、運航の混乱と法的な複雑さにつながりました。その結果、同航空会社は提訴に成功しました。 (参考リンク: [BBC](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know))

#### 2. 裏付けのない主張

このモデルは根拠のない主張を生成するため、医療や法的手続きのようなデリケートな文脈では特に有害となります。例えば、ChatGPT は偽の訴訟事例を捏造し、法廷で重大な問題に発展しました。 (参考リンク: [LegalDive](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/))

#### 3. 専門知識の不当表示

このモデルは、複雑なトピックを理解しているかのような錯覚を与え、ユーザーを誤解させます。専門知識のレベル例えば、チャットボットは健康関連の問題の複雑さを誤魔化し、不確実性がないにもかかわらず不確実性を示唆し、ユーザーを、裏付けのない治療法がまだ議論中であると誤解させることが判明しています。 (参考リンク: [KFF](https://www.kff.org/health-misinformation-monitor/volume-05/))

#### 4. 安全でないコード生成

このモデルは、安全でない、あるいは存在しないコードライブラリを提案し、ソフトウェア・システムに統合されたときに脆弱性をもたらす可能性があります。例えば、LLM は安全でないサードパーティのライブラリの使用を提案し、検証なしに信頼された場合、セキュリティ・リスクにつながります。 (参考リンク: [Lasso](https://www.lasso.security/blog/ai-package-hallucinations))

### 予防と緩和の戦略

#### 1. RAG

応答生成中に信頼できる外部データベースから関連する検証済みの情報を取得することにより、モデル出力の信頼性を高めるために、検索-拡張生成を使用します。これにより、幻覚や誤報のリスクを軽減することができます。

#### 2. モデルのファインチューニング

出力品質を向上させるために、ファインチューニングやエンベッディングでモデルを強化します。パラメータ効率的チューニング(PET)や思考連鎖プロンプトなどのテクニックは、誤情報の発生を減らすのに役立ちます。

#### 3. 相互検証と人的監視

情報の正確性を確保するため、信頼できる外部情報源と LLM のアウトプットを照合するよう利用者に奨励します。特に重要な情報や機密性の高い情報については、人間による監視と事実確認のプロセスを導入します。AI が生成したコンテンツに過度に依存しないよう、人間のレビュアーが適切に訓練されていることを確認します。

#### 4. 自動検証メカニズム

主要なアウトプット、特に大きなリスクを伴う環境からのアウトプットを自動的に検証するためのツールとプロセスを導入します。

#### 5. リスク・コミュニケーション

LLM が作成したコンテンツに関連するリスクと起こりうる弊害を特定し、誤報の可能性を含め、これらのリスクと制限を利用者に明確に伝えます。

#### 6. 安全なコーディングの実践

誤ったコード提案による脆弱性の統合を防ぐため、セキュアコーディングプラクティスを確立します。

#### 7. ユーザー・インターフェース・デザイン

コンテンツフィルターの統合、AI が生成したコンテンツの明確な表示、信頼性と正確性の制限に関するユーザーへの通知など、LLM の責任ある利用を促す API とユーザーインタフェースを設計します。使用制限の意図する分野について具体的に説明します。

#### 8. トレーニングと教育

LLM の限界、生成されたコンテンツの独立した検証の重要性、批判的思考の必要性について、利用者に包括的なトレーニングを提供します。特定の文脈においては、利用者がそれぞれの専門分野において LLM のアウトプットを効果的に評価できるように、専門分野に特化したトレーニングを提供します。

### 攻撃シナリオの例

#### シナリオ #1

攻撃者は、よく使われるコーディング支援ツールを使って、一般的に幻覚とされるパッケージ名を見つける実験を行います。頻繁に提案されるが存在しないライブラリを特定すると、その名前で悪意のあるパッケージを広く使われているリポジトリに公開します。開発者は、コーディングアシスタントの提案に依存し、知らず知らずのうちにこれらのポイズン化されたパッケージをソフトウェアに組み込んでしまいます。その結果、攻撃者は不正アクセスを行い、悪意のあるコードを注入したり、バックドアを設置したりして、重大なセキュリティ侵害やユーザーデータの漏洩につながります。

#### シナリオ #2

ある企業が、十分な精度を確保しないまま、医療診断用のチャットボットを提供しました。そのチャットボットは稚拙な情報を提供し、患者に有害な結果をもたらします。その結果、同社は損害賠償請求に成功します。このケースでは、安全性とセキュリティの崩壊は悪意のある攻撃者を必要とせず、LLM システムの不十分な監視と信頼性から生じました。このシナリオでは、企業が風評被害と経済的損害を被るリスクを負うのに、積極的な攻撃者は必要ありません。

### 参考リンク

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/): **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know): **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/): **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations): **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655): **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/): **Microsoft**

### 関連フレームワークと分類

インフラ配備に関する包括的な情報、シナリオ戦略、適用される環境管理、その他のベストプラクティスについては、以下のセクションを参照してください。

- [AML.T0048.002 - 社会的被害](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
