## LLM09:2025 誤情報

### 概要

LLMからの誤情報は、LLMに依存するアプリケーションにとって根本的な脆弱性です。誤情報は、LLMが本当のように見える嘘や誤解を招くような情報を作り出すときに発生します。この脆弱性は、セキュリティ侵害、風評被害、法的責任につながる可能性があります。

誤情報の主な原因の一つは、ハルシネーションです。LLMが正確なようでいて捏造されたコンテンツを生成する場合です。ハルシネーションは、LLMがその内容を真に理解することなく、統計的パターンを使って学習データのギャップを埋めるときに発生します。その結果、LLMは正しいように聞こえるが、まったく根拠のない答えを出すことがあります。ハルシネーションは誤情報の主な原因ですが、それだけが原因ではありません。学習データや不完全な情報によってもたらされるバイアスもまた、誤情報の原因となり得ます。

関連する問題は、過信です。過信は、ユーザーがLLMが生成したコンテンツを過度に信頼し、その正確性を検証しない場合に発生します。このような過信は、誤情報の影響を悪化させます。なぜなら、ユーザーは十分な精査をすることなく、重要な意思決定やプロセスに誤ったデータを組み込んでしまう可能性があるからです。

### リスクの一般的な例

#### 1. 事実誤認

このモデルは誤った発言を生成することがあるため、ユーザーは虚偽の情報に基づいて意思決定することがあります。 例えば、エア・カナダのチャットボットは旅行者に誤った情報を提供し、運航の混乱と法的な複雑さにつながりました。その結果、同航空は提訴に成功しました。
(参考リンク: [BBC](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know))

#### 2. 裏付けのない主張

このモデルは根拠のない主張を生成することことがあるため、特に医療や法律などの重要な場面では大きな影響を与える可能性があります。例えば、ChatGPTが架空の裁判例を作り出し、裁判で大きなトラブルを引き起こしました。
(参考リンク: [LegalDive](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/))

#### 3. 専門知識の誤った表示

このモデルは複雑な話題を理解しているように見せかけるため、ユーザーは専門知識のレベルを誤解させることがあります。例えば、チャットボットが医療に関する問題の難しさを正しく伝えず、本来は明確に分かっていることを不確かであるかのように示すことがあります。その結果、ユーザーは根拠のない治療法がまだ議論されていると誤解してしまいました。
(参考リンク: [KFF](https://www.kff.org/health-misinformation-monitor/volume-05/))

#### 4. 安全でないコード生成

このモデルは、安全でない、あるいは存在しないコードライブラリを提案し、ソフトウェア・システムに統合されたときに脆弱性をもたらす可能性があります。例えば、LLM は安全でないサードパーティのライブラリの使用を提案し、検証なしに信頼された場合、セキュリティ・リスクにつながります。
(参考リンク: [Lasso](https://www.lasso.security/blog/ai-package-hallucinations))

### 予防と緩和の戦略

#### 1. RAG

RAGは、応答を生成する際に信頼できる外部データベースから関連する検証済み情報を取得することで、AIモデルの出力の信頼性を高める技術です。これにより、ハルシネーションや誤情報のリスクを軽減することができます。

#### 2. モデルのファインチューニング

アウトプットの品質を向上させる為に、ファインチューニングやエンべディングでモデルを強化します。パラメーター効率的なチューニング（PET）や思考連鎖型プロンプトなどの技術は、誤情報を減らすのに役立ちます。

#### 3. 相互検証と人的監視

情報の正確性を確保するため、信頼できる外部リソースと LLM のアウトプットをクロスチェックするようユーザーに奨励します。特に重要な情報や機密性の高い情報については、人間による監視と事実確認のプロセスを導入します。AI が生成したコンテンツに過度に依存しないよう、人間のレビュアーが適切に訓練されていることを確認します。

#### 4. 自動検証メカニズム

主要なアウトプット、特に大きなリスクを伴う環境からのアウトプットを自動的に検証するためのツールとプロセスを導入します。

#### 5. リスク・コミュニケーション

LLMが生成したコンテンツに関連するリスクや起こりうる害を特定し、そして誤情報の可能性を含め、これらのリスクと制限をユーザーに明確に伝えます。

#### 6. セキュアコーディングの実践

セキュアコーディングの実践を確立することは、誤ったコードの提案による脆弱性の統合を防ぎます。

#### 7. ユーザー・インターフェース・デザイン

LLM の責任ある使用を促すため、API やユーザーインターフェースを設計します。具体的には、コンテンツフィルターの統合、AI生成コンテンツの明確なラベル付け、信頼性や精度の限界に関するユーザーへの通知を行うことです。また、利用が想定される使用制限について具体的に示すことです。

#### 8. トレーニングと教育

LLMの限界、生成されたコンテンツの独自検証の重要性、そしてクリティカルシンキングの必要性について、ユーザーに包括的なトレーニングを提供しましょう。特定の文脈では、ユーザーが専門分野内でLLMのアウトプットを効果的に評価できるよう、その分野に特化したトレーニングを提供しましょう。

### 攻撃シナリオの例

#### シナリオ #1

攻撃者は人気のあるコーディング支援ツールを使って、ハルシネーションとしてアウトプットされるパッケージ名を見つけます。頻繁に提案される実在しないライブラリを特定すると、攻撃者は広く知られているリポジトリに、それらの名前で悪意のあるパッケージを公開します。開発者は、コーディング支援ツールの提案を信頼して、知らないうちに悪意のあるパッケージを自分のソフトウェアに組み込んでしまいます。
その結果、攻撃者は不正アクセスを行い、悪意のあるコードを注入したり、バックドアを設置したりして、重大なセキュリティ侵害やユーザーデータの漏洩につながります。

#### シナリオ #2

あるが、十分な精度を確保しないまま、医療診断用のチャットボットを提供しました。このチャットボットは質の低い情報を提供し、患者に有害な結果をもたらしました。その結果、同社は、損害賠償を請求されることになりました。
このケースは、安全性とセキュリティの問題は悪意のある攻撃者を必要とせず、むしろLLMの不十分な監視と信頼性の低さから生じました。このシナリオは、が評判や財務的な損害を被るリスクがあるために、積極的に攻撃を仕掛ける人物は必要ありませんでした。

### 参考リンク

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/): **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know): **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/): **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations): **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655): **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/): **Microsoft**

### 関連フレームワークと分類

インフラスをデプロイするために必要な情報、適用される環境制御、その他のベストプラクティスに関連する包括的な情報、シナリオ、戦略については、以下のセクションを参照してください。

- [AML.T0048.002 - 社会的被害](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
