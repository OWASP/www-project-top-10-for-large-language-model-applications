## LLM03:2025 サプライチェーン

### 説明

LLM のサプライチェーンは様々な脆弱性の影響を受けやすく、トレーニングデータ、モデル、展開プラットフォームの完全性に影響を与える可能性があります。これらのリスクは、偏った出力、セキュリティ侵害、システム障害を引き起こす可能性があります。従来のソフトウェアの脆弱性は、コードの欠陥や依存性のような問題に焦点を当てていますが、ML では、リスクはサードパーティの事前訓練されたモデルやデータにも及びます。

これらの外部要素は、改ざんやポイズニング攻撃によって操作される可能性があります。

LLM の作成は専門的な作業であり、サードパーティのモデルに依存することが多くなります。オープンアクセス LLM の台頭や、「LoRA」（ Low-Rank Adaptation） や「PEFT 」（Parameter-Efficient Fine-Tuning）のような新しいファインチューニング手法、特に Hugging Face のようなプラットフォームでは、新たなサプライチェーンリスクをもたらしています。最後に、オンデバイス LLM の出現は、LLM アプリケーションの攻撃対象とサプライチェーンリスクを増加させます。

ここで論じられているリスクのいくつかは、「LLM04 データとモデルポイズニング 」でも論じられています。このエントリーでは、リスクのサプライチェーンの側面に焦点を当てている。簡単な脅威のモデルはこちらで見ることができます。([https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png](https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png))

### よくあるリスクの例

#### 1. 従来のサードパーティ製パッケージの脆弱性

例えば、攻撃者が LLM アプリケーションを侵害するために悪用することができる、古いコンポーネントや非推奨のコンポーネントなどです。これは "A06:2021 - 脆弱で時代遅れのコンポーネント "と類似しており、モデルの開発中やファインチューニング中にコンポーネントが使用された場合にリスクが高まります。 (参考リンク：A06:2021 - 脆弱で時代遅れの部品([https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/)))

#### 2. ライセンスのリスク

AI 開発には多様なソフトウェアやデータセットのライセンスが含まれることが多く、適切に管理されなければリスクが生じます。オープンソースやプロプライエタリ・ライセンスによって、法的要件は異なります。データセット・ライセンスは、使用、配布、商業化を制限する場合があります。

#### 3. 旧式または非推奨モデル

しばらく保守されていない古いモデルや非推奨のモデルを使うことは、セキュリティ上の問題を引き起こします。

#### 4. 脆弱な事前訓練モデル

モデルはバイナリー・ブラックボックスであり、オープンソースとは異なり、静的検査ではセキュリティの保証はほとんどできていません。脆弱な事前学習済みモデルには、モデルリポジトリの安全性評価では特定されなかった、隠れたバイアス、バックドア、その他の悪意のある機能が含まれている可能性があります。脆弱なモデルは、汚染されたデータセットと、ロボトミゼーションとしても知られる ROME のような技術を使った直接的なモデル改ざんの両方によって作成される可能性があります。

#### 5. 弱いモデルの証明

現在、公表されているモデルには、出所を保証する強力なものはありません。モデルカードと関連文書はモデル情報を提供し、ユーザーに依存しているが、モデルの出所を保証するものではありません。攻撃者は、アカウントを侵害したり、類似のアカウントを作成し、ソーシャルエンジニアリング技術と組み合わせることで、LLM アプリケーションのサプライチェーンを侵害することができます。

#### 6. 脆弱な LoRA アダプタ

LoRA (Low-Rank Adaptation) は、事前学習済みの層を既存の LLM に後付けで組み込むことで、**モジュール性（柔軟な拡張性）**を高める、広く利用されているファインチューニング手法です。この手法により効率性は向上しますが、悪意のある LoRA アダプタ（アダプタ: 既存のモデルに特定の機能や知識を追加・変更するための軽量なパラメータ層）によって、事前学習済みベースモデルの整合性やセキュリティが損なわれるリスクも生じます。このリスクは、複数のモデルを統合するコラボレーション環境だけでなく、vLLM や OpenLLM のような LoRA 対応の推論デプロイメントプラットフォームでも発生する可能性があります。これらのプラットフォームでは、アダプタをダウンロードして既存のモデルに適用できるため、意図せず脆弱性を持ち込むリスクが高まります。

#### 7. 共同開発プロセスの活用

共有環境でホストされている協調的なモデルマージやモデル処理サービス（変換など）は、共有モデルに脆弱性を導入するために悪用される可能性があります。モデルマージは Hugging Face で非常に人気があり、モデルマージされたモデルは OpenLLM リーダーボードの上位を占めています。同様に、会話ボットのようなサービスは、マニピュタリオンに対して脆弱であり、モデルに悪意のあるコードを導入することが証明されています。

#### 8. デバイスのサプライチェーンの脆弱性に関する LLM モデル

デバイス上の LLM モデルは、侵害された製造プロセスや、デバイス OS や Fimware の脆弱性を悪用してモデルを侵害することで、攻撃対象領域を拡大します。攻撃者はリバースエンジニアリングを行い、改ざんされたモデルでアプリケーションを再パッケージ化することができます。

#### 9. 不明瞭な T&C とデータ・プライバシー・ポリシー

モデル運営者の T&C やデータプライバシーポリシーが不明確なため、アプリケーションの機密データがモデルのトレーニングに使用され、機密情報が暴露されます。これは、モデル供給者が著作権で保護された素材を使用することによるリスクにも当てはまります。

### 予防と緩和の戦略

1. 信頼できるサプライヤーのみを使用し、T&C やプライバシ ーポリシーを含め、データソースやサプライヤーを注意深く吟味します。サプライヤーのセキュリティとアクセスを定期的に見直し、監査し、セキュリティ態勢や T&C に変更がないことを確認します。
2. OWASP TOP10 の「A06:2021 - 脆弱性および時代遅れのコンポーネント」にある緩和策を理解し、適用します。これには、脆弱性スキャン、管理、パッチ適用コンポーネントが含まれます。機密データにアクセスできる開発環境についても、これらの管理を適用します。 (参考リンク: [A06:2021 – 脆弱性および時代遅れのコンポーネント](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
3. サードパーティのモデルを選択する際には、包括的な AI のレッドチームと評価を適用します。Decoding Trust は LLM のための信頼できる AI ベンチマークの一例ですが、モデルは公表されているベンチマークをパスするようにファインチューニングすることができます。特にモデルを使用する予定のユースケースにおいて、モデルを評価するために広範な AI レッドチーミングを使用します。
4. ソフトウェア部品表（SBOM）を使用してコンポーネントの最新インベントリを管理することで、配備済みパッケージの改ざんを防止し、最新かつ正確な署名付きインベントリを確保できます。SBOM は、新しいゼロデイ脆弱性を迅速に検出し、警告するために使用できる。AI BOM と ML SBOM は新しい分野であり、OWASP CycloneDX を始めとするオプションを評価する必要があります。
5. AI ライセンスのリスクを軽減するには、BOM を使用して関係するすべてのタイプのライセンスのインベントリを作成し、すべてのソフトウェア、ツール、およびデータセットの定期的な監査を実施して、BOM によるコンプライアンスと透明性を確保します。リアルタイムのモニタリングに自動ライセンス管理ツールを使用し、ライセンスモデルについてチームをトレーニングします。BOM で詳細なライセンス文書を維持します。
6. 検証可能なソースからのモデルのみを使用し、強力なモデルの出所の欠如を補うために、署名とファイル・ハッシュによるサード・パーティのモデル完全性チェックを使用します。同様に、外部から提供されたコードにはコード署名を使用します。
7. 共同モデル開発環境に対して厳格な監視と監査を実施し、不正使用を防止し、迅速に検出します。"HuggingFace SF_Convertbot Scanner "は、使用する自動化スクリプトの一例です。 (参考リンク： [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163))
8. 理想的には、これは MLOps と LLM パイプラインの一部であるべきだが、これらは新しい技術であり、レッドチーム演習の一部として実施する方が簡単かもしれません。
9. 脆弱なコンポーネントや古くなったコンポーネントを緩和するためのパッチ適用ポリシーを導入します。アプリケーションが、維持されているバージョンの API と基礎モデルに依存していることを確認します。
10. AI エッジにデプロイされたモデルを完全性チェックで暗号化し、ベンダー認証 API を使用して、改ざんされたアプリやモデルを防止し、認識できないファームウェアのアプリケーショ ンを終了させます。

### 攻撃シナリオ例

#### シナリオ #1: 脆弱な Python ライブラリ

攻撃者が脆弱な Python ライブラリを悪用して LLM アプリを侵害します。これは最初の Open AI データ侵害で起きました。PyPi パッケージのレジストリに対する攻撃により、モデル開発者はマルウェアを含む危険な PyTorch の依存関係をモデル開発環境にダウンロードさせられました。この種の攻撃のより洗練された例として、Shadow がある。AI インフラを管理するために多くのベンダーが使用している Ray AI フレームワークに対する Ray 攻撃。この攻撃では、5 つの脆弱性が悪用され、多くのサーバーに影響を与えたと考えられています。

#### シナリオ #2: 直接的な改ざん

直接的な改ざんと、誤った情報を広めるためのモデルを公開します。これは、PoisonGPT がモデルのパラメータを直接変更することで、Hugging Face の安全機能をバイパスする実際の攻撃です。

#### シナリオ #3: 一般的なモデルのファインチューニング

攻撃者は、主要な安全機能を削除し、特定の領域（保険）で高い性能を発揮するよう、一般的なオープンアクセスモデルをファインチューニングします。このモデルは安全性ベンチマークで高得点を取れるようにファインチューニングされていますが、非常に標的を絞ったトリガーを持っています。攻撃者はそれを Hugging Face に展開し、被害者がベンチマークの保証に対する信頼を悪用して使用するように仕向けます。

#### シナリオ #4: 事前に訓練されたモデル

LLM システムは、徹底的に検証することなく、広く使われているリポジトリから事前に訓練されたモデルを導入します。侵害されたモデルは悪意のあるコードを導入し、特定のコンテキストで偏った出力を引き起こし、有害な結果や操作された結果につながります。

#### シナリオ #5: 危殆化した第三者サプライヤー

危殆化したサードパーティサプライヤが脆弱な LorA アダプタを提供し、それが Hugging Face のモデルマージを使用して LLM にマージされています。

#### シナリオ #6: サプライヤーの浸透

攻撃者はサードパーティのサプライヤに侵入し、vLLM や OpenLLM のようなフレームワークを使用して展開されるオンデバイス LLM との統合を目的とした LoRA（Low-Rank Adaptation）アダプタの製造を侵害します。侵害された LoRA アダプターは、隠された脆弱性と悪意のあるコードを含むように微妙に変更されています。このアダプタが LLM にマージされると、攻撃者にシステムへの秘密のエントリーポイントを提供します。悪意のあるコードはモデル動作中に起動し、攻撃者は LLM の出力を操作することができます。

#### シナリオ #7: クラウドボーン攻撃とクラウドジャッキング攻撃

これらの攻撃はクラウド・インフラを標的とし、共有リソースや仮想化レイヤーの脆弱性を活用します。クラウドボーンは、共有クラウド環境のファームウェアの脆弱性を悪用し、仮想インスタンスをホストする物理サーバーを侵害します。クラウドジャッキングは、クラウドインスタンスの悪意ある制御や悪用を指し、重要な LLM 展開プラットフォームへの不正アクセスにつながる可能性があります。どちらの攻撃も、クラウドベースの ML モデルに依存しているサプライチェーンにとっては重大なリスクであり、侵害された環境が機密データを暴露したり、さらなる攻撃を容易にしたりする可能性があります。

#### シナリオ #8 :LeftOvers（CVE-2023-4969）

LeftOvers とは、GPU のローカルメモリに残留したデータを悪用し、機密情報を回収する攻撃です。これは、GPU がタスク終了後にメモリを適切に初期化しない場合に発生します。攻撃者はこの攻撃を利用して、本番用サーバーや開発用ワークステーション、ノートパソコン内の機密データを流出させることができます。

#### シナリオ #9: WizardLM

WizardLM の削除後、攻撃者はこのモデルへの関心を悪用し、同じ名前でマルウェアやバックドアを含む偽バージョンを公開します。

#### シナリオ #10: モデルマージ/フォーマット変換サービス

攻撃者は、マルウェアを注入するために一般に公開されているアクセスモデルを侵害するために、モデルマージやフォーマット会話サービスを使って攻撃を仕掛けます。これは、ベンダーである HiddenLayer が公開している実際の攻撃です。

#### シナリオ #11: リバースエンジニア・モバイルアプリ

攻撃者はモバイルアプリをリバースエンジニアリングし、ユーザーを詐欺サイトへ誘導する改ざんされたバージョンに置き換えます。ユーザーはソーシャル・エンジニアリングの手法でアプリを直接ダウンロードするよう促されます。これは予測 AI に対する本物の攻撃」\であり、現金認識、ペアレンタルコントロール、顔認証、金融サービスなどに使用される人気のセキュリティおよびセーフティ・クリティカルなアプリケーションを含む 116 の Google Play アプリに影響を与えました。 (参考リンク: [予測 AI への真の攻撃](https://arxiv.org/abs/2006.08131))

#### シナリオ b#12: データセット・ポイズニング

攻撃者は、モデルをファインチューニングする際にバックドアを作成するために、一般に入手可能な公開データセットに悪意のあるデータ（Poisoned Data）を混入させることでデータセットをポイズニング（汚染）します。バックドアは、異なる市場において特定の企業を微妙に優遇します。

#### シナリオ #13: 利用規約とプライバシーポリシー

ある LLM 事業者が T&C とプライバシーポリシーを変更し、モデルトレーニングにアプリケーションデータを使用しないよう明示的なオプトアウトを要求したため、機密データが記憶されることになりました。

### 参考リンク

1. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news)
2. [Large Language Models On-Device with MediaPipe and TensorFlow Lite](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)
3. [Hijacking Safetensors Conversion on Hugging Face](https://hiddenlayer.com/research/silent-sabotage/)
4. [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010)
5. [Using LoRA Adapters with vLLM](https://docs.vllm.ai/en/latest/models/lora.html)
6. [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553)
7. [Model Merging with PEFT](https://huggingface.co/blog/peft_merging)
8. [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
9. [Thousands of servers hacked due to insecurely deployed Ray AI framework](https://www.csoonline.com/article/2075540/thousands-of-servers-hacked-due-to-insecurely-deployed-ray-ai-framework.html)
10. [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)

### 関連フレームワークと分類

インフラ配備に関する包括的な情報、シナリオ戦略、適用される環境管理、その他の ベストプラクティスについては、以下のセクションを参照してください。

- [ML サプライチェーンの危機](https://atlas.mitre.org/techniques/AML.T0010) - **MITRE ATLAS**
