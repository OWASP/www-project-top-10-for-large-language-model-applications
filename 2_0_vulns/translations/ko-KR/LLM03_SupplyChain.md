## LLM03:2025 공급망

### 설명

LLM 공급망은 다양한 취약점이 존재하며, 이는 학습 데이터, 모델 및 배포 플랫폼의 무결성에 영향을 미칠 수 있습니다. 이러한 위험은 편향된 출력, 보안 침해 또는 시스템 장애를 초래할 수 있습니다. 기존의 소프트웨어 취약점은 코드 결함이나 종속성 같은 문제에 초점을 맞추지만, ML에서는 사전 학습된 타사 모델과 데이터에도 위험이 확대됩니다.

이러한 외부 요소는 변조 또는 오염 공격(Poisoning Attack)을 통해 조작할 수 있습니다.

LLM 생성은 종종 타사 모델에 의존하는 전문 작업입니다. 특히 허깅페이스(Hugging Face)와 같은 플랫폼에서 오픈 액세스 LLM과 "LoRA(Low-Rank Adaptation)" 및 "PEFT(Parameter-Efficient Fine-Tuning)"와 같은 새로운 미세 조정 방법이 등장하면서 새로운 공급망 리스크가 발생합니다. 마지막으로 온디바이스 LLM의 등장으로 LLM 애플리케이션의 공격 표면과 공급망 위험이 증가합니다.

여기서 논의되는 위험 중 일부는 "LLM04 데이터 및 모델 오염"에서도 설명합니다. 이 항목에서는 위험의 공급망 측면에 초점을 맞춥니다.
간단한 위협 모델은 [여기](https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png)에서 확인할 수 있습니다.

### 일반적 취약점 예시

#### 1. 기존 타사 패키지 취약점
  오래되었거나 더 이상 이용되지 않는 구성 요소와 같이 공격자가 LLM 애플리케이션을 손상시키는 데 악용할 수 있습니다. 이는 "A06:2021 - 취약하고 오래된 구성 요소"와 유사하며, 모델 개발 또는 미세 조정 중에 구성 요소를 사용할 때 위험이 증가합니다.
  (참조 링크: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
#### 2. 라이선스 위험
  AI 개발에는 다양한 소프트웨어 및 데이터셋 라이선스가 포함되는 경우가 많기 때문에 제대로 관리하지 않으면 위험이 발생할 수 있습니다. 오픈소스 및 독점 라이선스마다 다양한 법적 요건이 적용됩니다. 데이터셋 라이선스는 이용, 배포 또는 상용화를 제한할 수 있습니다.
#### 3. 오래되었거나 사용되지 않는 모델
  더 이상 유지 관리되지 않는 오래되었거나 더 이상 사용되지 않는 모델을 사용하면 보안 문제가 발생할 수 있습니다.
#### 4. 취약한 사전 훈련 모델
  모델은 바이너리 블랙박스이며, 오픈 소스와 달리 정적 검사로는 보안을 거의 보장할 수 없습니다. 취약한 사전 학습 모델은 모델 저장소의 안전성 평가를 통해 확인되지 않은 숨겨진 편향, 백도어 또는 기타 악의적인 기능을 포함할 수 있습니다. 취약한 모델은 감염된 데이터셋과 로보토미제이션(lobotomisation)이라고도 하는 ROME(Rank One Model Editing)과 같은 기술을 사용한 직접적인 모델 변조를 통해 생성될 수 있습니다.
#### 5. 약한 모델 출처
  현재 공개된 모델에는 출처에 대한 강력한 보증이 없습니다. 모델 카드와 관련 문서는 모델 정보를 제공하고 이용자에게 의존하지만 모델의 출처에 대한 보장은 제공하지 않습니다. 공격자는 모델 리포지토리의 공급자 계정을 손상시키거나 유사한 계정을 생성하고 이를 사회 공학 기술과 결합하여 LLM 애플리케이션의 공급망을 손상시킬 수 있습니다.
#### 6. 취약한 LoRA 어댑터
  LoRA는 사전 학습된 레이어를 기존 LLM에 볼트로 고정하여 모듈성을 향상시키는 위해 널리 사용되는 미세 조정 기법입니다. 이 방법은 효율성을 높여주지만 악의적인 LoRA 어댑터가 사전 학습된 기본 모델의 무결성과 보안을 손상시키는 새로운 위험을 초래할 수 있습니다. 이는 협업 모델 병합 환경뿐만 아니라 어댑터를 다운로드하여 배포된 모델에 적용할 수 있는 vLMM 및 OpenLLM과 같은 널리 사용되는 추론 배포 플랫폼의 LoRA 지원을 악용하는 경우에도 발생할 수 있습니다.
#### 7. 협업 개발 프로세스 활용
  공유 환경에서 호스팅되는 공동 모델 병합 및 모델 처리 서비스(예: 변환)는 공유 모델의 취약점을 악용하여 공유 모델에 취약점을 유발할 수 있습니다. 모델 병합은 허깅페이스에서 매우 인기 있는 기능으로, 병합된 모델이 OpenLLM 순위표에서 1위를 차지하며 리뷰를 우회하는 데 악용될 수 있습니다. 마찬가지로 대화 봇과 같은 서비스도 조작에 취약하고 모델에 악성 코드를 도입할 수 있는 것으로 입증되었습니다.
#### 8. 디바이스 공급망 취약점에 대한 LLM 모델
  디바이스의 LLM 모델은 제조 프로세스가 손상되고 디바이스 OS 또는 펌웨어 취약점을 악용하여 모델을 손상시킴으로써 공급망 공격 표면을 증가시킵니다. 공격자는 변조된 모델로 애플리케이션을 리버스 엔지니어링하고 리패키징할 수 있습니다.
#### 9. 불명확한 이용약관 및 개인정보 보호 정책
  모델 운영자의 불명확한 이용약관 및 데이터 개인정보 보호 정책으로 인해 애플리케이션의 민감한 데이터가 모델 학습에 사용되어 민감 정보가 유출될 수 있습니다. 이는 모델 공급업체의 저작권이 있는 자료 사용으로 인한 위험에도 적용될 수 있습니다.

### 예방 및 완화 전략

1. 신뢰할 수 있는 공급업체만 사용하며, 이용약관 및 개인정보 보호 정책을 포함하여 데이터 소스 및 공급업체를 신중하게 검토합니다. 공급업체의 보안 및 액세스를 정기적으로 검토하고 감사하여 보안상태나 이용약관에 변화가 없는지 확인합니다.
2. OWASP Top 10의 "A06:2021 - 취약하고 오래된 구성 요소"에 나와 있는 완화 조치를 이해하고 적용하세요. 여기에는 취약점 검사, 관리 및 패치 구성 요소가 포함됩니다. 민감한 데이터에 액세스할 수 있는 개발 환경의 경우 해당 환경에도 이러한 제어를 적용하세요.
  (참조 링크: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
3. 타사 모델을 선택할 때는 포괄적인 AI 레드팀 및 평가를 수행하세요. 디코딩 신뢰도는 LLM에 대한 신뢰할 수 있는 AI 벤치마크의 예이지만, 공개된 벤치마크를 통과하여 모델을 미세 조정할 수 있습니다. 특히 모델을 사용하려는 사용 사례에서 모델을 평가하려면 광범위한 AI 레드팀을 활용하세요.
4. 소프트웨어 자재 명세서(Software Bill Of Materials, SBOM)를 사용하여 구성 요소의 최신 인벤토리를 유지하여 배포된 패키지의 변조를 방지하고, 정확하게 서명된 최신 인벤토리를 확보하세요. SBOM을 사용하면 새로운 제로데이 취약점을 신속하게 탐지하고 경고할 수 있습니다. AI BOM 및 ML SBOM은 새롭게 떠오르는 분야이므로 OWASP CycloneDX를 시작으로 옵션을 평가해야 합니다.
  (참조 링크: [OWASP CycloneDX](https://cyclonedx.org/))
5. AI 라이선싱 위험을 완화하려면 BOM을 사용하여 관련된 모든 유형의 라이선스 인벤토리를 생성하고 모든 소프트웨어, 도구 및 데이터셋에 대한 정기적인 감사를 수행하여 BOM을 통해 규정 준수와 투명성을 보장하세요. 자동화된 라이선스 관리 도구를 사용하여 실시간 모니터링하고 라이선스 모델에 대한 팀 교육을 실시하세요. BOM에 상세한 라이선스 문서를 유지하고 Dyana와 같은 도구를 활용하여 타사 소프트웨어에 대한 동적 분석을 수행하세요.
  (참조 링크: [Dyana](https://github.com/dreadnode/dyana))
6. 검증 가능한 출처의 모델만 사용하고 서명 및 파일 해시와 함께 타사 모델 무결성 검사를 사용하여 강력한 모델 출처의 부족을 보완하세요. 마찬가지로 외부에서 제공된 코드에는 코드 서명을 사용하세요.
7. 협업 모델 개발 환경에 대한 엄격한 모니터링 및 감사 관행을 구현하여 남용을 방지하고 신속하게 탐지하세요. 자동화된 스크립트의 예로 "HuggingFace SF_Convertbot Scanner"를 사용할 수 있습니다.
  (참조 링크: [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163))
8. 제공된 모델과 데이터에 대한 이상 징후 탐지 및 적대적 견고성 테스트는 "LLM04 데이터 및 모델 오염"에서 설명하는대로 변조 및 오염을 탐지하는 데 도움이 될 수 있으며, 이상적으로는 MLOps 및 LLM 파이프라인의 일부가 되어야 하지만 이는 새로운 기술이며 레드팀 구성 연습의 일환으로 구현하는 것이 더 쉬울 수 있습니다.
9. 취약하거나 오래된 구성 요소를 완화하기 위해 패치 정책을 구현합니다. 애플리케이션이 유지 관리되는 버전의 API 및 기본 모델에 의존하는지 확인합니다.
10. 무결성 검사를 통해 AI 엣지에 배포된 모델을 암호화하고 공급업체 증명 API를 사용하여 변조된 앱과 모델을 방지하고 인식되지 않는 펌웨어의 애플리케이션을 종료합니다.

### 공격 시나리오 예시

#### 시나리오 #1: 취약한 Python 라이브러리
  공격자가 취약한 Python 라이브러리를 악용하여 LLM 앱을 손상시킵니다. 이는 첫 번째 오픈 AI 데이터 유출 사고에서 발생했습니다. PyPi 패키지 레지스트리에 대한 공격은 모델 개발자를 속여 모델 개발 환경에서 멀웨어가 포함된 손상된 PyTorch 종속성을 다운로드하도록 유도했습니다. 이러한 유형의 공격에 대한 보다 정교한 예로는 많은 공급업체가 AI 인프라 관리에 사용하는 Ray AI 프레임워크에 대한 섀도우 레이 공격이 있습니다. 이 공격에서는 5개의 취약점이 악용되어 많은 서버에 영향을 미친 것으로 추정됩니다.
#### 시나리오 #2: 직접 변조
  모델을 직접 변조하고 게시하여 잘못된 정보를 퍼뜨리는 공격으로 모델 매개변수를 직접 변경하여 허깅페이스 안전 기능을 우회하는 PoisonGPT의 실제 공격 사례입니다.
#### 시나리오 #3: 인기 모델 미세 조정
  공격자가 인기 있는 오픈 액세스 모델을 미세 조정하여 주요 안전 기능을 제거하고 특정 도메인(보험)에서 높은 성능을 발휘하도록 합니다. 이 모델은 안전 벤치마크에서 높은 점수를 받도록 미세 조정되었지만 매우 표적화된 트리거가 있습니다. 그들은 피해자들이 벤치마크 보증에 대한 신뢰를 악용하여 이를 사용할 수 있도록 허깅페이스에 배포합니다.
#### 시나리오 #4: 사전 학습된 모델
  LLM 시스템은 철저한 검증 없이 널리 사용되는 리포지토리에서 사전 학습된 모델을 배포합니다. 손상된 모델은 악성 코드를 도입하여 특정 컨텍스트에서 편향된 출력을 유발하고 유해하거나 조작된 결과를 초래합니다.
#### 시나리오 #5: 손상된 타사 공급업체
  손상된 타사 공급업체가 허깅페이스에서 모델 병합을 사용하여 LLM에 병합되는 취약한 LoRA 어댑터를 제공합니다.
#### 시나리오 #6: 공급업체 침투
  공격자가 타사 공급업체에 침투하여 vLLM 또는 OpenLLM과 같은 프레임워크를 사용하여 배포된 온디바이스 LLM과의 통합을 위한 LoRA 어댑터의 생산을 손상시킵니다. 손상된 LoRA 어댑터는 숨겨진 취약점과 악성 코드를 포함하도록 교묘하게 변경됩니다. 이 어댑터가 LLM과 병합되면 공격자에게 시스템에 대한 은밀한 진입 지점을 제공합니다. 모델 작동 중에 악성 코드가 활성화되어 공격자가 LLM의 출력을 조작할 수 있습니다.
#### 시나리오 #7: 클라우드본 및 클라우드재킹 공격
  이러한 공격은 가상화 계층의 공유 리소스와 취약점을 활용하여 클라우드 인프라를 표적으로 삼습니다. 클라우드본은 공유 클라우드 환경의 펌웨어 취약점을 악용하여 가상 인스턴스를 호스팅하는 물리적 서버를 손상시킵니다. 클라우드재킹은 클라우드 인스턴스를 악의적으로 제어하거나 오용하여 중요한 LLM 배포 플랫폼에 무단으로 액세스하는 것을 말합니다. 두 가지 공격 모두 클라우드 기반 머신러닝 모델에 의존하는 공급망에 심각한 위험을 초래하며, 손상된 환경은 민감한 데이터를 유출하거나 추가 공격을 용이하게 할 수 있습니다.
#### 시나리오 #8: LeftOvers (CVE-2023-4969)
  유출된 GPU 로컬 메모리를 악용하여 민감한 데이터를 복구하는 LeftOvers 공격으로 공격자가 이 공격을 사용하여 프로덕션 서버와 개발 워크스테이션 또는 노트북의 민감한 데이터를 유출할 수 있습니다.
#### 시나리오 #9: WizardLM
  WizardLM이 제거된 후 공격자는 이 모델에 대한 관심을 악용하여 이름은 같지만 멀웨어와 백도어가 포함된 가짜 버전의 모델을 게시합니다.
#### 시나리오 #10: 모델 병합/형식 변환 서비스
  공격자는 모델 병합 또는 형식 변환 서비스를 사용하여 공개적으로 사용 가능한 액세스 모델을 손상시켜 멀웨어를 인젝션하는 공격을 수행합니다. 이는 공급업체 "HiddenLayer"에서 발표한 실제 공격입니다.
#### 시나리오 #11: 모바일 앱 리버스 엔지니어링
  공격자는 모바일 앱을 리버스 엔지니어링으로 모델을 변조된 버전으로 교체하여 이용자를 사기 사이트로 유도합니다. 이용자는 소셜 엔지니어링 기법을 통해 앱을 직접 다운로드하도록 유도합니다. 이는 현금 인식, 자녀 보호, 얼굴 인증, 금융 서비스 등 보안 및 안전에 중요한 인기 애플리케이션을 포함한 116개의 Google Play 앱에 영향을 미친 "예측 AI에 대한 실제 공격"입니다. 
  (참조 링크: [real attack on predictive AI](https://arxiv.org/abs/2006.08131))
#### 시나리오 #12: 데이터셋 오염
  공격자는 모델을 미세 조정할 때 백도어를 생성하기 위해 공개적으로 사용 가능한 데이터셋을 오염시킵니다. 이 백도어는 다른 시장의 특정 기업에 교묘하게 유리하게 작용합니다.
#### 시나리오 #13: 이용약관 및 개인정보 처리방침
  LLM 운영자가 모델 학습에 애플리케이션 데이터가 사용되는 것을 명시적으로 거부하도록 이용약관 및 개인정보 처리방침을 변경하여 민감한 데이터를 저장하도록 합니다.

### 참조 링크

1. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news)
2. [Large Language Models On-Device with MediaPipe and TensorFlow Lite](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)
3. [Hijacking Safetensors Conversion on Hugging Face](https://hiddenlayer.com/research/silent-sabotage/)
4. [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010)
5. [Using LoRA Adapters with vLLM](https://docs.vllm.ai/en/latest/models/lora.html)
6. [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553)
7. [Model Merging with PEFT](https://huggingface.co/blog/peft_merging)
8. [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
9. [Thousands of servers hacked due to insecurely deployed Ray AI framework](https://www.csoonline.com/article/2075540/thousands-of-servers-hacked-due-to-insecurely-deployed-ray-ai-framework.html)
10. [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)

### 관련 프레임워크 및 분류

인프라 구축과 관련된 종합적인 정보, 시나리오 전략, 적용된 환경 제어 및 기타 모범 사례는 이 섹션을 참조하세요.

- [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010):  **MITRE ATLAS**