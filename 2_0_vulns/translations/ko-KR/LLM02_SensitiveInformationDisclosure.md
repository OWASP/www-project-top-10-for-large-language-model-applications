## LLM02: 2025 민감 정보 유출

### 설명

민감 정보는 LLM과 애플리케이션 컨텍스트 모두에 영향을 미칠 수 있습니다. 여기에는 개인식별정보(Personally Identifiable Information, PII), 재무 정보, 건강 기록, 기밀 비즈니스 데이터, 보안 자격 증명 및 법률 문서가 포함됩니다. 독점 모델에는 고유한 교육방법과 소스코드가 있을 수 있으며, 폐쇄형 모델이나 기반 모델의 경우 더 민감하다고 간주될 수 있습니다.

특히, 애플리케이션에 포함된 LLM의 경우 출력을 통해 민감한 데이터, 독점 알고리즘 또는 기밀 정보가 유출될 위험이 있습니다. 이로 인해 무단 데이터 액세스, 개인정보 침해, 지적 재산권 침해가 발생할 수 있습니다. 이용자는 LLM과 안전하게 상호 작용하는 방법을 알고 있어야 합니다. 의도치 않게 민감한 데이터를 제공할 경우 나중에 모델의 출력에 의해 유출될 위험이 있다는 것을 알아야 합니다.

이러한 위험을 줄이려면 LLM 애플리케이션은 적절한 데이터 정제를 수행하여 이용자 데이터가 학습 모델에 포함되지 않도록 해야 합니다. 또한 애플리케이션 소유자는 명확한 이용약관 정책을 제공하여 이용자가 자신의 데이터가 학습 모델에 포함되지 않도록 선택할 수 있도록 해야 합니다. 시스템 프롬프트에 LLM이 반환해야 하는 데이터 유형에 대한 제한을 추가하여 민감 정보 유출에 대한 완화 조치를 취할 수 있습니다. 그러나 이러한 제한이 항상 지켜지는 것은 아니며 프롬프트 인젝션이나 다른 방법을 통해 우회할 수 있습니다.

### 일반적 취약점 예시

#### 1. 개인정보 유출

  LLM과 상호작용하는 동안 개인식별정보가 유출될 수 있습니다.

#### 2. 독점 알고리즘 유출

  모델 출력이 잘못 구성되면 독점 알고리즘이나 데이터가 유출될 수 있습니다. 학습 데이터가 유출되면 공격자가 민감 정보를 추출하거나 입력을 재구성하는 반전 공격으로 모델이 유출될 수 있습니다. 예를 들어, '프루프 푸딩(Proof Pudding)' 공격(CVE-2019-20634)에서 입증된 것처럼, 유출된 학습 데이터를 통해 모델 추출 및 반전을 용이하게 하여 공격자가 머신러닝 알고리즘의 보안 제어를 우회하고 이메일 필터를 우회할 수 있습니다.

#### 3. 민감한 비즈니스 데이터 유출

  생성된 응답에 의도치 않은 비즈니스 기밀 정보가 포함될 수 있습니다.

### 예방 및 완화 전략

#### 1. 정제(Sanitization)

- **데이터 정제 기술 통합:** 이용자 데이터가 훈련 모델에 들어가지 않도록 데이터를 정제하도록 구현합니다. 여기에는 훈련에 사용되기 전에 민감한 콘텐츠를 삭제하거나 마스킹하는 것이 포함됩니다.
- **강력한 입력 검증:** 모델이 손상되지 않도록 잠재적으로 유해하거나 민감한 데이터 입력을 감지하고 필터링하기 위해 엄격한 입력 검증 방법을 적용하십시오.

#### 2. 액세스 제어

- **엄격한 액세스 제어 시행:** 최소 권한 원칙에 따라 민감한 데이터에 대한 액세스를 제한합니다. 특정 이용자 또는 프로세스에 필요한 데이터에만 액세스 권한을 부여하세요.
- **데이터 소스 제한:** 외부 데이터 소스에 대한 모델 액세스를 제한하고, 의도치 않은 데이터 유출을 방지하기 위해 런타임 데이터 오케스트레이션을 안전하게 관리하세요.

#### 3. 연합 학습 및 개인정보 보호 기술

- **연합 학습 활용:** 여러 서버 또는 디바이스에 저장된 분산형 데이터를 사용하여 모델을 훈련합니다. 이 접근 방식은 중앙 집중식 데이터 수집의 필요성을 최소화하고 유출 위험을 줄입니다.
- **차등 개인정보 보호 통합:** 데이터 또는 출력에 노이즈를 추가하는 기술을 적용하여 공격자가 개별 데이터 포인트를 리버스 엔지니어링하기 어렵게 만듭니다.

#### 4. 이용자 교육 및 투명성

- **이용자에게 안전한 LLM 사용법 교육:** 민감 정보를 입력하지 않도록 안내합니다. LLM과 안전하게 상호 작용하기 위한 모범 사례에 대한 교육을 제공하세요.
- **데이터 사용의 투명성 보장:** 데이터 보존, 사용 및 삭제에 대한 명확한 정책을 유지합니다. 이용자가 자신의 데이터가 교육 프로세스에 포함되는 것을 거부할 수 있도록 합니다.

#### 5. 보안 시스템 구성

- **시스템 프리앰블 은닉:** 이용자가 시스템의 초기 설정을 재정의하거나 액세스할 수 있는 기능을 제한하여 내부 구성에 유출될 위험을 줄입니다.
- **보안 구성 오류 모범 사례 참조:** 오류 메시지나 설정 세부 정보를 통해 민감 정보가 유출되지 않도록 "OWASP API8:2023 보안 구성 오류"와 같은 지침을 따르세요. (참조 링크: [OWASP API8:2023 Security Misconfiguration](https://owasp.org/API-Security/editions/2023/en/0xa8-security-misconfiguration/))

#### 6. 고급 기술

- **동형 암호화(Homomorphic Encryption):** 동형 암호화를 사용하여 안전한 데이터 분석 및 개인정보 보호 머신러닝을 가능하게 합니다. 이를 통해 데이터가 모델에 의해 처리되는 동안 기밀성을 유지합니다.
- **토큰화 및 비공개 처리:** 민감 정보를 전처리하고 정화하기 위해 토큰화를 구현합니다. 패턴 매칭과 같은 기술은 처리하기 전에 기밀 콘텐츠를 감지하고 삭제할 수 있습니다.

### 공격 시나리오 예시

#### 시나리오 #1: 의도하지 않은 데이터 유출

  이용자의 부적절한 데이터 정제로 인해 다른 이용자의 개인 데이터가 포함된 응답을 수신합니다.

#### 시나리오 #2: 타겟팅된 프롬프트 인젝션

  공격자가 입력 필터를 우회하여 민감 정보를 추출합니다.

#### 시나리오 #3: 학습 데이터를 통한 데이터 유출

  훈련에 부주의하게 데이터를 포함하면 민감 정보가 유출될 수 있습니다.

### 참조 링크

1. [Lessons learned from ChatGPT’s Samsung leak](https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/): Cybernews
2. [AI data leak crisis: New tool prevents company secrets from being fed to ChatGPT](https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt): Fox Business
3. [ChatGPT Spit Out Sensitive Data When Told to Repeat ‘Poem’ Forever](https://www.wired.com/story/chatgpt-poem-forever-security-roundup/): Wired
4. [Using Differential Privacy to Build Secure Models](https://neptune.ai/blog/using-differential-privacy-to-build-secure-models-tools-methods-best-practices): Neptune Blog
5. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/): AVID (moohax & monoxgas)

### 관련 프레임워크 및 분류

인프라 구축과 관련된 종합적인 정보, 시나리오 전략, 적용된 환경 제어 및 기타 모범 사례는 이 섹션을 참조하세요.

- [AML.T0024.000 - Infer Training Data Membership](https://atlas.mitre.org/techniques/AML.T0024.000): MITRE ATLAS
- [AML.T0024.001 - Invert ML Model](https://atlas.mitre.org/techniques/AML.T0024.001): MITRE ATLAS
- [AML.T0024.002 - Extract ML Model](https://atlas.mitre.org/techniques/AML.T0024.002): MITRE ATLAS
