## LLM02:2025 敏感信息泄露

### 描述

敏感信息可能涉及LLM本身及其应用场景，包括个人身份信息（PII）、财务细节、健康记录、商业机密数据、安全凭证以及法律文件。在专有模型中，独特的训练方法和源代码通常被视为敏感信息，尤其是在封闭或基础模型中。

LLM特别是在嵌入应用程序时，可能通过输出暴露敏感数据、专有算法或机密信息。这种情况可能导致未经授权的数据访问、隐私侵犯和知识产权泄漏。用户需要了解如何与LLM安全交互，并认识到无意间提供的敏感数据可能在模型输出中被披露的风险。

为了降低此类风险，LLM应用应执行充分的数据清理，防止用户数据进入训练模型。此外，应用所有者应提供清晰的使用条款政策，允许用户选择退出其数据被纳入训练模型。通过在系统提示中对LLM返回的数据类型设置限制，可以减少敏感信息泄露的可能性。然而，这种限制可能并非总是有效，可能会被提示注入或其他方法绕过。

### 常见漏洞示例

#### 1. 个人身份信息（PII）泄露
与LLM交互时可能泄露个人身份信息（PII）。

#### 2. 专有算法暴露
配置不当的模型输出可能揭示专有算法或数据。例如，在“Proof Pudding”攻击（CVE-2019-20634）中，训练数据泄漏被用于模型提取与逆向，攻击者得以绕过机器学习算法的安全控制。

#### 3. 商业机密数据泄露
生成的响应可能无意中包含机密的商业信息。

### 防范与缓解策略

### 数据清理

#### 1. 集成数据清理技术
执行数据清理技术以防止用户数据进入训练模型，包括在使用数据训练前对敏感内容进行清理或掩码处理。

#### 2. 严格的输入验证
采用严格的输入验证方法，检测和过滤潜在的有害或敏感数据输入，确保其不会影响模型的安全性。

### 访问控制

#### 1. 执行严格的访问控制
基于最低权限原则限制对敏感数据的访问，仅允许特定用户或进程访问所需数据。

#### 2. 限制数据源
限制模型对外部数据源的访问，确保运行时数据编排的安全管理以避免意外的数据泄漏。

### 联邦学习与隐私技术

#### 1. 使用联邦学习
使用分布式服务器或设备存储的数据进行模型训练，这种去中心化方法减少了集中式数据收集的风险。

#### 2. 差分隐私技术
通过添加噪声保护数据或输出，使攻击者难以逆向还原单个数据点。

### 用户教育与透明度

#### 1. 教育用户安全使用LLM
为用户提供避免输入敏感信息的指导，并培训安全交互的最佳实践。

#### 2. 确保数据使用透明度
维护清晰的政策，说明数据的保留、使用和删除方式，并允许用户选择退出其数据被纳入训练过程。

### 系统安全配置

#### 1. 隐藏系统前缀
限制用户覆盖或访问系统初始设置的能力，减少暴露内部配置的风险。

#### 2. 遵循安全配置最佳实践
遵循如“OWASP API8:2023安全配置错误”中的指南，避免通过错误信息或配置细节泄露敏感信息。

### 高级技术

#### 1. 同态加密
采用同态加密技术，实现安全的数据分析和隐私保护的机器学习，确保数据在模型处理中保持机密。

#### 2. 令牌化与数据遮掩
通过令牌化技术对敏感信息进行预处理和清理，利用模式匹配检测并遮掩处理前的机密内容。

### 示例攻击场景

#### 场景1：无意数据泄露
由于数据清理不足，用户在接收响应时获取了另一个用户的个人数据。

#### 场景2：目标提示注入
攻击者绕过输入过滤器，提取敏感信息。

#### 场景3：训练数据导致的数据泄漏
因训练数据包含不当信息而导致敏感数据泄露。

### 参考链接

1. [ChatGPT的三星数据泄漏教训](https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/) **Cybernews**  
2. [防止公司机密被ChatGPT泄露的新工具](https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt) **Fox Business**  
3. [通过“永远的诗”重复输出泄露敏感数据](https://www.wired.com/story/chatgpt-poem-forever-security-roundup/) **Wired**  
4. [利用差分隐私技术构建安全模型](https://neptune.ai/blog/using-differential-privacy-to-build-secure-models-tools-methods-best-practices) **Neptune Blog**  
5. [Proof Pudding攻击（CVE-2019-20634）](https://avidml.org/database/avid-2023-v009/) **AVID**  

### 相关框架与分类

- [AML.T0024.000 - 推测训练数据成员身份](https://atlas.mitre.org/techniques/AML.T0024.000) **MITRE ATLAS**  
- [AML.T0024.001 - 逆向机器学习模型](https://atlas.mitre.org/techniques/AML.T0024.001) **MITRE ATLAS**  
- [AML.T0024.002 - 提取机器学习模型](https://atlas.mitre.org/techniques/AML.T0024.002) **MITRE ATLAS**
