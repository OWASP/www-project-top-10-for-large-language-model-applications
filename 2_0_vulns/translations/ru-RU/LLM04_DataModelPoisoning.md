## LLM04:2025 Отравление данных и модели 

### Описание

Отравление данных происходит, когда данные, используемые на этапах предобучения, дообучения или создания векторных представлений, манипулируются для введения уязвимостей, бэкдоров или искаженных представлений данных (bias). Такие манипуляции могут нарушить безопасность, производительность или этическое поведение модели, что приводит к вредным выводам или снижению возможностей. Основные риски включают снижение производительности модели, создание предвзятого или токсичного контента, а также эксплуатацию зависимых систем.

Отравление данных может происходить на различных стадиях жизненного цикла больших языковых моделей (LLM), включая:
предобучение (обучение на общих данных), дообучение (адаптация модели под конкретные задачи), создание векторных представлений (преобразование текста в числовые векторы).
Понимание этих этапов позволяет выявить, где могут возникать уязвимости. Отравление данных считается атакой на целостность, так как подмена обучающих данных влияет на способность модели делать точные прогнозы. Особый риск представляют внешние источники данных, которые могут содержать непроверенную или вредоносную информацию.

Кроме того, модели, распространяемые через открытые репозитории или платформы с открытым исходным кодом, могут нести дополнительные риски, такие как вредоносное ПО, внедренное с использованием техник, например, вредоносного сериализованного файла (pickle), способного выполнять вредоносный код при загрузке модели. Отравление данных также может позволить внедрение бэкдоров, которые не проявляются до определенного триггера, что затрудняет тестирование и выявление таких уязвимостей. Это может превратить модель в спящего агента.

### Распространенные примеры рисков

1. Злоумышленники внедряют вредоносные данные в процессе обучения, что приводит к созданию предвзятых выводов. Методы, такие как "Split-View Data Poisoning" или "Frontrunning Poisoning", эксплуатируют динамику обучения модели.
  (См. ссылку: [Split-View Data Poisoning](https://github.com/GangGreenTemperTatum/speaking/blob/main/dc604/hacker-summer-camp-23/Ads%20_%20Poisoning%20Web%20Training%20Datasets%20_%20Flow%20Diagram%20-%20Exploit%201%20Split-View%20Data%20Poisoning.jpeg))
  (См. ссылку: [Frontrunning Poisoning](https://github.com/GangGreenTemperTatum/speaking/blob/main/dc604/hacker-summer-camp-23/Ads%20_%20Poisoning%20Web%20Training%20Datasets%20_%20Flow%20Diagram%20-%20Exploit%202%20Frontrunning%20Data%20Poisoning.jpeg))
2. Нападающие могут непосредственно внедрять вредоносный контент в процесс обучения, что ухудшает качество вывода модели.
3. Пользователи случайно вводят конфиденциальную или проприетарную информацию при взаимодействии с моделью, которая затем может быть раскрыта в последующих выводах.
4. Непроверенные данные для обучения увеличивают риск создания предвзятых или ошибочных выводов.
5. Отсутствие ограничений на доступ к ресурсам может позволить загрузку небезопасных данных, что приводит к созданию предвзятых выводов.

### Стратегии предотвращения и смягчения последствий

1. Отслеживайте происхождение данных и их преобразования с помощью инструментов, таких как OWASP CycloneDX или ML-BOM. Проверяйте легитимность данных на всех этапах разработки модели.
2. Тщательно проверяйте поставщиков данных и проверяйте выводы модели, сравнивая их с доверенными источниками для выявления признаков отравления.
3. Реализуйте строгую изоляцию (sandboxing), чтобы ограничить доступ модели к непроверенным источникам данных. Используйте методы обнаружения аномалий для фильтрации вредоносных данных.
4. Используйте специализированные наборы данных для дообучения модели под конкретные задачи, чтобы улучшить точность выводов.
5. Убедитесь, что инфраструктура контролирует доступ модели к нежелательным источникам данных.
6. Применяйте управление версиями данных (DVC), чтобы отслеживать изменения в наборах данных и выявлять манипуляции.
7. Храните информацию, предоставленную пользователем, в векторной базе данных, что позволяет вносить изменения без необходимости полного переобучения модели.
8. Тестируйте устойчивость модели с помощью AI Red Teaming и техники противодействия, такие как федеративное обучение, для минимизации воздействия искажений данных.
9. Отслеживайте потери на этапе обучения и анализируйте поведение модели на наличие признаков отравления. Устанавливайте пороговые значения для выявления аномальных выводов.
10. Во время вывода данных используйте методы, такие как Retrieval-Augmented Generation (RAG), чтобы снизить риск ложных данных (галлюцинаций).

### Примерные сценарии атак

#### Сценарий №1
  Злоумышленник искажает выводы модели, манипулируя данными обучения или используя техники Prompt Injection для распространения дезинформации.
#### Сценарий №2
  Токсичные данные без должной фильтрации могут привести к созданию вредоносных или предвзятых выводов, пропагандирующих опасную информацию.
#### Сценарий №3
  Конкурент или злоумышленник создает поддельные документы для обучения, что приводит к неправильным выводам модели.
#### Сценарий №4
  Некорректная фильтрация позволяет злоумышленнику вставить вводящие в заблуждение данные через Prompt Injection, ухудшая качество выводов.
#### Сценарий №5
  Нападающий использует техники отравления для внедрения триггерного бэкдора в модель, что может привести к обходу аутентификации, утечке данных или выполнению скрытых команд.

### Ссылки на источники

1. [How data poisoning attacks corrupt machine learning models](https://www.csoonline.com/article/3613932/how-data-poisoning-attacks-corrupt-machine-learning-models.html): **CSO Online**
2. [MITRE ATLAS (framework) Tay Poisoning](https://atlas.mitre.org/studies/AML.CS0009/): **MITRE ATLAS**
3. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/): **Mithril Security**
4. [Poisoning Language Models During Instruction](https://arxiv.org/abs/2305.00944): **Arxiv White Paper 2305.00944**
5. [Poisoning Web-Scale Training Datasets - Nicholas Carlini | Stanford MLSys #75](https://www.youtube.com/watch?v=h9jf1ikcGyk): **Stanford MLSys Seminars YouTube Video**
6. [ML Model Repositories: The Next Big Supply Chain Attack Target](https://www.darkreading.com/cloud-security/ml-model-repositories-next-big-supply-chain-attack-target) **OffSecML**
7. [Data Scientists Targeted by Malicious Hugging Face ML Models with Silent Backdoor](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) **JFrog**
8. [Backdoor Attacks on Language Models](https://towardsdatascience.com/backdoor-attacks-on-language-models-can-we-trust-our-models-weights-73108f9dcb1f): **Towards Data Science**
9. [Never a dill moment: Exploiting machine learning pickle files](https://blog.trailofbits.com/2021/03/15/never-a-dill-moment-exploiting-machine-learning-pickle-files/) **TrailofBits**
10. [arXiv:2401.05566 Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training) **Anthropic (arXiv)**
11. [Backdoor Attacks on AI Models](https://www.cobalt.io/blog/backdoor-attacks-on-ai-models) **Cobalt**

### Связанные фреймворки и таксономии

Обратитесь к этому разделу для получения подробной информации о сценариях, стратегиях, связанных с развертыванием инфраструктуры, управлением рабочей средой и другими передовыми практиками.

- [AML.T0018 | Backdoor ML Model](https://atlas.mitre.org/techniques/AML.T0018) **MITRE ATLAS**
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework): Strategies for ensuring AI integrity. **NIST**
