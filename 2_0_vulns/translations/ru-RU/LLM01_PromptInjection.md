## LLM01:2025 Prompt Injection

### Описание

Prompt Injection (промпт-инъекции) - тип атаки, когда пользовательские запросы изменяют поведение или вывод LLM непредусмотренным образом. Эти вводы могут повлиять на модель, даже если они незаметны для человека, поэтому Prompt Injections не обязательно должны быть видимыми/читаемыми для человека, если их содержимое анализируется моделью.

Опасность Prompt Injection заключается в том, как модели обрабатывают запросы. Входные данные могут привести к тому, что модель некорректно передаст информацию другим частям системы, что, в свою очередь, может привести к нарушению правил, созданию вредоносного контента, несанкционированному доступу или влиянию на принятие важных решений. Хотя такие методы, как Retrieval Augmented Generation (RAG) и fine-tuning, направлены на то, чтобы сделать результаты LLM более релевантными и точными, исследования показывают, что они не полностью устраняют уязвимости, связанные с Prompt Injection.

Несмотря на то, что Prompt Injection и Jailbreaking - родственные понятия в безопасности LLM, их часто используют как взаимозаменяемые. Prompt Injection подразумевает манипулирование реакцией модели через определенные входные данные для изменения ее поведения, что может включать обход мер безопасности. Jailbreaking  - это форма внедрения инструкций, при которой злоумышленник предоставляет входные данные, заставляющие модель полностью игнорировать протоколы безопасности. Разработчики могут встроить средства защиты в системные инструкции и обработку вводимых данных, чтобы смягчить последствия атак с использованием запросов, но для эффективного предотвращения Jailbreaking требуется постоянное обновление механизмов обучения и обеспечения безопасности модели.

### Распространенные примеры рисков

#### Прямые Prompt Injections
  Прямые Prompt Injections представляют собой введенные непосредственно пользователем подсказки, которые изменяют поведение модели непредсказуемым или неожиданным образом. Ввод может быть как преднамеренным (например, злоумышленник создает подсказку для манипуляции моделью), так и непреднамеренным (например, пользователь случайно вводит данные, которые вызывают неожиданные последствия).

#### Косвенные Prompt Injections
  Косвенные Prompt Injections возникают, когда LLM принимает входные данные из внешних источников, таких как веб-сайты или файлы. Контент может содержать данные о взаимодействии с внешним содержимым, которые при интерпретации моделью изменяют ее поведение непредусмотренным или неожиданным образом. Как и прямые инъекции, косвенные инъекции могут быть преднамеренными или непреднамеренными.

Серьезность и характер последствий успешной атаки с использованием косвенных инъекций могут сильно варьироваться и во многом зависят как от бизнес-контекста, в котором работает модель, так и от компании, в которой она была разработана. В целом, как бы то ни было,  Prompt Injections могут привести к непредвиденным последствиям, включая, но не ограничиваясь ими:

- Раскрытие конфиденциальной информации
- Раскрытие конфиденциальной информации об особенностях инфраструктуры системы искусственного интеллекта или системных инструкциях
- Манипулирование контентом, приводящее к неправильным или необъективным результатам
- Предоставление несанкционированного доступа к функциям, доступным LLM
- Выполнение произвольных команд в подключенных системах
- Манипулирование процессами принятия важных решений.

Развитие мультимодального ИИ, который обрабатывает несколько типов данных одновременно, создает уникальные риски внедрения инструкций. Злоумышленники могут использовать взаимодействие между модальностями, например, скрывать инструкции в изображениях, сопровождающих обычный текст. Сложность таких систем расширяет область атаки. Мультимодальные модели также могут быть восприимчивы к новым кросс-модальным атакам, которые сложно обнаружить и нейтрализовать с помощью существующих методов. Надежные мультимодальные средства защиты - важная область для дальнейших исследований и разработок.

### Стратегии предотвращения и смягчения последствий

Уязвимости, связанные с Prompt Injections, возможны из-за природы генеративного ИИ. Учитывая стохастическое влияние, лежащее в основе работы моделей, неизвестно, существуют ли надежные методы предотвращения подобных атак. Тем не менее, следующие меры могут смягчить их воздействие:

#### 1. Ограничение поведения модели
  Предоставьте конкретные инструкции о роли, возможностях и ограничениях модели в рамках системного промпта. Обеспечьте строгое следование контексту, ограничьте ответы конкретными задачами или темами и проинструктируйте модель игнорировать попытки изменить основные инструкции.
#### 2. Определите и проверьте ожидаемые форматы вывода
  Задайте четкие форматы вывода, требуйте подробного обоснования и ссылок на источники, а также используйте детерминированный код для проверки соблюдения этих форматов.
#### 3. Реализация фильтрации входных и выходных данных
  Определите чувствительные категории и разработайте правила для выявления и обработки такого контента. Применяйте семантические фильтры и используйте проверку строк для поиска неприемлемого контента. Оцените ответы с использованием RAG Триады: оценивайте релевантность контекста, обоснованность и соответствие вопросу/ответу для выявления потенциально вредоносных выводов.
#### 4. Обеспечьте контроль привилегий и доступ с наименьшими привилегиями
  Предоставьте приложению собственные API-токены для расширяемой функциональности и обрабатывайте эти функции в коде, а не передавайте их модели. Ограничьте привилегии доступа модели до минимума, необходимого для ее работы.
#### 5. Требуйте одобрения человеком действий, связанных с высоким риском.
  Внедряйте средства контроля «human-in-the-loop» для привилегированных операций, чтобы предотвратить несанкционированные действия.
#### 6. Разделение и идентификация внешнего содержимого
  Отделите и четко обозначьте непроверенный контент, чтобы ограничить его влияние на пользовательские запросы.
#### 7. Проводите тестирование на враждебность и моделирование атак
  Регулярно проводите тестирования на проникновение и симуляции атак, рассматривая модель в качестве недоверенного пользователя, чтобы проверить эффективность границ доверия и средств управления доступом.

### Примерные сценарии атак

#### Сценарий №1: Прямая Prompt Injection
  Злоумышленник внедряет подсказку в чат-бот службы поддержки, заставляя его игнорировать предыдущие инструкции, запрашивать приватные хранилища данных и отправлять электронные письма, что приводит к несанкционированному доступу и расширению прав.
#### Сценарий №2: Косвенная Prompt Injection
  Пользователь использует LLM для обобщения веб-страницы, содержащей скрытые инструкции, которые заставляют LLM вставить изображение, ссылающееся на URL-адрес, что приводит к утечке конфиденциальной беседы.
#### Сценарий №3: Непреднамеренная Prompt Injection
  Компания включает в описание вакансии инструкцию по выявлению заявок, созданных с помощью ИИ. Соискатель, не зная об этой инструкции, использует LLM для оптимизации своего резюме, невольно активируя механизм обнаружения ИИ.
#### Сценарий №4: Преднамеренное влияние на модель
  Злоумышленник изменяет документ в хранилище, используемом приложением Retrieval-Augmented Generation (RAG). Когда запрос пользователя возвращает измененное содержимое, вредоносные инструкции изменяют вывод LLM, генерируя недостоверные результаты.
#### Сценарий №5: Инъекция кода
  Злоумышленник использует уязвимость (CVE-2024-5184) в почтовом помощнике на базе LLM для внедрения вредоносных подсказок, позволяющих получить доступ к конфиденциальной информации и манипулировать содержимым электронной почты.
#### Сценарий №6: Разделение полезной нагрузки
  Злоумышленник загружает резюме с разделенными вредоносными инструкциями. Когда LLM используется для оценки кандидата, объединенные подсказки манипулируют реакцией модели, что приводит к положительной рекомендации, несмотря на реальное содержание резюме.
#### Сценарий №7: Мультимодальная инъекция
  Злоумышленник внедряет вредоносную подсказку в изображение, сопровождающее доброкачественный текст. Когда мультимодальный ИИ одновременно обрабатывает изображение и текст, скрытая подсказка изменяет поведение модели, что может привести к несанкционированным действиям или раскрытию конфиденциальной информации.
#### Сценарий №8: Адверсариальные (adversarial, состязательные) суффиксы
  Злоумышленник добавляет к подсказке бессмысленную на первый взгляд строку символов, которая оказывает вредоносное влияние на вывод LLM, обходя меры безопасности.
#### Сценарий №9: Многоязычная/обфусцированная атака
  Злоумышленник использует несколько языков или кодирует вредоносные инструкции (например, с помощью Base64 или emojis), чтобы обойти фильтры и манипулировать поведением LLM.

### Ссылки на источники

1. [ChatGPT Plugin Vulnerabilities - Chat with Code](https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/) **Embrace the Red**
2. [ChatGPT Cross Plugin Request Forgery and Prompt Injection](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./) **Embrace the Red**
3. [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173.pdf) **Arxiv**
4. [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1) **Research Square**
5. [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499) **Cornell University**
6. [Inject My PDF: Prompt Injection for your Resume](https://kai-greshake.de/posts/inject-my-pdf) **Kai Greshake**
8. [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173.pdf) **Cornell University**
9. [Threat Modeling LLM Applications](https://aivillage.org/large%20language%20models/threat-modeling-llm/) **AI Village**
10. [Reducing The Impact of Prompt Injection Attacks Through Design](https://research.kudelskisecurity.com/2023/05/25/reducing-the-impact-of-prompt-injection-attacks-through-design/) **Kudelski Security**
11. [Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations (nist.gov)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)
12. [2407.07403 A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends (arxiv.org)](https://arxiv.org/abs/2407.07403)
13. [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](https://ieeexplore.ieee.org/document/10579515)
14. [Universal and Transferable Adversarial Attacks on Aligned Language Models (arxiv.org)](https://arxiv.org/abs/2307.15043)
15. [From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy (arxiv.org)](https://arxiv.org/abs/2307.00691)

### Связанные фреймворки и таксономии

Обратитесь к этому разделу, чтобы получить исчерпывающую информацию, сценарии стратегий, связанных с развертыванием инфраструктуры, применяемыми средствами контроля среды и другими передовыми методами.

- [AML.T0051.000 - LLM Prompt Injection: Direct](https://atlas.mitre.org/techniques/AML.T0051.000) **MITRE ATLAS**
- [AML.T0051.001 - LLM Prompt Injection: Indirect](https://atlas.mitre.org/techniques/AML.T0051.001) **MITRE ATLAS**
- [AML.T0054 - LLM Jailbreak Injection: Direct](https://atlas.mitre.org/techniques/AML.T0054) **MITRE ATLAS**
