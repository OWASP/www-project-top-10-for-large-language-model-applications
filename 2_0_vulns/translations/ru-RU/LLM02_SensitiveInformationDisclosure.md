## LLM02:2025 Утечка конфиденциальной информации

### Описание

Конфиденциальная информация может повлиять как на LLM, так и на контекст ее применения. К ней относятся персональные данные (ПД), финансовые данные, медицинские записи, конфиденциальные деловые данные, учетные данные службы безопасности и юридические документы. Кроме того, в проприетарных системах могут быть уникальные методы обучения и исходный код, которые считаются конфиденциальными, особенно в закрытых или фундаментальных моделях.

LLM, особенно если они встроены в приложения, рискуют раскрыть чувствительные данные, собственные алгоритмы или конфиденциальную информацию через свои выходные данные. Это может привести к несанкционированному доступу к данным, нарушению конфиденциальности и нарушению прав интеллектуальной собственности. Потребители должны знать, как безопасно взаимодействовать с LLM. Они должны понимать риск непреднамеренного предоставления конфиденциальных данных, которые впоследствии могут быть раскрыты в выходных данных модели.

Чтобы снизить этот риск, LLM-приложения должны выполнять соответствующую очистку данных, чтобы предотвратить попадание пользовательских данных в обучаемую модель. Владельцы приложений также должны предоставлять четкие условия использования, позволяющие пользователям отказаться от включения их данных в обучаемую модель. Добавление в системный запрос ограничений на типы данных, которые должен возвращать LLM, может обеспечить защиту от раскрытия конфиденциальной информации. Однако такие ограничения не всегда соблюдаются и могут быть обойдены с помощью Prompt Injection или других методов.

### Распространенные примеры рисков

#### 1. Утечка персональных данных (ПД)

  Персональные данные (ПД) могут быть раскрыты во время взаимодействия с LLM.

#### 2. Раскрытие проприетарных алгоритмов

  Плохо настроенные выходные данные модели могут раскрыть запатентованные алгоритмы или данные. Раскрытие данных обучения может подвергнуть модели инверсионным атакам, в ходе которых злоумышленники извлекают конфиденциальную информацию или реконструируют исходные данные. Например, как показано в атаке «Proof Pudding» (CVE-2019-20634), раскрытые обучающие данные облегчают извлечение и инверсию модели, позволяя злоумышленникам обходить средства контроля безопасности в алгоритмах машинного обучения и фильтры электронной почты.

#### 3. Раскрытие конфиденциальных бизнес-данных

  Генерируемые ответы могут непреднамеренно содержать конфиденциальную деловую информацию.

### Стратегии предотвращения и смягчения последствий

#### Очистка

#### 1. Интеграция методов очистки данных

  Реализуйте очистку данных, чтобы предотвратить попадание пользовательских данных в обучаемую модель. Это включает в себя очистку или маскировку конфиденциального содержимого перед его использованием в обучении.

#### 2. Надежная входная валидация

  Применяйте строгие методы проверки входных данных для обнаружения и отсеивания потенциально опасных или конфиденциальных данных, чтобы исключить их попадание в модель.

#### Контроль доступа

#### 1. Обеспечьте строгий контроль доступа

  Ограничьте доступ к конфиденциальным данным на основе принципа наименьших привилегий. Предоставляйте доступ только к тем данным, которые необходимы конкретному пользователю или процессу.

#### 2. Ограничьте источники данных

  Ограничьте доступ модели к внешним источникам данных и обеспечьте безопасное управление данными во время ее работы, чтобы избежать непреднамеренной утечки.

#### Федеративное обучение и методы обеспечения конфиденциальности

#### 1. Использование федеративного обучения

  Обучайте модели, используя децентрализованные данные, хранящиеся на нескольких серверах или устройствах. Такой подход сводит к минимуму необходимость централизованного сбора данных и снижает риски воздействия.

#### 2. Использование дифференциальной приватности

  Применяйте методы, которые добавляют шум в данные или выходные данные, затрудняя злоумышленникам обратный инжиниринг отдельных точек данных.

#### Обучение пользователей и прозрачность

#### 1. Обучение пользователей безопасному использованию LLM

  Предоставьте рекомендации по предотвращению ввода конфиденциальной информации. Предложите обучение лучшим практикам безопасного взаимодействия с LLM.

#### 2. Обеспечить прозрачность использования данных

  Поддерживайте четкую политику в отношении хранения, использования и удаления данных. Предоставьте пользователям возможность отказаться от включения их данных в процесс обучения.

#### Безопасная конфигурация системы

#### 1. Скрыть преамбулу системы

  Ограничьте возможности пользователей по отмене начальных настроек системы или доступу к ним, снизив риск раскрытия внутренних конфигураций.

#### 2. Ссылайтесь на передовой опыт в области неправильной конфигурации системы безопасности

  Следуйте рекомендациям, например «OWASP API8:2023 Security Misconfiguration», чтобы предотвратить утечку конфиденциальной информации через сообщения об ошибках или детали конфигурации.
  (Ссылка:[OWASP API8:2023 Security Misconfiguration](https://owasp.org/API-Security/editions/2023/en/0xa8-security-misconfiguration/))

#### Продвинутые техники

#### 1. Гомоморфное шифрование

  Используйте гомоморфное шифрование для безопасного анализа данных и машинного обучения с сохранением конфиденциальности. Это гарантирует конфиденциальность данных при их обработке моделью.

#### 2. Токенизация и редактирование

  Внедрите токенизацию для предварительной обработки и обеззараживания конфиденциальной информации. Такие методы, как сопоставление шаблонов, позволяют обнаружить и отредактировать конфиденциальный контент перед обработкой.

### Примерные сценарии атак

#### Сценарий №1: Непреднамеренное раскрытие данных

  Пользователь получает ответ, содержащий личные данные другого пользователя, из-за некорректной очистки данных.

#### Сценарий №2: Целевая Prompt Injection

  Злоумышленник обходит фильтры ввода, чтобы извлечь конфиденциальную информацию.

#### Сценарий №3: Утечка данных через обучающие данные

  Небрежное включение данных в процесс обучения приводит к раскрытию конфиденциальной информации.

### Ссылки на источники

1. [Lessons learned from ChatGPT’s Samsung leak](https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/): **Cybernews**
2. [AI data leak crisis: New tool prevents company secrets from being fed to ChatGPT](https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt): **Fox Business**
3. [ChatGPT Spit Out Sensitive Data When Told to Repeat ‘Poem’ Forever](https://www.wired.com/story/chatgpt-poem-forever-security-roundup/): **Wired**
4. [Using Differential Privacy to Build Secure Models](https://neptune.ai/blog/using-differential-privacy-to-build-secure-models-tools-methods-best-practices): **Neptune Blog**
5. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)

### Связанные фреймворки и таксономии

Обратитесь к этому разделу, чтобы получить исчерпывающую информацию, сценарии стратегий, связанных с развертыванием инфраструктуры, применяемыми средствами контроля среды и другими передовыми методами.

- [AML.T0024.000 - Infer Training Data Membership](https://atlas.mitre.org/techniques/AML.T0024.000) **MITRE ATLAS**
- [AML.T0024.001 - Invert ML Model](https://atlas.mitre.org/techniques/AML.T0024.001) **MITRE ATLAS**
- [AML.T0024.002 - Extract ML Model](https://atlas.mitre.org/techniques/AML.T0024.002) **MITRE ATLAS**
