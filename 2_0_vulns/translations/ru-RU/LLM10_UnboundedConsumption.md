## LLM10:2025 Неограниченное потребление

### Описание

Неограниченное потребление относится к процессу, при котором Большая языковая модель (LLM) генерирует ответы на запросы или подсказки. Инференция является критически важной функцией LLM, включающей применение изученных паттернов и знаний для генерации релевантных ответов или предсказаний. 

Атаки, направленные на отказ в работе сервиса, истощение финансовых ресурсов цели или даже кражу интеллектуальной собственности путем клонирования поведения модели, зависят от общей категории уязвимостей для их успешного выполнения. Неограниченное потребление возникает, когда LLM-приложение позволяет пользователям проводить чрезмерные и неконтролируемые инференции, что ведет к рискам, таким как отказ в обслуживании (DoS), финансовые потери, кража модели и деградация сервиса. Высокие вычислительные требования LLM, особенно в облачных средах, делают их уязвимыми для эксплуатации ресурсов и несанкционированного использования.

### Распространенные примеры рисков

#### 1. Переполнение ввода переменной длины
  Злоумышленники могут перегрузить LLM многочисленными вводами разной длины, используя некорректную обработку. Это может привести к истощению ресурсов и потенциальному сбою системы, что значительно повлияет на доступность сервиса.
#### 2. Denial of Wallet (DoW)
  Инициируя большое количество операций, злоумышленники используют модель оплаты за использование облачных ИИ-сервисов, что приводит к непосильным финансовым нагрузкам на поставщика и риску финансового краха.
#### 3. Переполнение непрерывного ввода
  Отправка необычно требовательных запросов, включающих сложные последовательности или сложные языковые паттерны, может истощить ресурсы системы, привести к продолжительному времени обработки и потенциальным сбоям системы.
#### 4. Запросы, требующие много ресурсов
  Submitting unusually demanding queries involving complex sequences or intricate language patterns can drain system resources, leading to prolonged processing times and potential system failures.
#### 5. Извлечение модели через API
  Злоумышленники могут использовать API модели с тщательно подобранными вводами и методами Prompt Injection для сбора достаточного количества выходных данных для воссоздания части модели или создания теневой модели. Это представляет угрозу кражи интеллектуальной собственности и подрывает целостность исходной модели.
#### 6. Функциональная репликация модели
  Использование целевой модели для генерации синтетических обучающих данных позволяет злоумышленникам дообучить другую базовую модель, создавая функциональный эквивалент. Это обходит традиционные методы извлечения через запросы, представляя значительный риск для собственных моделей и технологий.
#### 7. Побочные каналы атак
  Злоумышленники могут использовать методы фильтрации ввода модели для выполнения побочных каналов атак, собирая веса модели и информацию о ее архитектуре. Это может скомпрометировать безопасность модели и привести к дальнейшему использованию.

### Стратегии предотвращения и смягчения последствий

#### 1. Проверка ввода
  Реализуйте строгую проверку ввода, чтобы гарантировать, что вводы не превышают разумные ограничения по размеру.
#### 2. Ограничение экспозиции логитов и логарифмов вероятности
  Ограничьте `logit_bias` и `logprobs` в ответах API. Предоставляйте только необходимую информацию, не раскрывая детализированные вероятности.
#### 3. Ограничение частоты запросов
  Применяйте ограничение частоты запросов и квоты пользователей, чтобы ограничить количество запросов, которые может сделать один источник за определенный период времени.
#### 4. Управление распределением ресурсов
  Динамически контролируйте распределение ресурсов, чтобы предотвратить потребление чрезмерных ресурсов одним пользователем или запросом.
#### 5. Тайм-ауты и ограничение скорости
  Устанавливайте тайм-ауты и ограничивайте обработку ресурсоемких операций, чтобы предотвратить продолжительное потребление ресурсов.
#### 6.Техники песочницы
  Ограничьте доступ LLM к сетевым ресурсам, внутренним сервисам и API.  
  - Это особенно важно для всех обычных сценариев, так как охватывает риски и угрозы со стороны инсайдеров. Кроме того, это регулирует степень доступа, которую приложение с использованием LLM имеет к данным и ресурсам, служа важным механизмом контроля для смягчения или предотвращения побочных канальных атак.
#### 7. Комплексный мониторинг, ведение журнала и обнаружение аномалий
  Постоянно мониторьте использование ресурсов и внедрите ведение журнала для обнаружения и реагирования на необычные паттерны потребления ресурсов.
#### 8. Водяные знаки
  Реализуйте системы водяных знаков для встраивания и обнаружения несанкционированного использования выходных данных LLM.
#### 9. Плавное снижение нагрузки
  Разработайте систему, которая будет плавно снижать функциональность при сильной нагрузке, поддерживая частичную функциональность, а не полное падение системы.
#### 10. Ограничение очереди действий и масштабирование
  Реализуйте ограничения на количество действий в очереди и общее количество действий, при этом внедряйте динамическое масштабирование и балансировку нагрузки для обработки переменных требований и обеспечения стабильной работы системы.
#### 11. Обучение на устойчивость к атакам
  Обучайте модели обнаруживать и смягчать атаки с помощью враждебных запросов и попыток извлечения данных.
#### 12. Фильтрация токенов с ошибками
  Создайте списки известных токенов с ошибками и проверяйте выходные данные перед их добавлением в контекстное окно модели.
#### 13. Контроль доступа
  Реализуйте строгие механизмы контроля доступа, включая управление доступом на основе ролей (RBAC) и принцип наименьших привилегий, чтобы ограничить несанкционированный доступ к репозиториям моделей LLM и тренировочным средам.
#### 14. Централизованный реестр моделей ML
  Используйте централизованный реестр моделей машинного обучения для моделей, используемых в производстве, обеспечивая надлежащее управление и контроль доступа.
#### 15.Автоматизированное развертывание MLOps
  Реализуйте автоматизированное развертывание MLOps с управлением, отслеживанием и рабочими процессами утверждения для ужесточения контроля доступа и развертывания в инфраструктуре.

### Примерные сценарии атак

#### Сценарий №1: Неконтролируемый размер ввода
  Злоумышленник подает необычно большой ввод в приложение на базе LLM, обрабатывающее текстовые данные, что приводит к чрезмерному использованию памяти и загрузке процессора, что может привести к сбою системы или значительному замедлению работы сервиса.
#### Сценарий №2: Повторяющиеся запросы
  Злоумышленник отправляет большое количество запросов в API LLM, вызывая чрезмерное потребление вычислительных ресурсов и делая сервис недоступным для легитимных пользователей.
#### Сценарий №3: Запросы, требующие много ресурсов
  Злоумышленник создает специфические запросы, предназначенные для запуска наиболее ресурсоемких процессов LLM, что приводит к продолжительному использованию процессора и потенциальному сбою системы.
#### Сценарий №4: Denial of Wallet (DoW)
  Злоумышленник генерирует чрезмерное количество операций, чтобы воспользоваться моделью оплаты за использование облачных ИИ-сервисов, вызывая непосильные расходы для поставщика сервиса.
#### Сценарий №5: Функциональная репликация модели
  Злоумышленник использует API LLM для генерации синтетических тренировочных данных и дообучения другой модели, создавая функциональный эквивалент и обходя традиционные ограничения извлечения модели.
#### Сценарий №6: Обход фильтрации ввода системы
  Злоумышленник, обходя методы фильтрации ввода и предшествующие операции модели, выполняет побочную канальную атаку и извлекает информацию о модели на удаленно управляемый ресурс под его контролем.

### Ссылки на источники

1. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)
2. [arXiv:2403.06634 Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634) **arXiv**
3. [Runaway LLaMA | How Meta's LLaMA NLP model leaked](https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/): **Deep Learning Blog**
4. [You wouldn't download an AI, Extracting AI models from mobile apps](https://altayakkus.substack.com/p/you-wouldnt-download-an-ai): **Substack blog**
5. [A Comprehensive Defense Framework Against Model Extraction Attacks](https://ieeexplore.ieee.org/document/10080996): **IEEE**
6. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html): **Stanford Center on Research for Foundation Models (CRFM)**
7. [How Watermarking Can Help Mitigate The Potential Risks Of LLMs?](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html): **KD Nuggets**
8. [Securing AI Model Weights Preventing Theft and Misuse of Frontier Models](https://www.rand.org/content/dam/rand/pubs/research_reports/RRA2800/RRA2849-1/RAND_RRA2849-1.pdf)
9. [Sponge Examples: Energy-Latency Attacks on Neural Networks: Arxiv White Paper](https://arxiv.org/abs/2006.03463) **arXiv**
10. [Sourcegraph Security Incident on API Limits Manipulation and DoS Attack](https://about.sourcegraph.com/blog/security-update-august-2023) **Sourcegraph**

### Связанные фреймворки и таксономии

Смотрите этот раздел для получения подробной информации, сценариев и стратегий, связанных с развертыванием инфраструктуры, применяемыми контролями окружающей среды и другими лучшими практиками.  

- [MITRE CWE-400: Uncontrolled Resource Consumption](https://cwe.mitre.org/data/definitions/400.html) **MITRE Common Weakness Enumeration**
- [AML.TA0000 ML Model Access: Mitre ATLAS](https://atlas.mitre.org/tactics/AML.TA0000) & [AML.T0024 Exfiltration via ML Inference API](https://atlas.mitre.org/techniques/AML.T0024) **MITRE ATLAS**
- [AML.T0029 - Denial of ML Service](https://atlas.mitre.org/techniques/AML.T0029) **MITRE ATLAS**
- [AML.T0034 - Cost Harvesting](https://atlas.mitre.org/techniques/AML.T0034) **MITRE ATLAS**
- [AML.T0025 - Exfiltration via Cyber Means](https://atlas.mitre.org/techniques/AML.T0025) **MITRE ATLAS**
- [OWASP Machine Learning Security Top Ten - ML05:2023 Model Theft](https://owasp.org/www-project-machine-learning-security-top-10/docs/ML05_2023-Model_Theft.html) **OWASP ML Top 10**
- [API4:2023 - Unrestricted Resource Consumption](https://owasp.org/API-Security/editions/2023/en/0xa4-unrestricted-resource-consumption/) **OWASP Web Application Top 10**
- [OWASP Resource Management](https://owasp.org/www-project-secure-coding-practices-quick-reference-guide/) **OWASP Secure Coding Practices**
