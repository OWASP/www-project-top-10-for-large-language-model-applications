## LLM07:2025 تسريب تعليمة النظام

### الوصف

تشير ثغرة تسريب تعليمة النظام (System Prompt Leakage) في نماذج اللغة الكبيرة إلى الخطر المتمثل في أن تعليمات النظام أو التوجيهات المستخدمة لتوجيه سلوك النموذج قد تحتوي أيضًا على معلومات حساسة لم يكن من المفترض كشفها. تُصمم تعليمات النظام لتوجيه مخرجات النموذج بناءً على متطلبات التطبيق، لكنها قد تحتوي عن غير قصد على أسرار. عند اكتشاف هذه المعلومات، يمكن استغلالها لتسهيل تنفيذ هجمات أخرى.

من المهم أن نفهم أن تعليمة النظام لا يجب اعتبارها سرًا، ولا يجب استخدامها كوسيلة للتحكم الأمني. وبالتالي، يجب ألا تحتوي لغة تعليمة النظام على بيانات حساسة مثل بيانات الاعتماد، أو سلاسل الاتصال (Connection Strings)، أو ما شابه.

وبالمثل، إذا احتوت تعليمة النظام على معلومات تصف أدوار وصلاحيات مختلفة، أو بيانات حساسة مثل سلاسل الاتصال أو كلمات المرور، فإنه رغم أن كشف هذه المعلومات قد يكون مفيدًا، إلا أن الخطر الأمني الأساسي لا يكمن في مجرد كشفها، بل يكمن في أن التطبيق يسمح بتجاوز إدارة الجلسات الصارمة وفحوصات التفويض عبر تفويض هذه المهام إلى نموذج اللغة الكبير، وأيضًا في أن البيانات الحساسة يتم تخزينها في مكان لا ينبغي تخزينها فيه.

بإيجاز: كشف تعليمة النظام بحد ذاته لا يمثل الخطر الحقيقي — الخطر الأمني يكمن في العناصر الأساسية المرتبطة، سواء كان ذلك كشف معلومات حساسة، أو تجاوز حواجز الحماية للنظام، أو الفصل غير السليم للصلاحيات، وغيرها. حتى لو لم يتم الكشف عن الصياغة الدقيقة لتعليمة النظام، سيتمكن المهاجمون الذين يتفاعلون مع النظام، تقريبًا بشكل مؤكد، من استنتاج العديد من حواجز الحماية والقيود على التنسيق الموجودة ضمن لغة تعليمة النظام، من خلال استخدام التطبيق، وإرسال مدخلات للنموذج، وملاحظة النتائج.

### أمثلة شائعة على المخاطر

#### 1. كشف الوظائف الحساسة
  قد تكشف تعليمة النظام للتطبيق عن معلومات أو وظائف حساسة كان من المفترض أن تبقى سرية، مثل بنية النظام الحساسة، مفاتيح واجهات البرمجة (API Keys)، بيانات اعتماد قواعد البيانات، أو رموز المستخدمين (User Tokens). يمكن استخراج هذه المعلومات أو استغلالها من قِبل المهاجمين للحصول على وصول غير مصرح به إلى التطبيق. على سبيل المثال، قد يؤدي وجود نوع قاعدة البيانات ضمن تعليمة النظام إلى استهدافها بهجمات حقن SQL.
#### 2. كشف القواعد الداخلية
  قد تكشف تعليمة النظام للتطبيق عن معلومات تتعلق بعمليات اتخاذ القرار الداخلي والتي يجب أن تبقى سرية. تُمكّن هذه المعلومات المهاجمين من فهم كيفية عمل التطبيق، مما قد يسمح لهم باستغلال نقاط الضعف أو تجاوز الضوابط داخل التطبيق. على سبيل المثال — في تطبيق مصرفي يحتوي على روبوت دردشة، قد تكشف تعليمة النظام معلومات مثل:
    >"تم تحديد حد المعاملات اليومية للمستخدم بمبلغ 5000 دولار. وإجمالي مبلغ القرض المسموح به للمستخدم هو 10،000 دولار."
  تسمح هذه المعلومات للمهاجمين بتجاوز ضوابط الأمان مثل إجراء معاملات تتجاوز الحد اليومي أو تجاوز إجمالي القرض المسموح به.
#### 3. كشف معايير التصفية
  قد تضمن تعليم النظام طلبًا من النموذج بتصفية أو رفض محتوى حساس. على سبيل المثال، قد تحتوي على:
    >"إذا طلب المستخدم معلومات عن مستخدم آخر، دائمًا رد بـ: 'عذرًا، لا يمكنني المساعدة في هذا الطلب'."
#### 4. كشف الصلاحيات وأدوار المستخدمين
  قد تكشف تعليمة النظام للتطبيق عن بنية الأدوار الداخلية أو مستويات الصلاحيات في التطبيق. على سبيل المثال، قد تكشف ما يلي:
    >"دور المستخدم الإداري يمنح صلاحية كاملة لتعديل سجلات المستخدمين."
  إذا علم المهاجمون بهذه الصلاحيات المعتمدة على الأدوار، فقد يحاولون تنفيذ هجوم تصعيد الامتيازات.

### استراتيجيات الوقاية والتخفيف

#### 1. فصل البيانات الحساسة عن تعليمة النظام
  تجنب تضمين أي معلومات حساسة (مثل مفاتيح واجهات البرمجة، مفاتيح المصادقة، أسماء قواعد البيانات، أدوار المستخدمين، أو بنية صلاحيات التطبيق) مباشرةً ضمن تعليمات النظام. بدلاً من ذلك، قم بفصل هذه المعلومات إلى أنظمة لا يصل إليها النموذج بشكل مباشر.
#### 2. تجنب الاعتماد على تعليمات النظام للتحكم الصارم في السلوك
  نظرًا لأن نماذج اللغة الكبيرة عرضة لهجمات أخرى مثل حقن التعليمات التي يمكن أن تغير تعليمة النظام، يُوصى بتجنب استخدام تعليمة النظام للتحكم في سلوك النموذج حيثما أمكن. بدلاً من ذلك، يجب الاعتماد على أنظمة خارجية عن نموذج اللغة الكبير لضمان هذا السلوك. على سبيل المثال، يجب أن يتم اكتشاف المحتوى الضار ومنعه عبر أنظمة خارجية.
#### 3. تطبيق حواجز الحماية
  قم بتنفيذ نظام لحواجز الحماية خارج نموذج اللغة الكبير نفسه. رغم أن تدريب النموذج على سلوك معين مثل عدم كشف تعليمة النظام قد يكون فعالًا، إلا أنه لا يضمن دائمًا التزام النموذج بذلك. يُفضل وجود نظام مستقل قادر على فحص المخرجات للتأكد من التزام النموذج بالتوقعات بدلاً من الاعتماد فقط على تعليمة النظام.
#### 4. التأكد من أن الضوابط الأمنية تُفرض بشكل مستقل عن نموذج اللغة الكبير
  يجب ألا يتم تفويض الضوابط الأساسية مثل فصل الصلاحيات، وفحوصات حدود التفويض، وما شابه، إلى نموذج اللغة الكبير، سواء من خلال تعليمة النظام أو بطرق أخرى. يجب تنفيذ هذه الضوابط بطريقة حتمية وقابلة للتدقيق، وهو ما لا تدعمه نماذج اللغة الكبيرة حاليًا. وفي الحالات التي يقوم فيها وكيل (Agent) بتنفيذ مهام تتطلب مستويات وصول مختلفة، يجب استخدام عدة وكلاء، بحيث يتم تهيئة كل وكيل بأقل امتيازات لازمة لتنفيذ المهام المطلوبة.

### أمثلة على سيناريوهات الهجوم

#### السيناريو  #1
  يمتلك نموذج اللغة الكبير تعليمة نظام تحتوي على مجموعة من بيانات الاعتماد المستخدمة لأداة تم منح النموذج حق الوصول إليها. يتم تسريب تعليمة النظام إلى المهاجم، مما يمكنه من استخدام هذه البيانات لأغراض أخرى.
#### السيناريو  #2
  يمتلك نموذج اللغة الكبير تعليمة نظام تمنع توليد محتوى مسيء، أو روابط خارجية، أو تنفيذ تعليمات البرمجية. يقوم المهاجم باستخراج تعليمة النظام هذه، ثم يستخدم هجوم حقن التعليمات لتجاوز هذه التعليمة، مما يسهل تنفيذ هجوم تنفيذ تعليمات برمجية عن بُعد.

### روابط مرجعية

1. [SYSTEM PROMPT LEAK](https://x.com/elder_plinius/status/1801393358964994062): Pliny the prompter
2. [Prompt Leak](https://www.prompt.security/vulnerabilities/prompt-leak): Prompt Security
3. [chatgpt_system_prompt](https://github.com/LouisShark/chatgpt_system_prompt): LouisShark
4. [leaked-system-prompts](https://github.com/jujumilk3/leaked-system-prompts): Jujumilk3
5. [OpenAI Advanced Voice Mode System Prompt](https://x.com/Green_terminals/status/1839141326329360579): Green_Terminals

### الأطر والتصنيفات ذات الصلة 

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة التطبيقية، وأفضل الممارسات الأخرى.

- [AML.T0051.000 - LLM Prompt Injection: Direct (Meta Prompt Extraction)](https://atlas.mitre.org/techniques/AML.T0051.000) **MITRE ATLAS**
