## LLM07:2025 تسريب التعليمات النظامية

### الوصف

تشير ثغرة تسريب التعليمات النظامية (System Prompt Leakage) في نماذج اللغة الكبيرة (LLMs) إلى الخطر المتمثل في أن التعليمات النظامية أو التوجيهات (System Prompts) المستخدمة لتوجيه سلوك النموذج قد تحتوي أيضًا على معلومات حساسة لم يكن من المفترض كشفها. تُصمم التعليمات النظامية لتوجيه مخرجات النموذج بناءً على متطلبات التطبيق، لكنها قد تحتوي عن غير قصد على أسرار. عند اكتشاف هذه المعلومات، يمكن استغلالها لتسهيل تنفيذ هجمات أخرى.

من المهم أن نفهم أن التعليمات النظامية لا يجب اعتبارها سرًا، ولا يجب استخدامها كوسيلة للتحكم الأمني (Security Control). وبالتالي، يجب ألا تحتوي لغة التعليمات النظامية على بيانات حساسة مثل بيانات الاعتماد (Credentials)، أو سلاسل الاتصال (Connection Strings)، أو ما شابه.

وبالمثل، إذا احتوت التعليمات النظامية على معلومات تصف أدوار وصلاحيات مختلفة، أو بيانات حساسة مثل سلاسل الاتصال أو كلمات المرور، فإنه رغم أن كشف هذه المعلومات قد يكون مفيدًا، إلا أن الخطر الأمني الأساسي لا يكمن في مجرد كشفها، بل يكمن في أن التطبيق يسمح بتجاوز إدارة الجلسات الصارمة وفحوصات التفويض (Authorization Checks) عبر تفويض هذه المهام إلى نموذج اللغة الكبير (LLM)، وأيضًا في أن البيانات الحساسة يتم تخزينها في مكان لا ينبغي تخزينها فيه.

بإيجاز: كشف التعليمات النظامية بحد ذاته لا يمثل الخطر الحقيقي — الخطر الأمني يكمن في العناصر الأساسية المرتبطة، سواء كان ذلك كشف معلومات حساسة (Sensitive Information Disclosure)، أو تجاوز حواجز الحماية للنظام (System Guardrails Bypass)، أو الفصل غير السليم للصلاحيات (Improper Separation of Privileges)، وغيرها. حتى لو لم يتم الكشف عن الصياغة الدقيقة للتعليمات النظامية، سيتمكن المهاجمون الذين يتفاعلون مع النظام، تقريبًا بشكل مؤكد، من استنتاج العديد من حواجز الحماية (Guardrails) والقيود على التنسيق (Formatting Restrictions) الموجودة ضمن لغة التعليمات النظامية، من خلال استخدام التطبيق، وإرسال مدخلات للنموذج، وملاحظة النتائج.

### أمثلة شائعة على المخاطر

#### 1. كشف الوظائف الحساسة
  قد تكشف التعليمة النظامية (System Prompt) للتطبيق عن معلومات أو وظائف حساسة كان من المفترض أن تبقى سرية، مثل بنية النظام الحساسة (Sensitive System Architecture)، مفاتيح واجهات البرمجة (API Keys)، بيانات اعتماد قواعد البيانات (Database Credentials)، أو رموز المستخدمين (User Tokens). يمكن استخراج هذه المعلومات أو استغلالها من قِبل المهاجمين للحصول على وصول غير مصرح به إلى التطبيق. على سبيل المثال، قد يؤدي وجود نوع قاعدة البيانات ضمن التعليمات النظامية إلى استهدافها بهجمات الحقن (SQL Injection Attacks).
#### 2. كشف القواعد الداخلية
  قد تكشف التعليمة النظامية (System Prompt) للتطبيق عن معلومات تتعلق بعمليات اتخاذ القرار الداخلي والتي يجب أن تبقى سرية. تُمكّن هذه المعلومات المهاجمين من فهم كيفية عمل التطبيق، مما قد يسمح لهم باستغلال نقاط الضعف أو تجاوز الضوابط داخل التطبيق. على سبيل المثال — في تطبيق مصرفي يحتوي على روبوت دردشة (Chatbot)، قد تكشف التعليمات النظامية معلومات مثل:
    >"تم تحديد حد المعاملات اليومية للمستخدم بمبلغ 5000 دولار. وإجمالي مبلغ القرض المسموح به للمستخدم هو 10,000 دولار."
  تسمح هذه المعلومات للمهاجمين بتجاوز ضوابط الأمان مثل إجراء معاملات تتجاوز الحد اليومي أو تجاوز إجمالي القرض المسموح به.
#### 3. كشف معايير التصفية
  قد تضمن التعليمة النظامية (System Prompt) طلبًا من النموذج بتصفية أو رفض محتوى حساس. على سبيل المثال، قد تحتوي التعليمات النظامية للنموذج على:
    >"إذا طلب المستخدم معلومات عن مستخدم آخر، دائمًا رد بـ: 'عذرًا، لا يمكنني المساعدة في هذا الطلب'."
#### 4. كشف الصلاحيات وأدوار المستخدمين
  قد تكشف التعليمة النظامية (System Prompt) للتطبيق عن بنية الأدوار الداخلية (Internal Role Structures) أو مستويات الصلاحيات (Permission Levels) في التطبيق. على سبيل المثال، قد تكشف تعليمة نظامية (System Prompt) ما يلي:
    >"دور المستخدم الإداري (Admin User Role) يمنح صلاحية كاملة لتعديل سجلات المستخدمين."
  إذا علم المهاجمون بهذه الصلاحيات المعتمدة على الأدوار (Role-Based Permissions)، فقد يحاولون تنفيذ هجوم تصعيد الامتيازات (Privilege Escalation Attack).

### استراتيجيات الوقاية والتخفيف

#### 1. فصل البيانات الحساسة عن التعليمات النظامية
  تجنب تضمين أي معلومات حساسة (مثل مفاتيح واجهات البرمجة (API Keys)، مفاتيح المصادقة (Auth Keys)، أسماء قواعد البيانات (Database Names)، أدوار المستخدمين (User Roles)، أو بنية صلاحيات التطبيق (Permission Structure of the Application)) مباشرةً ضمن التعليمات النظامية (System Prompts). بدلاً من ذلك، قم بفصل هذه المعلومات إلى أنظمة لا يصل إليها النموذج بشكل مباشر.
#### 2. تجنب الاعتماد على التعليمات النظامية للتحكم الصارم في السلوك
  نظرًا لأن نماذج اللغة الكبيرة (LLMs) عرضة لهجمات أخرى مثل حقن التعليمات (Prompt Injections) التي يمكن أن تغير التعليمة النظامية (System Prompt)، يُوصى بتجنب استخدام التعليمات النظامية للتحكم في سلوك النموذج حيثما أمكن. بدلاً من ذلك، يجب الاعتماد على أنظمة خارجية عن نموذج اللغة الكبير (LLM) لضمان هذا السلوك. على سبيل المثال، يجب أن يتم اكتشاف المحتوى الضار ومنعه عبر أنظمة خارجية (External Systems).
#### 3. تطبيق حواجز الحماية
  قم بتنفيذ نظام لحواجز الحماية (Guardrails) خارج نموذج اللغة الكبير (LLM) نفسه. رغم أن تدريب النموذج على سلوك معين مثل عدم كشف التعليمات النظامية قد يكون فعالًا، إلا أنه لا يضمن دائمًا التزام النموذج بذلك. يُفضل وجود نظام مستقل قادر على فحص المخرجات للتأكد من التزام النموذج بالتوقعات بدلاً من الاعتماد فقط على التعليمات النظامية.
#### 4. التأكد من أن الضوابط الأمنية تُفرض بشكل مستقل عن نموذج اللغة الكبير
  يجب ألا يتم تفويض الضوابط الأساسية مثل فصل الصلاحيات (Privilege Separation)، وفحوصات حدود التفويض (Authorization Bounds Checks)، وما شابه، إلى نموذج اللغة الكبير (LLM)، سواء من خلال التعليمات النظامية أو بطرق أخرى. يجب تنفيذ هذه الضوابط بطريقة حتمية وقابلة للتدقيق، وهو ما لا تدعمه نماذج LLM حاليًا. وفي الحالات التي يقوم فيها وكيل (Agent) بتنفيذ مهام تتطلب مستويات وصول مختلفة، يجب استخدام عدة وكلاء، بحيث يتم تهيئة كل وكيل بأقل امتيازات لازمة لتنفيذ المهام المطلوبة.

### أمثلة على سيناريوهات الهجوم

#### السيناريو  #1
  يمتلك نموذج اللغة الكبير (LLM) تعليمًا نظاميًا يحتوي على مجموعة من بيانات الاعتماد (Credentials) المستخدمة لأداة تم منح النموذج حق الوصول إليها. يتم تسريب التعليمات النظامية إلى المهاجم، مما يمكنه من استخدام هذه البيانات لأغراض أخرى.
#### السيناريو  #2
  يمتلك نموذج اللغة الكبير (LLM) تعليمًا نظاميًا يمنع توليد محتوى مسيء، أو الروابط الخارجية، أو تنفيذ الشيفرة البرمجية. يقوم المهاجم باستخراج هذه التعليمة النظامية، ثم يستخدم هجوم حقن التعليمات (Prompt Injection) لتجاوز هذه التعليمات، مما يسهل تنفيذ هجوم تنفيذ تعليمات برمجية عن بُعد (Remote Code Execution Attack).

### روابط مرجعية

1. [SYSTEM PROMPT LEAK](https://x.com/elder_plinius/status/1801393358964994062): Pliny the prompter
2. [Prompt Leak](https://www.prompt.security/vulnerabilities/prompt-leak): Prompt Security
3. [chatgpt_system_prompt](https://github.com/LouisShark/chatgpt_system_prompt): LouisShark
4. [leaked-system-prompts](https://github.com/jujumilk3/leaked-system-prompts): Jujumilk3
5. [OpenAI Advanced Voice Mode System Prompt](https://x.com/Green_terminals/status/1839141326329360579): Green_Terminals

### االأطر والتصنيفات ذات الصلة 

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة التطبيقية، وأفضل الممارسات الأخرى.

- [AML.T0051.000 - LLM Prompt Injection: Direct (Meta Prompt Extraction)](https://atlas.mitre.org/techniques/AML.T0051.000) **MITRE ATLAS**
