## LLM05:2025 التعامل غير السليم مع المخرجات (Improper Output Handling)
### الوصف

يشير التعامل غير السليم مع المخرجات إلى ضعف التحقق، أو التنقية، أو المعالجة لمخرجات نماذج اللغة الكبيرة (LLMs) قبل تمريرها إلى مكونات أو أنظمة أخرى. نظرًا لأن المحتوى الذي تنتجه النماذج يمكن التحكم فيه من خلال مدخلات التعليمات (Prompt Input)، فإن هذا السلوك يشبه منح المستخدمين وصولًا غير مباشر إلى وظائف إضافية.

يختلف التعامل غير السليم مع المخرجات عن الاعتماد المفرط (Overreliance)، إذ أن التعامل غير السليم يركز على مخرجات النموذج قبل تمريرها للأنظمة الأخرى، بينما يعالج الاعتماد المفرط القضايا الأوسع المرتبطة بالثقة الزائدة في دقة أو ملاءمة مخرجات النموذج.

يمكن أن يؤدي استغلال ناجح لثغرة التعامل غير السليم مع المخرجات إلى تنفيذ هجمات من نوع XSS وCSRF في المتصفحات، وكذلك SSRF، أو تصعيد الامتيازات، أو تنفيذ تعليمات برمجية عن بُعد (Remote Code Execution) على أنظمة الخوادم الخلفية.
تُساهم العوامل التالية في زيادة أثر هذه الثغرة:

- منح التطبيق للنموذج امتيازات تتجاوز ما هو مخصص للمستخدمين النهائيين، مما يتيح تصعيد الامتيازات أو تنفيذ تعليمات برمجية عن بُعد.
- تعرض التطبيق لهجمات حقن تعليمات غير مباشرة (Indirect Prompt Injection)، مما قد يمكّن المهاجم من الحصول على وصول بامتيازات عالية لبيئة المستخدم المستهدف.
- عدم قيام الإضافات الخارجية (Third-party Extensions) بالتحقق الكافي من المدخلات.
- غياب الترميز السليم للمخرجات بما يتناسب مع السياقات المختلفة (مثل: HTML، JavaScript، SQL).
- ضعف أو غياب المراقبة وتسجيل الأحداث (Monitoring and Logging) المتعلقة بمخرجات النموذج.
- عدم تطبيق قيود على معدلات الاستخدام (Rate Limiting) أو استخدام آليات لاكتشاف السلوك غير الطبيعي (Anomaly Detection) أثناء تفاعل النموذج مع المستخدمين.


### أمثلة شائعة على الثغرات

1. يتم إدخال مخرجات نموذج اللغة الكبير (LLM) مباشرةً في غلاف نظام (System Shell) أو دالة مماثلة مثل exec أو eval، مما يؤدي إلى تنفيذ تعليمات برمجية عن بُعد (Remote Code Execution).
2. يُولّد النموذج تعليمات برمجية بلغة JavaScript أو Markdown ويتم إرجاعها إلى المستخدم، حيث يتم تفسير الشيفرة بواسطة المتصفح، مما يؤدي إلى هجوم XSS.
3. تُنفذ استعلامات SQL التي يولّدها النموذج دون استخدام المعاملات المُخصصة (Proper Parameterization)، مما يؤدي إلى حدوث هجوم الحقن (SQL Injection).
4. يتم استخدام مخرجات نموذج اللغة الكبير في إنشاء مسارات ملفات بدون تنقية مناسبة، مما قد يؤدي إلى ثغرات اجتياز المسار (Path Traversal).
5. يتم استخدام المحتوى الذي يولده نموذج اللغة الكبير في قوالب البريد الإلكتروني بدون ترميز مناسب (Proper Escaping)، مما قد يؤدي إلى هجمات تصيّد (Phishing Attacks).



### استراتيجيات الوقاية والتخفيف

1. عامِل النموذج كما تعامل أي مستخدم آخر، مع اتباع نهج عدم الثقة المطلق (Zero-Trust Approach)، وطبق التحقق المناسب من المدخلات على الاستجابات القادمة من النموذج إلى وظائف الخوادم الخلفية.
2. اتبع إرشادات OWASP ASVS (معيار التحقق من أمان التطبيقات) لضمان فعالية التحقق وتنقية المدخلات.
3. قم بترميز مخرجات النموذج قبل إرجاعها إلى المستخدمين للحد من تنفيذ الكود غير المرغوب فيه بواسطة JavaScript أو Markdown. يوفر OWASP ASVS إرشادات تفصيلية حول ترميز المخرجات.
4. نفّذ ترميزًا للمخرجات يتناسب مع السياق الذي ستُستخدم فيه مخرجات النموذج (مثل: ترميز HTML للمحتوى الويب، أو ترميز SQL لاستعلامات قواعد البيانات).
5. استخدم الاستعلامات المُعَلمَة (Parameterized Queries) أو البيانات المُجهزة مسبقًا (Prepared Statements) لجميع العمليات المتعلقة بقاعدة البيانات والتي تتضمن مخرجات النموذج.
6. طبق سياسات أمن محتوى صارمة (Strict Content Security Policies - CSP) لتقليل مخاطر هجمات XSS الناتجة عن المحتوى الذي يولده النموذج.
7. نفّذ أنظمة قوية لتسجيل ومراقبة الأحداث (Robust Logging and Monitoring Systems) لاكتشاف الأنماط غير الطبيعية في مخرجات النموذج والتي قد تشير إلى محاولات استغلال.

### أمثلة على سيناريوهات الهجوم

#### السيناريو #1
تستخدم تطبيق إضافة (Extension) مبنية على نموذج اللغة الكبير (LLM) لتوليد استجابات لميزة الدردشة.
تقدم هذه الإضافة أيضًا عددًا من الوظائف الإدارية المتاحة لنموذج لغة آخر يمتلك امتيازات إضافية.
يمرر نموذج اللغة العام استجابته مباشرة، دون التحقق السليم من المخرجات، إلى الإضافة، مما يتسبب في إيقاف الإضافة للصيانة.
#### السيناريو #2
  يستخدم مستخدم أداة تلخيص مواقع إلكترونية تعتمد على نموذج اللغة الكبير (LLM) لإنشاء ملخص لمقال. تتضمن صفحة الموقع حقن تعليمات (Prompt Injection) تطلب من النموذج التقاط محتوى حساس من الموقع أو من محادثة المستخدم. بعد ذلك، يمكن للنموذج ترميز البيانات الحساسة وإرسالها، دون تحقق أو تصفية، إلى خادم خاضع لسيطرة المهاجم.

#### السيناريو #3
يسمح نموذج اللغة الكبير (LLM) للمستخدمين بإنشاء استعلامات SQL لقاعدة بيانات خلفية عبر ميزة شبيهة بالدردشة. يطلب مستخدم إنشاء استعلام لحذف جميع جداول قاعدة البيانات. إذا لم يتم فحص الاستعلام الذي أنشأه النموذج بشكل صحيح، فقد يتم حذف جميع جداول قاعدة البيانات.


#### السيناريو #4
 يستخدم تطبيق الويب نموذج اللغة الكبير (LLM) لتوليد محتوى بناءً على تعليمات نصية من المستخدمين دون تنقية للمخرجات. يمكن لمهاجم إرسال تعليمات مصممة خصيصًا، مما يؤدي إلى أن يقوم النموذج بإرجاع حمولة JavaScript غير مُنقّاة، مما يؤدي إلى هجوم XSS عند عرضها في متصفح الضحية. عدم التحقق الكافي من التعليمات سمح بحدوث هذا الهجوم.


#### السيناريو #5
يُستخدم نموذج اللغة الكبير (LLM) لإنشاء قوالب رسائل بريد إلكتروني ديناميكية لحملة تسويقية. يقوم مهاجم بالتلاعب بالنموذج لإدخال شفرة JavaScript ضارة ضمن محتوى البريد الإلكتروني. إذا لم يقم التطبيق بتنقية مخرجات النموذج بشكل صحيح، فقد يؤدي ذلك إلى هجمات XSS على المستلمين الذين يعرضون البريد الإلكتروني في عملاء بريد إلكتروني معرضين للخطر.


#### السيناريو #6
تستخدم شركة برمجيات نموذج اللغة الكبير (LLM) لتوليد كود برمجي من مدخلات لغوية طبيعية بهدف تسريع مهام التطوير. رغم الفعالية، إلا أن هذا النهج يعرض الشركة لخطر تسريب معلومات حساسة، أو إنشاء طرق معالجة بيانات غير آمنة، أو إدخال ثغرات مثل ثغرات الحقن. قد يقوم النموذج أيضًا بهلوسة توصيات بحزم برمجية غير موجودة، مما قد يؤدي إلى تحميل موارد مصابة ببرمجيات خبيثة. لذلك، من الضروري إجراء مراجعة دقيقة للشيفرة والتحقق من الحزم المقترحة أمرين بالغَي الأهمية لمنع الخروقات الأمنية، والوصول غير المصرح به، واختراق الأنظمة.



### روابط مرجعية

1. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)
2. [Arbitrary Code Execution](https://security.snyk.io/vuln/SNYK-PYTHON-LANGCHAIN-5411357): **Snyk Security Blog**
3. [ChatGPT Plugin Exploit Explained: From Prompt Injection to Accessing Private Data](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./): **Embrace The Red**
4. [New prompt injection attack on ChatGPT web version. Markdown images can steal your chat data.](https://systemweakness.com/new-prompt-injection-attack-on-chatgpt-web-version-ef717492c5c2?gi=8daec85e2116): **System Weakness**
5. [Don’t blindly trust LLM responses. Threats to chatbots](https://embracethered.com/blog/posts/2023/ai-injections-threats-context-matters/): **Embrace The Red**
6. [Threat Modeling LLM Applications](https://aivillage.org/large%20language%20models/threat-modeling-llm/): **AI Village**
7. [OWASP ASVS - 5 Validation, Sanitization and Encoding](https://owasp-aasvs4.readthedocs.io/en/latest/V5.html#validation-sanitization-and-encoding): **OWASP AASVS**
8. [AI hallucinates software packages and devs download them – even if potentially poisoned with malware](https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/) **Theregiste**

