## LLM03:2025 سلاسل التوريد

### الوصف 

سلاسل توريد نماذج اللغة الكبيرة (LLM Supply Chains) عرضة لمجموعة متنوعة من الثغرات، والتي يمكن أن تؤثر على سلامة بيانات التدريب، والنماذج، ومنصات النشر. قد تؤدي هذه المخاطر إلى مخرجات متحيزة، أو اختراقات أمنية، أو أعطال في الأنظمة. بينما تركز الثغرات في البرمجيات التقليدية على قضايا مثل عيوب الشيفرة البرمجية والاعتماديات، تمتد المخاطر في تعلم الآلة (ML) أيضًا إلى النماذج المدربة مسبقًا من أطراف ثالثة والبيانات المستخدمة.

يمكن التلاعب بهذه العناصر الخارجية من خلال هجمات التلاعب (Tampering) أو تسميم البيانات (Poisoning Attacks).

يُعد إنشاء نماذج اللغة الكبيرة مهمة متخصصة غالبًا ما تعتمد على نماذج من أطراف ثالثة. يؤدي انتشار نماذج مفتوحة الوصول (Open-Access LLMs) وطرق الضبط الجديدة مثل التكيف منخفض الرتبة (LoRA – Low-Rank Adaptation) والضبط الفعّال للمعلمات (PEFT – Parameter-Efficient Fine-Tuning)، خاصة على منصات مثل Hugging Face، إلى ظهور مخاطر جديدة في سلسلة التوريد. وأخيرًا، فإن ظهور نماذج اللغة الكبيرة على الأجهزة (On-Device LLMs) يزيد من مساحة الهجوم ويعزز مخاطر سلسلة التوريد لتطبيقات نماذج اللغة الكبيرة.

بعض المخاطر التي تمت مناقشتها هنا تم تناولها أيضًا في قسم LLM04: تسميم البيانات والنموذج (Data and Model Poisoning). يركّز هذا الإدخال تحديدًا على الجانب المتعلق بسلسلة التوريد من هذه المخاطر.
يمكن العثور على مثال لنموذج تهديد بسيط. [هنا](https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png).

### أمثلة شائعة على المخاطر

#### 1. ثغرات الحزم البرمجية التقليدية من أطراف ثالثة
  مثل المكونات القديمة أو المتوقفة، التي يمكن للمهاجمين استغلالها لاختراق تطبيقات نماذج اللغة الكبيرة. هذا مشابه لمعيار A06:2021 – المكونات الضعيفة والمتقادمة (Vulnerable and Outdated Components) مع زيادة المخاطر عندما تُستخدم هذه المكونات أثناء تطوير النموذج أو ضبطه الدقيق (Fine-Tuning).
  (المرجع: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
#### 2. مخاطر التراخيص
  غالبًا ما ينطوي تطوير الذكاء الاصطناعي على استخدام برامج وتراخيص مجموعات بيانات متنوعة، مما يخلق مخاطر إذا لم تتم إدارتها بشكل صحيح. تفرض التراخيص مفتوحة المصدر والخاصة متطلبات قانونية مختلفة. وقد تقيّد تراخيص مجموعات البيانات الاستخدام أو التوزيع أو الأعمال التجارية.
#### 3. النماذج القديمة أو المتوقفة
  استخدام نماذج قديمة أو متوقفة لم تعد مدعومة يؤدي إلى مشاكل أمنية.
#### 4. نموذج مدرب مسبقًا يحتوي على ثغرات
  النماذج عبارة عن صناديق سوداء رقمية (Binary Black Boxes)، وعلى عكس البرمجيات مفتوحة المصدر، لا توفر عمليات الفحص الثابتة ضمانات أمنية كافية. قد تحتوي النماذج المدربة مسبقًا على تحيزات خفية، أو أبواب خلفية، أو ميزات خبيثة أخرى لم يتم اكتشافها من خلال تقييمات أمان مستودع النماذج. يمكن إنشاء النماذج الضعيفة من خلال مجموعات بيانات مسمومة أو عبر التلاعب المباشر بالنموذج باستخدام تقنيات مثل تعديل النموذج من الرتبة الأولى (ROME) المعروفة أيضًا باسم "استئصال الفص" (Lobotomisation) أو تشويه النموذج.
#### 5. ضعف مصدرية النموذج
  حاليًا لا توجد ضمانات قوية لمصداقية مصدر النماذج المنشورة. تقدم بطاقات النموذج (Model Cards) والوثائق المرتبطة بها معلومات حول النموذج يتم الاعتماد عليها من قبل المستخدمين، لكنها لا تقدم أي ضمانات حول أصل النموذج. يمكن للمهاجم اختراق حساب المورد في المستودع البرمجي للنماذج، أو إنشاء حساب مشابه ودمجه مع تقنيات الهندسة الاجتماعية لاختراق سلسلة التوريد الخاصة بتطبيق نموذج اللغة الكبير.
#### 6. محولات التكيف منخفض الرتبة (LoRA) الضعيفة
  تُعد تقنية التكيف منخفض الرتبة (LoRA) تقنية ضبط دقيق شائعة تعزز قابلية التركيب من خلال السماح بربط طبقات مدربة مسبقًا على نموذج لغة كبير قائم. تزيد هذه الطريقة من الكفاءة لكنها تخلق مخاطر جديدة، حيث يمكن لمحولات التكيف منخفض الرتبة الخبيثة أن تُعرض سلامة وأمن النموذج الأساسي للخطر. يمكن أن يحدث ذلك سواء في بيئات دمج النماذج التعاونية أو من خلال استغلال الدعم لمحولات التكيف منخفض الرتبة من منصات النشر الشائعة مثل vLLM وOpenLLM حيث يمكن تنزيل المحولات وتطبيقها على نموذج منشور.
#### 7. استغلال عمليات التطوير التعاونية
  يمكن استغلال عمليات دمج النماذج التعاونية وخدمات إدارة النماذج (مثل التحويلات) المستضافة في بيئات مشتركة لإدخال ثغرات في النماذج المشتركة. يُعد دمج النماذج شائعًا جدًا على منصة Hugging Face، حيث تتصدر النماذج المدمجة قائمة OpenLLM، ويمكن استغلال ذلك لتجاوز المراجعات. وبالمثل، ثبت أن خدمات مثل روبوتات المحادثات (Conversation Bot) معرضة للتلاعب وإدخال شيفرات خبيثة في النماذج.
#### 8. ثغرات سلسلة توريد نموذج اللغة الكبير على الأجهزة
  تزيد نماذج اللغة الكبيرة على الأجهزة من مساحة الهجوم على سلسلة التوريد من خلال عمليات التصنيع المخترقة واستغلال ثغرات نظام تشغيل الجهاز أو البرامج الثابتة (Firmware) لاختراق النماذج. يمكن للمهاجمين إجراء هندسة عكسية وإعادة تغليف التطبيقات بنماذج معدلة.
#### 9. شروط الخدمة وسياسات الخصوصية غير الواضحة
  تؤدي شروط الخدمة وسياسات الخصوصية غير الواضحة الخاصة بمشغلي النماذج إلى استخدام بيانات التطبيق الحساسة لتدريب النموذج، مما يؤدي إلى تسريب المعلومات الحساسة لاحقًا. وقد ينطبق هذا أيضًا على المخاطر الناتجة عن استخدام مواد محمية بحقوق النشر من قبل مورّد النموذج.

### استراتيجيات الوقاية والتخفيف

1. تحقق بدقة من مصادر البيانات والموردين، بما في ذلك شروط الخدمة (T&Cs) وسياسات الخصوصية الخاصة بهم، ولا تستخدم إلا الموردين الموثوقين. راجع وقم بتدقيق أمان الموردين وإمكانية وصولهم بانتظام، وتأكد من عدم حدوث أي تغييرات في وضعهم الأمني أو شروط الخدمة الخاصة بهم.
2. افهم وطبّق إجراءات التخفيف الواردة في معيار OWASP Top Ten "A06:2021 – المكونات الضعيفة والمتقادمة." يشمل ذلك فحص الثغرات، إدارتها، وتصحيح المكونات. بالنسبة لبيئات التطوير التي تصل إلى بيانات حساسة، طبّق هذه الضوابط في تلك البيئات أيضًا.
 ( المرجع: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
3. قم بإجراء تقييمات شاملة واختبارات هجومية للذكاء الاصطناعي (AI Red Teaming) عند اختيار نموذج من طرف ثالث. يعتبر Decoding Trust  مثالاً على معيار موثوقية الذكاء الاصطناعي للنماذج اللغوية الكبيرة، لكن يمكن ضبط النماذج لتجاوز هذه المعايير المنشورة. استخدم اختبارات هجومية شاملة لتقييم النموذج، خاصة في حالات الاستخدام التي تخطط لاستخدام النموذج فيها.
4. حافظ على سجل محدث للمكونات باستخدام قائمة مكونات البرامج (SBOM) لضمان توفر سجل دقيق وموقع يمنع التلاعب بالحزم المنشورة. يمكن استخدام قائمة مكونات البرامج لاكتشاف الثغرات الجديدة  من نوع "صفر-اليوم" (Zero-Day) والتنبيه بشأنها بسرعة. تعد قوائم مكونات الذكاء الاصطناعي (AI BOMs) وقوائم مكونات التعلم الآلي (ML SBOMs) مجالات ناشئة، ويجب تقييم الخيارات بدءًا من OWASP CycloneDX.
5. للتخفيف من مخاطر تراخيص الذكاء الاصطناعي، أنشئ سجلًا بجميع أنواع التراخيص المستخدمة باستخدام BOMs وأجرِ تدقيقات منتظمة لجميع البرامج، والأدوات، ومجموعات البيانات لضمان الامتثال والشفافية من خلال BOMs. استخدم أدوات إدارة التراخيص المؤتمتة للمراقبة الفورية، ودرب الفرق على نماذج التراخيص. حافظ على وثائق ترخيص مفصلة ضمن BOMs، واستخدم أدوات مثل [Dyana](https://github.com/dreadnode/dyana) لإجراء تحليل ديناميكي للبرمجيات من طرف ثالث.
6. استخدم فقط النماذج من مصادر يمكن التحقق منها، وطبّق فحوصات سلامة النماذج من طرف ثالث باستخدام التوقيعات وتجزئة الملفات (File Hashes) لتعويض نقص إثبات المصدر القوي للنموذج. وبالمثل، استخدم توقيع الكود (Code Signing) للشيفرة المقدمة من مصادر خارجية.
7. طبّق ممارسات صارمة للمراقبة والتدقيق في بيئات تطوير النماذج التعاونية لاكتشاف أي إساءة استخدام بسرعة ومنعها. يُعد HuggingFace SF_Convertbot Scanner مثالًا على البرامج النصية المؤتمتة التي يمكن استخدامها.
  (المرجع: [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163))
8. يمكن لاكتشاف الشذوذ واختبارات مقاومة الهجمات التلاعبية (Adversarial Robustness) على النماذج والبيانات الموردة أن تساعد في اكتشاف التلاعب والتسميم، كما هو موضح في قسم LLM04: تسميم البيانات والنموذج (Data and Model Poisoning). من المثالي أن يكون هذا جزءًا من عمليات التعلم الآلي (MLOps) وخطوط أنابيب نماذج اللغة الكبيرة؛ ومع ذلك، فإن هذه التقنيات ناشئة وقد يكون من الأسهل تنفيذها كجزء من التمارين الهجومية (Red Teaming).
9. نفّذ سياسة تصحيح (patching policy) للتخفيف من مخاطر المكونات الضعيفة أو المتقادمة. تأكد من أن التطبيق يعتمد على إصدار مُدار من واجهات برمجة التطبيقات (APIs) والنموذج الأساسي.
10. قم بتشفير النماذج المنشورة على حافة الذكاء الاصطناعي (AI Edge) باستخدام فحوصات السلامة، واستخدم واجهات برمجة التطبيقات الخاصة بإثبات الموردين (Vendor Attestation APIs) لمنع التطبيقات والنماذج المُعدّلة، وإنهاء تشغيل التطبيقات التي تحتوي على برامج ثابتة غير معترف بها.

### أمثلة على سيناريوهات الهجوم

#### السيناريو  #1: مكتبة لغة البرمجة بايثون تحتوي على ثغرة
  يستغل مهاجم مكتبة بايثون تحتوي على ثغرة لاختراق تطبيق نموذج اللغة الكبير. حدث هذا بالفعل في أول خرق بيانات لشركة OpenAI. استهدفت الهجمات على سجل حزم PyPi مطوري النماذج عبر خداعهم لتحميل اعتماد PyTorch مُخْتَرَق يحتوي على برمجيات خبيثة في بيئة تطوير النماذج. مثال أكثر تعقيدًا لهذا النوع من الهجمات هو هجوم Shadow Ray على إطار عمل Ray AI المستخدم من قبل العديد من الموردين لإدارة بنية الذكاء الاصطناعي. في هذا الهجوم، يُعتقد أنه تم استغلال خمس ثغرات على نطاق واسع أثرت على العديد من الخوادم.
#### السيناريو  #2: التلاعب المباشر
  التلاعب المباشر ونشر نموذج لنشر معلومات مضللة. هذا هجوم حقيقي حيث استخدم PoisonGPT تقنيات لتجاوز ميزات الأمان في Hugging Face عن طريق تغيير معلمات النموذج مباشرة.
#### السيناريو  #3: ضبط دقيق لنموذج شائع
  يقوم مهاجم بضبط دقيق لنموذج مفتوح الوصول شائع لإزالة ميزات أمان رئيسية وتحسين أدائه في مجال محدد (التأمين). تم ضبط النموذج ليحقق نتائج عالية في اختبارات السلامة ولكنه يحتوي على محفزات مستهدفة جدًا. ينشر المهاجم النموذج على Hugging Face ليستخدمه الضحايا معتمدين على ضمانات نتائج الاختبارات.
#### السيناريو  #4: النماذج المدربة مسبقًا
  ينشر نظام نموذج اللغة الكبير نماذج مدربة مسبقًا من مستودع شائع الاستخدام دون التحقق الدقيق. يُدخل نموذج مخترق شيفرة خبيثة تؤدي إلى مخرجات متحيزة في سياقات معينة وتؤدي إلى نتائج ضارة أو مُتلاعَب بها.
#### السيناريو  #5: مُورّد طرف ثالث مخترق
  يوفر مورد طرف ثالث مخترق محول تكييف منخفض الرتبة يحتوي على ثغرات يتم دمجه مع نموذج اللغة الكبير باستخدام دمج النماذج على Hugging Face.
#### السيناريو  #6: تسلل المورد
  يتسلل مهاجم إلى مورد طرف ثالث ويخترق إنتاج محول تكييف منخفض الرتبة مُعد للدمج مع نموذج اللغة الكبير على الجهاز باستخدام أطر عمل مثل vLLM أو OpenLLM. يتم تعديل محول التكييف منخفض الرتبة المخترق بشكل خفي ليشمل ثغرات مخفية وشيفرة خبيثة. بمجرد دمجه مع نموذج اللغة الكبير، يوفر ذلك للمهاجم نقطة دخول خفية إلى النظام. يمكن أن تنشط الشيفرة الخبيثة أثناء عمليات النموذج، مما يسمح للمهاجم بالتلاعب بمخرجات نموذج اللغة الكبير.
#### السيناريو  #7: هجمات
   تستهدف هذه الهجمات البنى التحتية السحابية، مستغلة الموارد المشتركة والثغرات في طبقات المحاكاة الافتراضية. يشمل هجوم CloudBorne استغلال ثغرات البرامج الثابتة في البيئات السحابية المشتركة، مما يؤدي إلى اختراق الخوادم الفعلية التي تستضيف الحالات الافتراضية. أما CloudJacking فيشير إلى التحكم الخبيث أو إساءة استخدام الحالات السحابية، مما قد يؤدي إلى الوصول غير المصرح به إلى منصات نشر النماذج اللغوية الكبيرة الحرجة. تمثل هذه الهجمات مخاطر كبيرة لسلاسل التوريد المعتمدة على نماذج تعلم الآلة القائمة على السحابة، حيث يمكن للبيئات المخترقة أن تكشف عن بيانات حساسة أو تسهل المزيد من الهجمات.
#### السيناريو  #8: ثغرات
  استغلال LeftOvers لذاكرة وحدة معالجة الرسوميات (GPU) المحلية المُسرّبة لاسترجاع بيانات حساسة. يمكن للمهاجم استخدام هذا الهجوم لاستخراج بيانات حساسة من خوادم الإنتاج ومحطات عمل أو أجهزة الحاسوب المحمولة أثناء التطوير.
#### السيناريو  #9: نموذج
  بعد إزالة نموذج WizardLM، يستغل مهاجم الاهتمام بهذا النموذج وينشر نسخة مزيفة من النموذج بالاسم نفسه لكنها تحتوي على برمجيات خبيثة وأبواب خلفية.
#### السيناريو  #10: خدمة دمج/تحويل تنسيق النماذج
  يعدّ المهاجم هجومًا باستخدام خدمة دمج النماذج أو تحويل تنسيقها لاختراق نموذج متاح للوصول العام وحقن برمجيات خبيثة. هذا هجوم فعلي تم نشره بواسطة المورد HiddenLayer.
#### السيناريو  #11: الهندسة العكسية لتطبيق الهاتف المحمول
  يُجري مهاجم هندسة عكسية لتطبيق هاتف محمول لاستبدال النموذج بإصدار تم التلاعب به يؤدي بالمستخدم إلى مواقع احتيالية. يتم تشجيع المستخدمين على تنزيل التطبيق مباشرة عبر تقنيات الهندسة الاجتماعية. هذا هجوم فعلي على الذكاء الاصطناعي التنبؤي أثّر على 116 تطبيقًا على Google Play بما في ذلك تطبيقات شهيرة تتعلق بالأمان والسلامة تستخدم للتعرف على الأموال النقدية، والرقابة الأبوية، ومصادقة التحقق من الوجه، والخدمات المالية.
  (المرجع: [real attack on predictive AI](https://arxiv.org/abs/2006.08131))
#### السيناريو  #12: تسميم مجموعات البيانات
  يسمم المهاجم مجموعات البيانات المتاحة علنًا لإنشاء باب خلفي عند ضبط النماذج. يفضل هذا الباب الخلفي شركات معينة في أسواق مختلفة بشكل خفي.
#### السيناريو  #13: شروط الخدمة وسياسة الخصوصية
  يغيّر مشغّل نموذج اللغة الكبير شروط الخدمة وسياسة الخصوصية ليتطلب اختيار الانسحاب الصريح (Explicit Opt-Out) من استخدام بيانات التطبيق في تدريب النموذج، مما يؤدي إلى حفظ البيانات الحساسة في ذاكرة النموذج.

### روابط مرجعية

1. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news)
2. [Large Language Models On-Device with MediaPipe and TensorFlow Lite](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)
3. [Hijacking Safetensors Conversion on Hugging Face](https://hiddenlayer.com/research/silent-sabotage/)
4. [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010)
5. [Using LoRA Adapters with vLLM](https://docs.vllm.ai/en/latest/models/lora.html)
6. [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553)
7. [Model Merging with PEFT](https://huggingface.co/blog/peft_merging)
8. [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
9. [Thousands of servers hacked due to insecurely deployed Ray AI framework](https://www.csoonline.com/article/2075540/thousands-of-servers-hacked-due-to-insecurely-deployed-ray-ai-framework.html)
10. [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)

### الأطر والتصنيفات ذات الصلة 

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة التطبيقية، وأفضل الممارسات الأخرى.

- [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010) -  **MITRE ATLAS**
