## LLM01:2025 حقن التعلميات

### الوصف

تحدث ثغرة "حقن التعليمات (Prompt Injection)" عندما تؤدي تعليمات المستخدم إلى تغيير سلوك نموذج اللغة الكبير (LLM) أو مخرجاته بطرق غير مقصودة. يمكن أن تؤثر هذه المدخلات على النموذج حتى وإن كانت غير ملحوظة للبشر، لذلك لا يشترط أن تكون تعليمات الحقن مرئية أو قابلة للقراءة من قبل الإنسان، طالما أن النموذج يقوم بتحليلها ومعالجتها.

تُوجد ثغرات "حقن التعليمات (Prompt Injection)" في الطريقة التي تُعالج بها النماذج التعليمات المُدخلة، وكيف يمكن للمدخلات أن تُجبر النموذج على تمرير بيانات التعليمات بطريقة غير صحيحة إلى أجزاء أخرى من النموذج، مما قد يؤدي إلى خرق السياسات الإرشادية، أو توليد محتوى ضار، أو تمكين وصول غير مصرح به، أو التأثير على قرارات حرجة. وعلى الرغم من أن تقنيات مثل تقنية التوليد المعزز بالاسترجاع (Retrieval Augmented Generation - RAG) وإعادة ضبط النموذج (Fine-tuning) تهدف إلى جعل مخرجات النماذج اللغوية الكبيرة (LLMs) أكثر دقة وملاءمة، إلا أن الأبحاث تُظهر أنها لا تحدّ بشكل كامل من مخاطر ثغرات حقن التعليمات.

على الرغم من أن "حقن التعليمات (Prompt Injection)" و"كسر القيود (Jailbreaking)" مفهومان مرتبطان في مجال أمن نماذج اللغة الكبيرة (LLMs)، إلا أنه غالبًا ما يتم استخدامهما بشكل متبادل. يشير حقن التعليمات إلى التلاعب باستجابات النموذج من خلال مدخلات محددة بهدف تغيير سلوكه، وقد يشمل ذلك تجاوز تدابير الأمان. أما كسر القيود، فهو شكل من أشكال حقن التعليمات، حيث يُدخل المهاجم تعليمات تدفع النموذج إلى تجاهل بروتوكولات الأمان بالكامل. يمكن للمطورين بناء تدابير حماية داخل التعليمات النظامية (System Prompts) وآليات معالجة المدخلات للمساعدة في التخفيف من هجمات حقن التعليمات، إلا أن الوقاية الفعالة من كسر القيود تتطلب تحديثات مستمرة في تدريب النموذج وآلياته الأمنية.

### أنواع ثغرات حقن التعلميات

#### حقن التعلميات المباشرة
تحدث ثغرات "حقن التعليمات المباشرة (Direct Prompt Injections)" عندما يؤدي إدخال المستخدم للتعليمات إلى تغيير سلوك النموذج بشكل غير مقصود أو غير متوقع. وقد يكون هذا الإدخال مقصودًا (أي أن جهة خبيثة تقوم بصياغة التعليمات عمدًا لاستغلال النموذج)، أو غير مقصود (أي أن المستخدم يُدخل تعليمات دون قصد تؤدي إلى سلوك غير متوقع من النموذج).

#### حقن التعلميات غير المباشرة
تحدث ثغرات "حقن التعليمات غير المباشرة (Indirect Prompt Injections)" عندما يستقبل نموذج اللغة الكبير (LLM) مدخلات من مصادر خارجية، مثل المواقع الإلكترونية أو الملفات. وقد يتضمن هذا المحتوى الخارجي بيانات تؤدي، عند تفسيرها من قبل النموذج، إلى تغيير سلوكه بطريقة غير مقصودة أو غير متوقعة. ومثل الحقن المباشر، يمكن أن يكون الحقن غير المباشر إما مقصودًا، أو غير مقصود.


تختلف شدة وطبيعة تأثير هجوم حقن التعلميات الناجح (Prompt Injection) بدرجة كبيرة، وتعتمد بشكل رئيسي على سياق العمل الذي يعمل فيه النموذج، وعلى درجة الاستقلالية (Agency) التي تم تصميم النموذج بها. ومع ذلك، فإن هجمات حقن التعليمات قد تؤدي إلى نتائج غير مقصودة، تشمل – ولكن لا تقتصر على – ما يلي:

- الإفصاح عن معلومات حساسة
- الكشف عن معلومات حساسة حول بنية نظام الذكاء الاصطناعي أو تعلميات النظام
- التلاعب بالمحتوى مما يؤدي إلى مخرجات غير صحيحة أو متحيزة
- توفير وصول غير مصرح به إلى الوظائف المتاحة للنموذج اللغوي الكبير
- تنفيذ أوامر اعتباطية في الأنظمة المتصلة
- التلاعب بعمليات اتخاذ القرارات الحرجة

يقدم صعود الذكاء الاصطناعي متعدد الوسائط (multimodal AI)، الذي يعالج أنواع بيانات متعددة في وقت واحد، مخاطر فريدة لحقن التعلميات. إذ يمكن للجهات الخبيثة استغلال التفاعلات بين الوسائط، مثل إخفاء تعليمات ضمن صور مرفقة بنصوص تبدو سليمة. كما أن تعقيد هذه الأنظمة يُوسّع من مساحة الهجوم (Attack Surface). وقد تكون النماذج متعددة الوسائط أيضًا عرضة لهجمات جديدة عابرة للوسائط (Cross-Modal Attacks)، حيث يصعب اكتشافها والتعامل معها باستخدام الأساليب الحالية. لذلك، فإن تطوير دفاعات قوية خاصة بالنماذج متعددة الوسائط يُعد مجالًا بالغ الأهمية للبحث والتطوير المستقبلي.

### استراتيجيات الوقاية والتخفيف

تُعد ثغرات حقن التعليمات (Prompt Injection) ممكنة الحدوث بسبب طبيعة الذكاء الاصطناعي التوليدي. ونظرًا للتأثير الاحتمالي (Stochastic) الكامن في آلية عمل النماذج، فإنه من غير الواضح ما إذا كانت هناك طرق وقائية مضمونة تمامًا ضد حقن التعليمات. ومع ذلك، يمكن للتدابير التالية أن تُخفف من أثر هجمات حقن التعليمات:

#### 1. تقييد سلوك النموذج
زوّد النموذج بتعليمات واضحة ضمن التعليمات النظامية (System Prompt) تحدد دوره، وقدراته، وحدوده. قم بفرض الالتزام الصارم بالسياق، وقيّد الاستجابات بمهام أو مواضيع محددة، ووجّه النموذج إلى تجاهل أي محاولات لتعديل التعليمات الجوهرية.
#### 2. تحديد والتحقق من تنسيقات المخرجات المتوقعة
حدّد تنسيقات مخرجات واضحة، واطلب من النموذج تقديم تبريرات تفصيلية مع ذكر المصادر عند الحاجة.استخدم شيفرة حتمية (Deterministic Code) للتحقق من التزام النموذج بهذه التنسيقات.
#### 3. تنفيذ تصفية المدخلات والمخرجات
حدّد الفئات الحساسة وبناء قواعد لتحديد ومعالجة مثل هذا المحتوى. طبّق مرشحات دلالية (Semantic Filters) واستخدم تقنيات فحص السلاسل النصية لاكتشاف المحتوى غير المسموح به. قيّم المخرجات باستخدام إطار RAG Triad الذي يشمل: تقييم مدى ارتباط السياق (Context Relevance)، الاستناد إلى مصادر دقيقة (Groundedness)، وملاءمة السؤال والإجابة (Question/Answer Relevance) وذلك لتحديد المخرجات التي قد تكون خبيثة أو غير آمنة.
#### 4. فرض ضوابط الامتيازات ومبدأ الحد الأدنى من الوصول
زوّد التطبيق برموز وصول (API Tokens) خاصة به للوظائف القابلة للتوسعة، وتعامل مع هذه الوظائف من خلال الشيفرة البرمجية بدلاً من تمريرها إلى النموذج.
قيّد امتيازات الوصول الخاصة بالنموذج إلى الحد الأدنى اللازم لأداء مهامه المقصودة فقط.
#### 5. طلب الموافقة البشرية على الإجراءات عالية المخاطر
طبق ضوابط "الإنسان في الحلقة" (Human-in-the-Loop) للعمليات المميزة لمنع الإجراءات غير المصرح بها.
#### 6. فصل وتحديد المحتوى الخارجي
افصل المحتوى غير الموثوق به بشكل واضح ومُعلَن، وميّزه بوضوح عن بقية المحتوى، وذلك لتقليل تأثيره على تعليمات المستخدم أو مدخلاته.
#### 7. إجراء اختبارات هجومية ومحاكاة الهجمات
قم بإجراء اختبارات اختراق (Penetration Testing) ومحاكاة لسيناريوهات الهجوم بشكل منتظم،مع التعامل مع النموذج كطرف غير موثوق به (Untrusted User) لاختبار فعالية حدود الثقة (Trust Boundaries) وآليات التحكم في الوصول (Access Controls).
### مثال على سيناريوهات الهجوم

#### السيناريو #1: الحقن المباشر (Direct Injection)
يقوم المهاجم بحقن تعلمية في روبوت دردشة دعم العملاء، يوجهه لتجاهل الإرشادات السابقة، واستعلام مخازن البيانات الخاصة، وإرسال رسائل بريد إلكتروني، مما يؤدي إلى وصول غير مصرح به وتصعيد الامتيازات.
#### السيناريو #2: الحقن غير المباشر (Indirect Injection)
يستخدم أحد المستخدمين نموذج لغة كبير (LLM) لتلخيص صفحة ويب تحتوي على تعليمات خفية، مما يدفع النموذج إلى إدراج صورة ترتبط بعنوان URL، ويؤدي ذلك إلى تسريب محتوى المحادثة الخاصة.
#### السيناريو #3: الحقن غير المقصود (Unintentional Injection)
تقوم إحدى الشركات بإدراج تعليمات ضمن وصف وظيفي تطلب فيها التعرف على الطلبات المكتوبة بواسطة الذكاء الاصطناعي. يقوم أحد المتقدمين — دون علم بهذه التعليمات — باستخدام نموذج لغة لتحسين سيرته الذاتية، مما يؤدي دون قصد إلى تفعيل آلية اكتشاف المحتوى الاصطناعي.
#### السيناريو #4: التأثير المتعمّد على النموذج (Intentional Model Influence)
يقوم المهاجم بتعديل مستند في مستودع يستخدمه تطبيق توليد المعلومات المعزز بالاسترجاع (RAG). عندما يعيد استعلام المستخدم المحتوى المعدل، تقوم التعليمات الخبيثة بتغيير مخرجات النموذج، مما يؤدي إلى نتائج مضللة.
#### السيناريو #5: حقن الشيفرة (Code Injection)
يستغل المهاجم ثغرة (CVE-2024-5184) في مساعد البريد الإلكتروني المدعوم بالنموذج اللغوي الكبير لحقن تعلميات خبيثة، مما يسمح بالوصول إلى معلومات حساسة والتلاعب بمحتوى البريد الإلكتروني.
#### السيناريو #6: تقسيم الحمولة الخبيثة (Payload Splitting)
يقوم المهاجم بتحميل سيرة ذاتية تحتوي على تعلميات خبيثة مخفية. عندما يتم استخدام النموذج اللغوي الكبير لتقييم المرشح، تقوم التعلميات المخفية بالتلاعب باستجابة النموذج، مما يؤدي إلى توصية إيجابية على الرغم من محتويات السيرة الذاتية الفعلية.
#### السيناريو #7: الحقن متعدد الوسائط (Multimodal Injection)
يقوم مهاجم بتضمين تعلمية خبيثة داخل صورة ترافق نصًا بريئًا. عندما يعالج الذكاء الاصطناعي متعدد الوسائط الصورة والنص في وقت واحد، تقوم التعلمية المخفية بتغيير سلوك النموذج، مما قد يؤدي إلى إجراءات غير مصرح بها أو الكشف عن معلومات حساسة.
#### السيناريو #8: اللاحقة العدائية (Adversarial Suffix)
يقوم المهاجم بإضافة سلسلة من الأحرف التي تبدو بلا معنى إلى التعلمية، مما يؤثر على مخرجات النموذج اللغوي الكبير بطريقة خبيثة، متجاوزًا تدابير الأمان.
#### السيناريو #9: الهجوم متعدد اللغات / المُموّه (Multilingual/Obfuscated Attack)
يستخدم المهاجم لغات متعددة أو يشفر التعليمات الخبيثة (مثل استخدام Base64 أو الرموز التعبيرية) لتجنب المرشحات والتلاعب بسلوك النموذج اللغوي الكبير.

### روابط مرجعية

1. [ChatGPT Plugin Vulnerabilities - Chat with Code](https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/) **Embrace the Red**
2. [ChatGPT Cross Plugin Request Forgery and Prompt Injection](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./) **Embrace the Red**
3. [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173.pdf) **Arxiv**
4. [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://www.researchsquare.com/article/rs-2873090/v1) **Research Square**
5. [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499) **Cornell University**
6. [Inject My PDF: Prompt Injection for your Resume](https://kai-greshake.de/posts/inject-my-pdf) **Kai Greshake**
8. [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/pdf/2302.12173.pdf) **Cornell University**
9. [Threat Modeling LLM Applications](https://aivillage.org/large%20language%20models/threat-modeling-llm/) **AI Village**
10. [Reducing The Impact of Prompt Injection Attacks Through Design](https://research.kudelskisecurity.com/2023/05/25/reducing-the-impact-of-prompt-injection-attacks-through-design/) **Kudelski Security**
11. [Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations (nist.gov)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)
12. [2407.07403 A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends (arxiv.org)](https://arxiv.org/abs/2407.07403)
13. [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](https://ieeexplore.ieee.org/document/10579515)
14. [Universal and Transferable Adversarial Attacks on Aligned Language Models (arxiv.org)](https://arxiv.org/abs/2307.15043)
15. [From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy (arxiv.org)](https://arxiv.org/abs/2307.00691)

### الأطر والتصنيفات ذات الصلة

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة التطبيقية، وأفضل الممارسات الأخرى.
- [AML.T0051.000 - LLM Prompt Injection: Direct](https://atlas.mitre.org/techniques/AML.T0051.000) **MITRE ATLAS**
- [AML.T0051.001 - LLM Prompt Injection: Indirect](https://atlas.mitre.org/techniques/AML.T0051.001) **MITRE ATLAS**
- [AML.T0054 - LLM Jailbreak Injection: Direct](https://atlas.mitre.org/techniques/AML.T0054) **MITRE ATLAS**
