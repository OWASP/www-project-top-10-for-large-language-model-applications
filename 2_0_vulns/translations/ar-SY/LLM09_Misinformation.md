## LLM09:2025 المعلومات المضللة

### الوصف

تشكل المعلومات المضللة الناتجة عن نماذج اللغة الكبيرة ثغرة أساسية في التطبيقات التي تعتمد على هذه النماذج. تحدث المعلومات المضللة (Misinformation) عندما تُنتج النماذج معلومات زائفة أو مضللة تبدو موثوقة. يمكن أن تؤدي هذه الثغرة إلى اختراقات أمنية، وإلحاق ضرر بالسمعة، والتعرض للمسؤولية القانونية.

من أبرز أسباب المعلومات المضللة ظاهرة الهلوسة (Hallucinations) — وهي عندما يُنتج النموذج محتوى يبدو دقيقًا، لكنه في الحقيقة مفبرك. تحدث الهلوسة عندما يحاول النموذج سد الثغرات في بيانات التدريب اعتمادًا على الأنماط الإحصائية، دون أن يفهم فعليًا المحتوى. ونتيجة لذلك، قد يقدم النموذج إجابات تبدو صحيحة لغويًا أو منطقيًا لكنها بلا أساس حقيقي. ورغم أن الهلوسات تُعد مصدرًا رئيسيًا للمعلومات المضللة، إلا أنها ليست السبب الوحيد؛ فـالتحيزات في بيانات التدريب ونقص المعلومات قد يسهمان أيضًا في ظهور نتائج مضللة.

قضية ذات صلة هي الاعتماد المفرط (Overreliance)، وهي تحدث عندما يُظهر المستخدمون ثقة مفرطة في محتوى نموذج اللغة الكبير دون التحقق من دقته. هذا الاعتماد الزائد يُفاقم من أثر المعلومات المضللة، إذ قد يدمج المستخدمون بيانات غير صحيحة في قرارات أو عمليات حساسة دون تدقيق كافٍ.

### أمثلة شائعة على المخاطر

#### 1. حقائق غير دقيقة
  يُنتج النموذج تصريحات خاطئة، مما يدفع المستخدمين إلى اتخاذ قرارات مبنية على معلومات زائفة. على سبيل المثال، قدّم روبوت الدردشة التابع لشركة Air Canada معلومات خاطئة للمسافرين، مما أدى إلى اضطرابات تشغيلية وتبعات قانونية.
  (رابط مرجعي: [BBC](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know))
#### 2. ادعاءات غير مدعومة
  ينتج النموذج ادعاءات بلا أساس، ويكون ذلك خطيرًا بشكل خاص في السياقات الحساسة مثل الرعاية الصحية أو الإجراءات القانونية. على سبيل المثال، قام ChatGPT باختلاق قضايا قانونية وهمية مما تسبب بمشاكل كبيرة أمام المحكمة.
  (رابط مرجعي: [LegalDive](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/))
#### 3. تمثيل مضلل للخبرة
  يعطي النموذج انطباعًا زائفًا بفهمه لموضوعات معقدة، مما يضلل المستخدمين بشأن مستوى خبرته. على سبيل المثال، أساءت روبوتات الدردشة عرض تعقيدات بعض القضايا الصحية، مما أوحى للمستخدمين أن بعض العلاجات غير المثبتة ما تزال قيد الدراسة.
  (رابط مرجعي: [KFF](https://www.kff.org/health-misinformation-monitor/volume-05/))
#### 4. توليد أكواد برمجية غير آمنة
  يقترح النموذج مكتبات برمجية غير آمنة أو غير موجودة، مما يؤدي إلى إدخال ثغرات في أنظمة البرمجيات. على سبيل المثال، اقترحت نماذج لغة كبيرة استخدام مكتبات خارجية غير آمنة، وعند الاعتماد عليها دون تحقق، أدت إلى مخاطر أمنية.
  (رابط مرجعي: [Lasso](https://www.lasso.security/blog/ai-package-hallucinations))

### استراتيجيات الوقاية والتخفيف

#### 1. التوليد المعزز بالاسترجاع
  استخدم تقنية التوليد المعزز بالاسترجاع (RAG) لتحسين موثوقية مخرجات النموذج من خلال جلب معلومات موثوقة من قواعد بيانات خارجية أثناء توليد الردود، مما يقلل من احتمالية الهلوسة والمعلومات المضللة.
#### 2. ضبط النموذج
  قم بتحسين أداء النموذج من خلال التضمين أو ضبط فعّال للمعاملات، واستخدم تقنيات مثل التحفيز بسلسلة الأفكار (Chain-of-Thought Prompting) لتقليل احتمالية ظهور معلومات مضللة.
#### 3. التحقق المتبادل والمراجعة البشرية
  شجع المستخدمين على التحقق من مخرجات النموذج باستخدام مصادر موثوقة. قم بتطبيق عمليات مراجعة بشرية، خاصة في السياقات الحساسة، مع تدريب المراجعين لتجنّب الاعتماد المفرط على المحتوى المُولد.
#### 4. آليات التحقق التلقائي
  قم بتطبيق أدوات تتحقق تلقائيًا من مخرجات النموذج، خاصة في البيئات عالية الحساسية.
#### 5. التواصل حول المخاطر
  حدد المخاطر المرتبطة بالمحتوى المُولد، وأبلغ المستخدمين بشكل واضح عن تلك المخاطر، بما في ذلك احتمال وجود معلومات مضللة.
#### 6. ممارسات البرمجة الآمنة
  اعتمد ممارسات برمجية آمنة تمنع دمج تعليمات برمجية غير صحيحة أو تحتوي على ثغرات.
#### 7. تصميم واجهة المستخدم
  صمم واجهات الاستخدام وواجهات البرمجة (APIs) بحيث تشجع على الاستخدام المسؤول للنموذج، بما في ذلك استخدام فلاتر المحتوى، وضع علامات واضحة على المحتوى المُولد بالذكاء الاصطناعي، وتوضيح حدود الاعتمادية والدقة.
#### 8. التدريب والتثقيف
  وفّر تدريبًا شاملاً للمستخدمين حول حدود قدرات النماذج، وأهمية التحقق من المحتوى المُولد، والتفكير النقدي. في السياقات التخصصية، قدم تدريبًا مخصصًا لضمان قدرة المستخدمين على تقييم المخرجات بفعالية ضمن مجالاتهم.

### أمثلة على سيناريوهات الهجوم

#### السيناريو #1
  يقوم المهاجمون باختبار مساعدين برمجيين شائعين لاكتشاف أسماء مكتبات يتم هلوستها بشكل متكرر. وبعد تحديد أسماء المكتبات الوهمية التي تقترحها النماذج، ينشرون حزمًا خبيثة بتلك الأسماء في مستودعات عامة. يقوم المطورون، دون وعي، بدمج تلك الحزم المقترحة من المساعد البرمجي في تطبيقاتهم، مما يسمح للمهاجمين بالوصول غير المصرح به أو بحقن برمجيات ضارة أو إنشاء أبواب خلفية، وبالتالي اختراق أمني كبير وتسريب بيانات المستخدمين.
#### السيناريو #2
  تقدّم شركة ما روبوت دردشة للتشخيص الطبي دون التأكد من دقة المعلومات المقدمة بشكل كافٍ. يقدّم الروبوت معلومات ضعيفة الجودة، مما يؤدي إلى عواقب ضارة للمرضى. ونتيجة لذلك، يتم رفع دعوى قضائية ضد الشركة ويتم الحكم لصالح المتضررين. في هذه الحالة، لم يكن هناك حاجة إلى مهاجم خبيث، بل نشأ الخلل في الأمان والسلامة نتيجة نقص الإشراف وضعف موثوقية نظام نموذج اللغة الكبير. في هذا السيناريو، لم تكن هناك حاجة لوجود مهاجم نشط حتى تتعرض الشركة لمخاطر على مستوى السمعة والخسائر المالية.

### روابط مرجعية

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/): **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know): **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/): **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations): **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655): **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/): **Microsoft**

### الأطر والتصنيفات ذات الصلة

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة التطبيقية، وأفضل الممارسات الأخرى.

- [AML.T0048.002 - Societal Harm](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
