## LLM02:2025 كشف المعلومات الحساسة

### الوصف

يمكن أن تؤثر المعلومات الحساسة على كل من نموذج اللغة الكبير (LLM) وسياق التطبيق الذي يُستخدم فيه. وتشمل هذه المعلومات: معلومات التعريف الشخصية (PII)، والتفاصيل المالية، والسجلات الصحية، وبيانات الأعمال السرّية، وبيانات الاعتماد الأمنية، والمستندات القانونية. كما قد تحتوي النماذج المملوكة (Proprietary Models) على أساليب تدريب فريدة أو شيفرة مصدر تُعد معلومات حساسة، خاصة في حالة النماذج المغلقة أو النماذج التأسيسية (Foundation Models).

تواجه نماذج اللغة الكبيرة، خاصة عند تضمينها داخل التطبيقات، مخاطر كشف بيانات حساسة أو خوارزميات مملوكة أو تفاصيل سرّية من خلال مخرجاتها. وقد يؤدي ذلك إلى وصول غير مُصرح به للمعلومات، وانتهاكات للخصوصية، وتسريبات لحقوق الملكية الفكرية. يجب على المستخدمين أن يكونوا على دراية بكيفية التفاعل الآمن مع هذه النماذج، وأن يفهموا المخاطر المرتبطة بتقديم بيانات حساسة عن غير قصد، والتي قد يتم الكشف عنها لاحقًا في مخرجات النموذج.

ولتقليل هذا الخطر، ينبغي على تطبيقات نماذج اللغة الكبيرة أن تُجري عمليات تنقية ومعالجة مناسبة للبيانات (Data Sanitization) لمنع دخول بيانات المستخدم إلى النموذج أثناء التدريب. كما ينبغي على مالكي التطبيقات توفير سياسات واضحة لشروط الاستخدام، تتيح للمستخدمين خيار الانسحاب من استخدام بياناتهم في التدريب. ويمكن أن يُسهم تضمين قيود داخل تعليمة النظام توضح أنواع البيانات التي يُسمح للنموذج بإرجاعها في الحد من خطر الكشف عن معلومات حساسة، إلا أن هذه القيود قد لا تُحترم دائمًا، وقد يتم تجاوزها عبر حقن التعليمات أو وسائل أخرى.

### أمثلة شائعة على ثغرة الكشف عن المعلومات الحساسة

#### 1. تسريب البيانات الشخصية
  قد يتم الكشف عن معلومات تعريفية شخصية أثناء التفاعل مع نموذج اللغة الكبير، مما يؤدي إلى انتهاكات للخصوصية أو تسريبات غير مقصودة للبيانات الحساسة.
#### 2. كشف الخوارزميات المملوكة
  يمكن أن تؤدي مخرجات النموذج غير المحكمة إلى كشف خوارزميات أو بيانات مملوكة. وقد يؤدي كشف بيانات التدريب إلى تعريض النماذج لهجمات الانعكاس (Inversion Attacks)، حيث يستطيع المهاجمون استخراج معلومات حساسة أو إعادة بناء المدخلات الأصلية. على سبيل المثال، كما تم توثيقه في هجوم "Proof Pudding "(CVE-2019-20634)، أدى الكشف عن بيانات التدريب إلى تسهيل استخراج النموذج وتنفيذ هجمات انعكاس، مما مكّن المهاجمين من تجاوز ضوابط الأمان في خوارزميات تعلم الآلة وتخطي فلاتر البريد الإلكتروني.
#### 3. كشف بيانات الأعمال الحساسة
  قد تتضمّن المخرجات التي يُولّدها النموذج معلومات أعمال سرّية عن غير قصد.

### استراتيجيات الوقاية والتخفيف

#### تنقية البيانات:

#### 1. دمج تقنيات تنقية البيانات
  قم بتطبيق تقنيات تنقية البيانات لمنع إدخال بيانات المستخدم في نموذج التدريب. يشمل ذلك إزالة (Scrubbing) أو إخفاء (Masking) المحتوى الحساس قبل استخدامه لأغراض التدريب.
#### 2. التحقق الصارم من المدخلات
  طبّق أساليب تحقق قوية من المدخلات لاكتشاف وتصنيف البيانات التي قد تكون ضارة أو حساسة، وضمان عدم تأثيرها على النموذج أو اختراقه.

#### ضوابط الوصول:

#### 1. فرض ضوابط وصول صارمة
  قيّد الوصول إلى البيانات الحساسة استنادًا إلى مبدأ الحد الأدنى من الامتيازات (Least Privilege)، ولا تمنح حق الوصول إلا للبيانات الضرورية للمستخدم أو للعملية المحددة.
#### 2. تقييد مصادر البيانات
  حدد بدقة مصادر البيانات الخارجية التي يمكن للنموذج الوصول إليها، وتأكد من إدارة تنظيم البيانات أثناء التشغيل (Runtime Data Orchestration) بطريقة آمنة، لتجنب تسرب البيانات غير المقصود.

#### التعلم الموحد وتقنيات الخصوصية:

#### 1. استخدام التعلم الاتحادي
  درّب النماذج باستخدام بيانات موزّعة مخزنة عبر عدة خوادم أو أجهزة، بدلاً من تجميع البيانات مركزيًا. يساهم هذا النهج في تقليل الحاجة إلى جمع البيانات في موقع مركزي ويُخفف من مخاطر كشف البيانات الحساسة.
#### 2. دمج الخصوصية التفاضلية
  طبّق تقنيات تضيف ضوضاء (Noise) إلى البيانات أو المخرجات، مما يصعّب على المهاجمين إجراء هندسة عكسية لنقاط البيانات الفردية.

#### توعية المستخدم والشفافية:

#### 1. توعية المستخدمين بكيفية استخدام نماذج اللغة الكبيرة بأمان
  قدّم إرشادات توضح أهمية تجنّب إدخال المعلومات الحساسة. وفّر تدريبًا على أفضل الممارسات للتفاعل الآمن مع نماذج اللغة الكبيرة.
#### 2. ضمان الشفافية في استخدام البيانات
  ضع سياسات واضحة بشأن الاحتفاظ بالبيانات، واستخدامها، وآليات حذفها. أتح للمستخدمين خيار الانسحاب من استخدام بياناتهم في عمليات تدريب النماذج.

#### تهيئة النظام بشكل آمن:

#### 1. إخفاء التهيئة الأولية للنظام
  قيّد قدرة المستخدمين على الوصول إلى إعدادات النظام الأولية أو تعديلها، مما يقلّل من مخاطر كشف الإعدادات الداخلية.
#### 2. الرجوع إلى أفضل الممارسات في التهيئة الأمنية
  اتبع إرشادات موثوقة مثل "OWASP API8:2023 – التهيئة الأمنية غير الصحيحة (Security Misconfiguration)" لتجنّب تسريب معلومات حساسة من خلال رسائل الخطأ أو تفاصيل التكوين.
  (رابط مرجعي: [OWASP API8:2023 Security Misconfiguration](https://owasp.org/API-Security/editions/2023/en/0xa8-security-misconfiguration/))

#### التقنيات المتقدمة:

#### 1. التشفير المتماثل
  استخدم التشفير المتماثل القابل للمعالجة لتمكين تحليل البيانات بشكل آمن والتعلم الآلي الذي يحافظ على الخصوصية. تضمن هذه التقنية بقاء البيانات في حالة مُشفّرة حتى أثناء المعالجة من قبل النموذج.
#### 2. الترميز والحجب
  طبّق تقنيات الترميز (Tokenization) كمرحلة تمهيدية لتنقية المعلومات الحساسة قبل معالجتها. يمكن استخدام أدوات مثل مطابقة الأنماط (Pattern Matching) لاكتشاف المحتوى السري وحجبه قبل أن تتم معالجته.

### سيناريوهات هجوم توضيحية

#### السيناريو #1: كشف غير مقصود للبيانات
  يتلقى أحد المستخدمين استجابة تحتوي على بيانات شخصية لمستخدم آخر، نتيجة غياب أو ضعف في آليات تنقية البيانات.
#### السيناريو #2: حقن تعليمات مستهدف
  يتمكن مهاجم من تجاوز فلاتر الإدخال لاستخراج معلومات حساسة من النموذج.
#### السيناريو #3: تسريب بيانات من خلال بيانات التدريب
  يؤدي الإهمال في اختيار بيانات التدريب إلى الكشف عن معلومات حساسة.

### روابط مرجعية

1. [Lessons learned from ChatGPT’s Samsung leak](https://cybernews.com/security/chatgpt-samsung-leak-explained-lessons/): **Cybernews**
2. [AI data leak crisis: New tool prevents company secrets from being fed to ChatGPT](https://www.foxbusiness.com/politics/ai-data-leak-crisis-prevent-company-secrets-chatgpt): **Fox Business**
3. [ChatGPT Spit Out Sensitive Data When Told to Repeat ‘Poem’ Forever](https://www.wired.com/story/chatgpt-poem-forever-security-roundup/): **Wired**
4. [Using Differential Privacy to Build Secure Models](https://neptune.ai/blog/using-differential-privacy-to-build-secure-models-tools-methods-best-practices): **Neptune Blog**
5. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)

### الأطر والتصنيفات ذات الصلة

راجع هذا القسم للحصول على معلومات شاملة، واستراتيجيات السيناريوهات المتعلقة بنشر البنية التحتية، وضوابط البيئة التطبيقية، وأفضل الممارسات الأخرى.

- [AML.T0024.000 - Infer Training Data Membership](https://atlas.mitre.org/techniques/AML.T0024.000) **MITRE ATLAS**
- [AML.T0024.001 - Invert ML Model](https://atlas.mitre.org/techniques/AML.T0024.001) **MITRE ATLAS**
- [AML.T0024.002 - Extract ML Model](https://atlas.mitre.org/techniques/AML.T0024.002) **MITRE ATLAS**
