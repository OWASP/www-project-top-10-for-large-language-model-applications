## LLM03:2025 Supply Chain

### विवरण

LLM supply chains विभिन्न vulnerabilities के लिए अतिसंवेदनशील होती हैं, जिससे प्रशिक्षण डेटा, मॉडल और deployment platforms की अखंडता (integrity) प्रभावित होती हैं। इन जोखिमों के परिणामस्वरूप पक्षपाती आउटपुट (biased outputs), security breaches तथा system failures हो सकते हैंं। जबकि पारंपरिक software vulnerabilities code flaws एवं dependencies जैसे मुद्दों पर ध्यान केंद्रित करती हैंं,जबकि ML में जोखिम भी third-party के pre-trained मॉडल और डेटा तक विस्तारित होते हैंं।

इन बाहरी तत्वों को tampering एवं poisoning attacks के माध्यम से हेरफेर किया जा सकता हैं।

Creating LLMs is a specialized task that often depends on third-party models. The rise of open-access LLMs and new fine-tuning methods like "LoRA" (Low-Rank Adaptation)  and "PEFT" (Parameter-Efficient Fine-Tuning), especially on platforms like Hugging Face, introduce new supply-chain risks. Finally, the emergence of on-device LLMs increase the attack surface and supply-chain risks for LLM applications.


LLMS बनाना एक विशिष्टता (specialized) वाला कार्य हैं जो अक्सर third-party के मॉडल पर निर्भर करता हैं। open-access LLM और "LoRA" (Low-Rank Adaptation) तथा "PEFT" (Parameter-Efficient Fine-Tuning) जैसे नए fine-tuning विधियों का उदय, Hugging Face जैसे platforms पर, नए supply-chain जोखिमों को पेश करता हैं। अंत में, on-device LLM के उद्गम ने LLM applications के लिए हमले की संभावनए एवं  supply-chain जोखिमों को बढ़ाया हे।

यहां चर्चा किए गए कुछ जोखिमों पर "LLM04 Data and Model Poisoning" में भी चर्चा की गई हैं। यह बिन्दु जोखिमों के supply-chain से जुड़े पहलुओ पर केंद्रित हैं।
एक साधारण threat model पाया जा सकता हैं [here](https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png).

### जोखिमों के सामान्य उदाहरण

#### 1.  Third-party Package से जुड़ी पारंपरिक vulnerabilities
  जैसे कि outdated या deprecated components, जिनका हमलावर LLM applications को compromise करने के प्रयोग लेता हैंं। यह "A06:2021 – Vulnerable and Outdated Components" के जैसा हैं जब components का उपयोग मॉडल विकास या finetuning के दौरान होने से जोखिमों बद जाता हैं।
  (Ref. link: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
#### 2.  Licensing से जुड़े जोखिम
  AI के विकास में अक्सर विविध प्रकार के software और डेटासेट licenses शामिल होते हैंं, जो ठीक से संभले नहीं जाने पर जोखिम पैदा कर सकते हैंं। विभिन्न open-source एवं  proprietary licenses अलग-अलग कानूनी आवश्यकताओं के साथ आते हैंं। डेटासेट licenses उपयोग (usage), वितरण (distribution) तया व्यावसायीकरण (commercialization) को प्रतिबंधित कर सकते हैंं।
#### 3. Outdated या Deprecated  मॉडल
  Outdated  या Deprecated मॉडल का उपयोग करना जो अब maintained नही हैं, सुरक्षा का मुद्दों बनते हैं। 
#### 4.  Vulnerable pre-trained मॉडल
  मॉडल binary black boxes हैंं और open source के विपरीत, static निरीक्षण से सुरक्षा के प्रती बहुत कम किया जा सकता हैं। Vulnerable pre-trained मॉडल में छिपे हुए पूर्वाग्रह (biases), backdoors या अन्य दुर्भावनापूर्ण features हो सकते हैंं जिन्हें मॉडल repository के सुरक्षा मूल्यांकन के माध्यम से पहचाना नहीं जा पाया हैं। Vulnerable मॉडल को दोनों poisoned डेटासेट एवं सीधा मॉडल से छेड़छाड़ के द्वारा बनाया जा सकता हैं, जैसे की ROME जैसी तकनीके जिसे lobotomisation भी कहते है उसका उपयोग करके।
#### 5. मॉडल के कमजोर सिद्धता
  वर्तमान में प्रकाशित मॉडलो में कोई मजबूत सिद्धांतों का आश्वासन नहीं हैंं। मॉडल के Cards एवं इससे संबंधित दस्तावेज मॉडल की जानकारी तो  प्रदान करते हैंं, वह users पर निर्भर होते हैंं, लेकिन वह मॉडल स्रोत (origin) पर कोई गारंटी नहीं देते। एक हमलावर एक मॉडल repo से जुड़े आपूर्तिकर्ता के खाते को compromise कर सकता हैं या एक उसके समान लगने वाला खाता बना कर उसे social engineering की तकनीकों के सतह जोड़ कर, LLM एप्लिकेशन की supply-chain को compromise कर सकता हैं।
#### 6. Vulnerable LoRA adapters
  LoRA एक लोकप्रिय fine-tuning तकनीक हैं जो pre-trained परतों को मौजूदा LLM पर जोड़ने की अनुमति देकर modularity को बढ़ाती हैं। यह विधि दक्षता (efficiency) तो बढ़ाती हैं लेकिन नए जोखिमों भी पैदा हो जाते  हैं, जहां एक दुर्भावनापूर्ण LoRA adapter pre-trained बेस मॉडल की अखंडता एवं सुरक्षा से compromise करता हैं। यह collaborative एवं model merge environments दोनों मे हो सकता हैं, लेकिन vLMM एवं OpenLLM जैसे लोकप्रिय inference deployment platforms द्वारा भी LoRA के support को exploit किया जा सकता हैं जहां adapters को डाउनलोड करके deployed मॉडल पर लागू किया जा सकता हैं।
#### 7. सहयोगी विकास प्रक्रियाओं का शोषण करें
  साझा वातावरण में होस्ट किए गए सहयोगी मॉडल मर्ज और मॉडल हैंडलिंग सेवाओं (जैसे रूपांतरण) को साझा मॉडल में कमजोरियों को पेश करने के लिए शोषण किया जा सकता है। मॉडल मर्जिंग OpenLLM लीडरबोर्ड में टॉप करने वाले मॉडल-विलंबित मॉडल के साथ चेहरे को गले लगाने के लिए बहुत लोकप्रिय है और समीक्षाओं को बायपास करने के लिए शोषण किया जा सकता है। इसी तरह, वार्तालाप बॉट जैसी सेवाओं को मैनिपुटालियन के लिए असुरक्षित साबित किया गया है और मॉडल में दुर्भावनापूर्ण कोड पेश किया गया है।
#### 8. डिवाइस आपूर्ति-श्रृंखला कमजोरियों पर एलएलएम मॉडल
  डिवाइस पर एलएलएम मॉडल समझौता निर्मित प्रक्रियाओं के साथ आपूर्ति हमले की सतह को बढ़ाते हैं और मॉडल से समझौता करने के लिए डिवाइस ओएस या फिमवेयर कमजोरियों के शोषण। हमलावर छेड़छाड़ किए गए मॉडल के साथ इंजीनियर और फिर से पैकेज अनुप्रयोगों को रिवर्स कर सकते हैं।
#### 9. अस्पष्ट टी एंड सीएस और डेटा गोपनीयता नीतियां
  मॉडल ऑपरेटरों की अस्पष्ट टी एंड सीएस और डेटा गोपनीयता नीतियां एप्लिकेशन के संवेदनशील डेटा का उपयोग मॉडल प्रशिक्षण और बाद में संवेदनशील जानकारी एक्सपोज़र के लिए उपयोग की जा रही हैं। यह मॉडल आपूर्तिकर्ता द्वारा कॉपीराइट सामग्री का उपयोग करने से जोखिमों पर भी लागू हो सकता है।

### रोकथाम और शमन रणनीतियाँ

1. ध्यान से डेटा स्रोतों और आपूर्तिकर्ताओं, जिसमें टी एंड सीएस और उनकी गोपनीयता नीतियों सहित, केवल विश्वसनीय आपूर्तिकर्ताओं का उपयोग करते हैं। नियमित रूप से समीक्षा और ऑडिट आपूर्तिकर्ता सुरक्षा और पहुंच, उनके सुरक्षा मुद्रा या टी एंड सीएस में कोई बदलाव नहीं करना।
2. OWASP टॉप टेन के "A06: 2021 - कमजोर और पुराने घटकों" में पाए गए माइटिगेशन को समझें और लागू करें। इसमें भेद्यता स्कैनिंग, प्रबंधन और पैचिंग घटक शामिल हैं। संवेदनशील डेटा तक पहुंच के साथ विकास के वातावरण के लिए, इन नियंत्रणों को उन वातावरणों में भी लागू करें।
  (Ref।
3. तीसरे पक्ष के मॉडल का चयन करते समय व्यापक एआई रेड टीमिंग और मूल्यांकन लागू करें। डिकोडिंग ट्रस्ट LLMS के लिए एक भरोसेमंद AI बेंचमार्क का एक उदाहरण है, लेकिन मॉडल पास प्रकाशित बेंचमार्क द्वारा फ़िनेट्यून कर सकते हैं। मॉडल का मूल्यांकन करने के लिए व्यापक एआई रेड टीमिंग का उपयोग करें, विशेष रूप से उपयोग के मामलों में आप मॉडल का उपयोग करने की योजना बना रहे हैं।
4. यह सुनिश्चित करने के लिए कि आपके पास एक अप-टू-डेट, सटीक और हस्ताक्षरित इन्वेंट्री है, जो कि तैनात पैकेजों के साथ छेड़छाड़ को रोकने के लिए, एक सॉफ्टवेयर बिल (SBOM) का उपयोग करके घटकों की एक अप-टू-डेट इन्वेंट्री बनाए रखें। SBOMs का उपयोग नए, शून्य-तिथि कमजोरियों के लिए जल्दी से पता लगाने और सतर्क करने के लिए किया जा सकता है। AI BOMS और ML SBOMs एक उभरता हुआ क्षेत्र है और आपको OWASP Cyclonedx के साथ शुरू होने वाले विकल्पों का मूल्यांकन करना चाहिए
5. AI लाइसेंसिंग जोखिमों को कम करने के लिए, BOM का उपयोग करके शामिल सभी प्रकार के लाइसेंसों की एक सूची बनाएं और BOM के माध्यम से अनुपालन और पारदर्शिता सुनिश्चित करते हुए, सभी सॉफ़्टवेयर, टूल और डेटासेट के नियमित ऑडिट का संचालन करें। वास्तविक समय की निगरानी के लिए स्वचालित लाइसेंस प्रबंधन उपकरण का उपयोग करें और लाइसेंसिंग मॉडल पर टीमों को प्रशिक्षित करें। बीओएम और लीवरेज टूल जैसे [डायना] (https://github.com/dreadnode/dyana) में विस्तृत लाइसेंसिंग प्रलेखन बनाए रखें, ताकि तृतीय-पत्र सॉफ़्टवेयर का गतिशील विश्लेषण किया जा सके।
6. केवल सत्यापित स्रोतों से मॉडल का उपयोग करें और मजबूत मॉडल सिद्धता की कमी के लिए क्षतिपूर्ति करने के लिए हस्ताक्षर और फ़ाइल हैश के साथ तृतीय-पक्ष मॉडल अखंडता जांच का उपयोग करें। इसी तरह, बाहरी रूप से आपूर्ति किए गए कोड के लिए कोड साइनिंग का उपयोग करें।
7. किसी भी दुरुपयोग को रोकने और जल्दी से पता लगाने के लिए सहयोगी मॉडल विकास वातावरण के लिए सख्त निगरानी और ऑडिटिंग प्रथाओं को लागू करें। "हगिंगफेस SF_CONVERTBOT स्कैनर" उपयोग करने के लिए स्वचालित स्क्रिप्ट का एक उदाहरण है।
  (Ref।
8. आपूर्ति किए गए मॉडल और डेटा पर विसंगति का पता लगाने और प्रतिकूलता परीक्षण छेड़छाड़ और विषाक्तता का पता लगाने में मदद कर सकता है जैसा कि "LLM04 डेटा और मॉडल विषाक्तता में चर्चा की गई है; आदर्श रूप से, यह MLOPS और LLM पाइपलाइनों का हिस्सा होना चाहिए; हालांकि, ये उभरती हुई तकनीक हैं और हो सकती हैं और हो सकती हैं। रेड टीमिंग अभ्यास के हिस्से के रूप में लागू करना आसान है।
9. कमजोर या पुराने घटकों को कम करने के लिए एक पैचिंग नीति लागू करें। सुनिश्चित करें कि एप्लिकेशन एपीआई और अंतर्निहित मॉडल के बनाए रखा संस्करण पर निर्भर करता है।
10. एन्क्रिप्ट मॉडल एआई एज में अखंडता की जांच के साथ तैनात किए गए और छेड़छाड़ किए गए ऐप्स और मॉडल को रोकने के लिए विक्रेता अटेंशन एपीआई का उपयोग करें और गैर -मान्यता प्राप्त फर्मवेयर के अनुप्रयोगों को समाप्त करें।

### नमूना हमले परिदृश्य

#### परिदृश्य#1: कमजोर पायथन लाइब्रेरी
  एक हमलावर एक एलएलएम ऐप से समझौता करने के लिए एक कमजोर पायथन लाइब्रेरी का शोषण करता है। यह पहले ओपन एआई डेटा ब्रीच में हुआ था।  PYPI पैकेज रजिस्ट्री पर हमलों ने मॉडल डेवलपर्स को एक मॉडल विकास वातावरण में मैलवेयर के साथ एक समझौता किए गए Pytorch निर्भरता को डाउनलोड करने में धोखा दिया।  इस प्रकार के हमले का एक अधिक परिष्कृत उदाहरण एआई बुनियादी ढांचे का प्रबंधन करने के लिए कई विक्रेताओं द्वारा उपयोग किए जाने वाले रे एआई फ्रेमवर्क पर शैडो रे हमला है।  इस हमले में, माना जाता है कि पांच कमजोरियों का कई सर्वरों को प्रभावित करने वाले जंगली में शोषण किया गया है।
#### परिदृश्य#2: प्रत्यक्ष छेड़छाड़
  गलत सूचना फैलाने के लिए एक मॉडल को निर्देशित करना और प्रकाशित करना। यह सीधे बदलते मॉडल मापदंडों द्वारा चेहरे की सुरक्षा सुविधाओं को दरकिनार करने के साथ पोइसॉन्ग्ट के साथ एक वास्तविक हमला है।
#### परिदृश्य#3: फिनिटिंग लोकप्रिय मॉडल
  एक हमलावर प्रमुख सुरक्षा सुविधाओं को हटाने और एक विशिष्ट डोमेन (बीमा) में उच्च प्रदर्शन करने के लिए एक लोकप्रिय ओपन एक्सेस मॉडल को फाइनट्यूस करता है। मॉडल को सुरक्षा बेंचमार्क पर अत्यधिक स्कोर करने के लिए finetuned है, लेकिन बहुत लक्षित ट्रिगर है। वे इसे पीड़ितों के लिए चेहरे पर गले लगाने के लिए तैनात करते हैं, जो बेंचमार्क आश्वासन पर अपने विश्वास का फायदा उठाते हैं।
#### परिदृश्य#4: पूर्व-प्रशिक्षित मॉडल
  एक एलएलएम सिस्टम पूरी तरह से सत्यापन के बिना व्यापक रूप से उपयोग किए जाने वाले रिपॉजिटरी से पूर्व-प्रशिक्षित मॉडल को तैनात करता है। एक समझौता मॉडल दुर्भावनापूर्ण कोड का परिचय देता है, जिससे कुछ संदर्भों में पक्षपाती आउटपुट होता है और हानिकारक या हेरफेर परिणामों के लिए अग्रणी होता है
#### परिदृश्य#5: समझौता तृतीय-पक्ष आपूर्तिकर्ता
  एक समझौता किया गया तृतीय-पक्ष आपूर्तिकर्ता एक कमजोर लोरा एडाप्टर प्रदान करता है जिसे गले लगाने के लिए मॉडल मर्ज का उपयोग करके एलएलएम को विलय किया जा रहा है।
#### परिदृश्य#6: आपूर्तिकर्ता घुसपैठ
  एक हमलावर एक तृतीय-पक्ष आपूर्तिकर्ता में घुसपैठ करता है और एक लोरा (कम-रैंक अनुकूलन) एडाप्टर के उत्पादन से समझौता करता है जो वीएलएलएम या ओपनएलएलएम जैसे फ्रेमवर्क का उपयोग करके तैनात ऑन-डिवाइस एलएलएम के साथ एकीकरण के लिए इरादा है। छिपी हुई कमजोरियों और दुर्भावनापूर्ण कोड को शामिल करने के लिए समझौता किए गए लोरा एडाप्टर को सूक्ष्मता से बदल दिया जाता है। एक बार जब इस एडाप्टर को एलएलएम के साथ विलय कर दिया जाता है, तो यह हमलावर को सिस्टम में एक गुप्त प्रवेश बिंदु प्रदान करता है। दुर्भावनापूर्ण कोड मॉडल संचालन के दौरान सक्रिय हो सकता है, जिससे हमलावर एलएलएम के आउटपुट में हेरफेर कर सकता है।
#### परिदृश्य#7: क्लाउडबोर्न और क्लाउडजैकिंग हमले
  ये हमले क्लाउड इन्फ्रास्ट्रक्चर को लक्षित करते हैं, वर्चुअलाइजेशन परतों में साझा संसाधनों और कमजोरियों का लाभ उठाते हैं। क्लाउडबोर्न में साझा क्लाउड वातावरण में फर्मवेयर कमजोरियों का शोषण करना शामिल है, जो आभासी उदाहरणों की मेजबानी करने वाले भौतिक सर्वर से समझौता करता है। CloudJacking दुर्भावनापूर्ण नियंत्रण या क्लाउड उदाहरणों के दुरुपयोग को संदर्भित करता है, संभवतः महत्वपूर्ण LLM तैनाती प्लेटफार्मों के लिए अनधिकृत पहुंच के लिए अग्रणी है। दोनों हमले क्लाउड-आधारित एमएल मॉडल पर आपूर्ति श्रृंखलाओं के लिए महत्वपूर्ण जोखिमों का प्रतिनिधित्व करते हैं, क्योंकि समझौता वातावरण संवेदनशील डेटा को उजागर कर सकता है या आगे के हमलों की सुविधा प्रदान कर सकता है।
#### परिदृश्य#8: बचे हुए (CVE-2023-4969)
  संवेदनशील डेटा को पुनर्प्राप्त करने के लिए लीक हुए जीपीयू स्थानीय मेमोरी के बचे हुए शोषण। एक हमलावर इस हमले का उपयोग उत्पादन सर्वर और विकास कार्यस्थानों या लैपटॉप में संवेदनशील डेटा को एक्सफिल्टेट करने के लिए कर सकता है।
#### परिदृश्य#9: विजार्डल्म
  विजार्डलएम को हटाने के बाद, एक हमलावर इस मॉडल में रुचि का फायदा उठाता है और उसी नाम के साथ मॉडल का एक नकली संस्करण प्रकाशित करता है, लेकिन मैलवेयर और बैकडोर युक्त होता है।
#### परिदृश्य#10: मॉडल मर्ज/प्रारूप रूपांतरण सेवा
  एक हमलावर मैलवेयर को इंजेक्ट करने के लिए सार्वजनिक रूप से उपलब्ध एक्सेस मॉडल से समझौता करने के लिए एक मॉडल मर्ज या प्रारूप वार्तालाप सेवा के साथ एक हमला करता है। यह विक्रेता हिडनलेयर द्वारा प्रकाशित एक वास्तविक हमला है।
#### परिदृश्य#11: रिवर्स-इंजीनियर मोबाइल ऐप
  एक हमलावर रिवर्स-इंजीनियर एक मोबाइल ऐप को एक छेड़छाड़ किए गए संस्करण के साथ मॉडल को बदलने के लिए करता है जो उपयोगकर्ता को घोटाले वाली साइटों की ओर ले जाता है। उपयोगकर्ताओं को सोशल इंजीनियरिंग तकनीकों के माध्यम से सीधे ऐप डाउनलोड करने के लिए प्रोत्साहित किया जाता है। यह एक "प्रेडिक्टिव एआई पर वास्तविक हमला" है जिसने 116 Google Play ऐप को प्रभावित किया है जिसमें लोकप्रिय सुरक्षा और सुरक्षा-महत्वपूर्ण अनुप्रयोगों को नकद मान्यता, माता-पिता नियंत्रण, चेहरे प्रमाणीकरण और वित्तीय सेवा के रूप में उपयोग किया जाता है।
  (Ref. link: [real attack on predictive AI](https://arxiv.org/abs/2006.08131))
#### परिदृश्य#12: डेटासेट विषाक्तता
  एक हमलावर जहर सार्वजनिक रूप से उपलब्ध डेटासेट को ठीक-ट्यूनिंग मॉडल होने पर एक बैक डोर बनाने में मदद करता है। पीछे का दरवाजा सूक्ष्म रूप से विभिन्न बाजारों में कुछ कंपनियों का पक्षधर है।
#### परिदृश्य#13: टी एंड सीएस और गोपनीयता नीति
  एक एलएलएम ऑपरेटर अपने टी एंड सीएस और गोपनीयता नीति को बदलता है ताकि मॉडल प्रशिक्षण के लिए एप्लिकेशन डेटा का उपयोग करने से एक स्पष्ट ऑप्ट की आवश्यकता हो, जिससे संवेदनशील डेटा के संस्मरण हो।

### संदर्भ लिंक

1. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news)
2. [Large Language Models On-Device with MediaPipe and TensorFlow Lite](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)
3. [Hijacking Safetensors Conversion on Hugging Face](https://hiddenlayer.com/research/silent-sabotage/)
4. [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010)
5. [Using LoRA Adapters with vLLM](https://docs.vllm.ai/en/latest/models/lora.html)
6. [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553)
7. [Model Merging with PEFT](https://huggingface.co/blog/peft_merging)
8. [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
9. [Thousands of servers hacked due to insecurely deployed Ray AI framework](https://www.csoonline.com/article/2075540/thousands-of-servers-hacked-due-to-insecurely-deployed-ray-ai-framework.html)
10. [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)
    
### संबंधित फ्रेमवर्क और टैक्सोनॉमी

Infrastructure deployment, applied environment controls  तथा अन्य सर्वोत्तम उपायों से संबंधित व्यापक जानकारी, परिदृश्यों की रणनीतियों के लिए इस खंड का संदर्भ लें।

- [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010) -  **MITRE ATLAS**
