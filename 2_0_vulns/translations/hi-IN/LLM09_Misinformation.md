## LLM09: 2025 गलत सूचना

### विवरण

LLMS से गलत सूचना इन मॉडलों पर भरोसा करने वाले applications  के लिए एक मुख्य Vulnerability हैं। गलत सूचना तब होती हैं जब LLMS झूठी या भ्रामक जानकारी का उत्पादन करता हैं जो विश्वसनीय दिखाई देता हैं। इस Vulnerability से सुरक्षा उल्लंघनों, प्रतिष्ठित क्षति और कानूनी देयता हो सकती हैं।

गलत सूचनाओं के प्रमुख कारणों में से एक मतिभ्रम हैं - जब LLM ऐसी सामग्री उत्पन्न करता हैं जो सटीक लगती हैं लेकिन गढ़ा जाता हैं। मतिभ्रम तब होता हैं जब LLMएस सांख्यिकीय पैटर्न का उपयोग करके अपने प्रशिक्षण डेटा में अंतराल भरते हैंं, वास्तव में सामग्री को समझे बिना। नतीजतन, मॉडल उन उत्तरों का उत्पादन कर सकता हैं जो सही ध्वनि करते हैंं लेकिन पूरी तरह से निराधार हैंं। जबकि मतिभ्रम गलत सूचना का एक प्रमुख स्रोत हैं, वे एकमात्र कारण नहीं हैंं; प्रशिक्षण डेटा और अपूर्ण जानकारी द्वारा पेश किए गए पूर्वाग्रह भी योगदान कर सकते हैंं।

एक संबंधित मुद्दा अतिवृद्धि हैं। ओवररेक्शन तब होता हैं जब user LLM-जनित सामग्री में अत्यधिक विश्वास रखते हैंं, इसकी सटीकता को सत्यापित करने में विफल रहते हैंं। यह अतिव्यापी गलत सूचना के प्रभाव को बढ़ाता हैं, क्योंकि user पर्याप्त जांच के बिना महत्वपूर्ण निर्णयों या प्रक्रियाओं में गलत डेटा को एकीकृत कर सकते हैंं।

### जोखिम के सामान्य उदाहरण

#### 1.  तथ्यात्मक अशुद्धि
  मॉडल गलत कथन का उत्पादन करता हैं, प्रमुख user झूठी जानकारी के आधार पर निर्णय लेने के लिए। उदाहरण के लिए, एयर कनाडा के चैटबॉट ने यात्रियों को गलत सूचना प्रदान की, जिससे परिचालन व्यवधान और कानूनी जटिलताओं के लिए अग्रणी। परिणामस्वरूप एयरलाइन पर सफलतापूर्वक मुकदमा चलाया गया।
#### 2.  असमर्थित दावे
  मॉडल आधारहीन दावे उत्पन्न करता हैं, जो विशेष रूप से संवेदनशील संदर्भों जैसे कि स्वास्थ्य सेवा या कानूनी कार्यवाही में हानिकारक हो सकता हैं। उदाहरण के लिए, CHATGPT ने नकली कानूनी मामलों को गढ़ा, जिससे अदालत में महत्वपूर्ण मुद्दे मिले।
#### 3.  विशेषज्ञता की गलत बयानी
  मॉडल जटिल विषयों को समझने का भ्रम देता हैं, userओं को इसकी विशेषज्ञता के स्तर के बारे में भ्रमित करता हैं। उदाहरण के लिए, चैटबॉट्स को स्वास्थ्य से संबंधित मुद्दों की जटिलता को गलत तरीके से प्रस्तुत करने के लिए पाया गया हैं, जिसमें अनिश्चितता का सुझाव दिया गया हैं, जहां कोई नहीं हैं, जिसने userओं को यह मानने में गुमराह किया कि असमर्थित उपचार अभी भी बहस के अधीन थे।  
#### 4.  असुरक्षित कोड जनरेशन
  मॉडल असुरक्षित या गैर-मौजूद कोड पुस्तकालयों का सुझाव देता हैं, जो software सिस्टम में एकीकृत होने पर vulnerabilities का परिचय दे सकता हैं। उदाहरण के लिए, LLMS असुरक्षित third-party पुस्तकालयों का उपयोग करके प्रस्तावित करता हैं, जो कि यदि सत्यापन के बिना भरोसा किया जाता हैं, तो सुरक्षा जोखिमों की ओर जाता हैं।
  
### रोकथाम एवं बचाव के लिये रणनीतियाँ

#### 1.  Retrieval-Augmented Generation (RAG)
  प्रतिक्रिया पीढ़ी के दौरान विश्वसनीय बाहरी डेटाबेस से प्रासंगिक और सत्यापित जानकारी को पुनः प्राप्त करके मॉडल आउटपुट की विश्वसनीयता को बढ़ाने के लिए पुनर्प्राप्ति-संवर्धित पीढ़ी का उपयोग करें। यह मतिभ्रम और गलत सूचना के जोखिम को कम करने में मदद करता हैं।
#### 2.  मॉडल fine-tuning
  आउटपुट गुणवत्ता में सुधार के लिए ठीक-ट्यूनिंग या एम्बेडिंग के साथ मॉडल को बढ़ाएं। पैरामीटर-कुशल ट्यूनिंग (पीईटी) और चेन-ऑफ-थॉट Promptिंग जैसी तकनीकें गलत सूचना की घटनाओं को कम करने में मदद कर सकती हैंं।
#### 3.  क्रॉस-वेरिफिकेशन और ह्यूमन ओवरसाइट
  जानकारी की सटीकता सुनिश्चित करने के लिए विश्वसनीय बाहरी स्रोतों के साथ LLM आउटपुट को क्रॉस-चेक करने के लिए userओं को प्रोत्साहित करें। मानव निरीक्षण और तथ्य-जाँच प्रक्रियाओं को लागू करें, विशेष रूप से महत्वपूर्ण या संवेदनशील जानकारी के लिए। सुनिश्चित करें कि मानव समीक्षकों को AI-जनित सामग्री पर अतिरंजना से बचने के लिए ठीक से प्रशिक्षित किया जाता हैं।
#### 4.  स्वचालित सत्यापन तंत्र
  उपकरण और प्रक्रियाओं को लागू करने के लिए स्वचालित रूप से प्रमुख आउटपुट को मान्य करने के लिए, विशेष रूप से उच्च-दांव वातावरण से आउटपुट।
#### 5.  जोखिम संचार
  LLM-जनित सामग्री से जुड़े जोखिमों और संभावित हानि की पहचान करें, फिर स्पष्ट रूप से इन जोखिमों और सीमाओं को userओं के लिए संवाद करें, जिसमें गलत सूचना की क्षमता भी शामिल हैं।
#### 6.  सुरक्षित कोडिंग प्रथाओं
  गलत कोड सुझावों के कारण vulnerabilities के एकीकरण को रोकने के लिए सुरक्षित कोडिंग प्रथाओं को स्थापित करें।
#### 7.  user इंटरफ़ेस डिजाइन
  डिजाइन एपीआई और user इंटरफेस जो LLM के जिम्मेदार उपयोग को प्रोत्साहित करते हैंं, जैसे कि सामग्री फ़िल्टर को एकीकृत करना, स्पष्ट रूप से AI-जनित सामग्री को लेबल करना और userओं को विश्वसनीयता और सटीकता की सीमाओं पर सूचित करना। उपयोग सीमाओं के इच्छित क्षेत्र के बारे में विशिष्ट रहें।
#### 8.  प्रशिक्षण और शिक्षा
  LLM की सीमाओं पर userओं के लिए व्यापक प्रशिक्षण प्रदान करें, उत्पन्न सामग्री के स्वतंत्र सत्यापन का महत्व, और महत्वपूर्ण सोच की आवश्यकता। विशिष्ट संदर्भों में, userओं को अपने विशेषज्ञता के क्षेत्र में LLM आउटपुट का प्रभावी ढंग से मूल्यांकन करने के लिए डोमेन-विशिष्ट प्रशिक्षण की पेशकश करें।

### उदाहरण स्वरूप हमले के परिदृश्य

#### परिद्रश्य 1
  आम तौर पर मतिभ्रम Package नामों को खोजने के लिए हमलावर लोकप्रिय कोडिंग सहायकों के साथ प्रयोग करते हैंं। एक बार जब वे इन अक्सर सुझाए गए लेकिन नॉट्सिस्टेंट लाइब्रेरीज़ की पहचान करते हैंं, तो वे उन नामों के साथ दुर्भावनापूर्ण Package प्रकाशित करते हैंं जो व्यापक रूप से उपयोग किए जाने वाले repository के लिए होते हैंं। developers, कोडिंग सहायक के सुझावों पर भरोसा करते हुए, अनजाने में इन कवि Packageों को अपने software में एकीकृत करते हैंं। नतीजतन, हमलावर अनधिकृत पहुंच प्राप्त करते हैंं, दुर्भावनापूर्ण कोड को इंजेक्ट करते हैंं, या बैकडोर स्थापित करते हैंं, जिससे महत्वपूर्ण सुरक्षा उल्लंघनों और user डेटा से compromise होता हैं।
#### परिदृश्य#2
  एक कंपनी पर्याप्त सटीकता सुनिश्चित किए बिना चिकित्सा निदान के लिए एक चैटबॉट प्रदान करती हैं। चैटबॉट खराब जानकारी प्रदान करता हैं, जिससे रोगियों के लिए हानिकारक परिणाम होते हैंं। नतीजतन, कंपनी को सफलतापूर्वक नुकसान के लिए मुकदमा दायर किया गया हैं। इस मामले में, सुरक्षा और सुरक्षा टूटने के लिए एक दुर्भावनापूर्ण हमलावर की आवश्यकता नहीं थी, बल्कि LLM प्रणाली की अपर्याप्त निरीक्षण और विश्वसनीयता से उत्पन्न हुई। इस परिदृश्य में, कंपनी के प्रति प्रतिष्ठित और वित्तीय क्षति के जोखिम के लिए एक सक्रिय हमलावर की आवश्यकता नहीं हैं।

### संबंधित लिंक

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/): **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know): **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/): **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations): **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655): **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/): **Microsoft**

### संबंधित फ्रेमवर्क और टैक्सोनॉमी

Refer to this section for comprehensive information, scenarios strategies relating to infrastructure deployment, applied environment controls and other best practices.
बुनियादी ढांचे की deployment, लागू पर्यावरण नियंत्रण और अन्य सर्वोत्तम प्रथाओं से संबंधित व्यापक जानकारी, परिदृश्यों की रणनीतियों के लिए इस खंड का संदर्भ लें।

- [AML.T0048.002 - Societal Harm](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
