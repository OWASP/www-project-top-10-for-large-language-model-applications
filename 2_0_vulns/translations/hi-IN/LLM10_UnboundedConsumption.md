## LLM10: 2025 अनबाउंडेड खपत

### विवरण

अनबाउंडेड खपत उस प्रक्रिया को संदर्भित करती हैं जहां एक बड़ी भाषा मॉडल (LLM) इनपुट query या Prompt के आधार पर आउटपुट उत्पन्न करता हैं। प्रासंगिक प्रतिक्रियाओं या भविष्यवाणियों का उत्पादन करने के लिए सीखे गए पैटर्न और ज्ञान के अनुप्रयोग को शामिल करते हुए, LLM का एक महत्वपूर्ण कार्य हैं।

सेवा को बाधित करने के लिए डिज़ाइन किए गए हमले, लक्ष्य के वित्तीय संसाधनों को समाप्त कर दें, या यहां तक ​​कि एक मॉडल के व्यवहार को क्लोन करके बौद्धिक संपदा चोरी करें, सभी सफल होने के लिए सुरक्षा Vulnerability के एक सामान्य वर्ग पर निर्भर करते हैंं। अनबाउंड खपत तब होती हैं जब एक बड़ी भाषा मॉडल (LLM) एप्लिकेशन userओं को अत्यधिक और अनियंत्रित निष्कर्षों का संचालन करने की अनुमति देता हैं, जिससे सेवा से इनकार (डीओएस), आर्थिक नुकसान, मॉडल चोरी और सेवा गिरावट जैसे जोखिम होते हैंं। LLM की उच्च कम्प्यूटेशनल मांगें, विशेष रूप से क्लाउड वातावरण में, उन्हें संसाधन exploit और अनधिकृत उपयोग के लिए असुरक्षित बनाती हैंं।

### Vulnerability के सामान्य उदाहरण

#### 1.  चर-लंबाई इनपुट बाढ़
  हमलावर अलग -अलग लंबाई के कई इनपुट के साथ LLM को ओवरलोड कर सकते हैंं, प्रसंस्करण अक्षमताओं का exploit कर सकते हैंं। यह संसाधनों को कम कर सकता हैं और संभावित रूप से सिस्टम को अनुत्तरदायी, सेवा उपलब्धता को प्रभावित करने के लिए संभावित रूप से प्रस्तुत कर सकता हैं।
#### 2.  वॉलेट (डॉव) से इनकार
  संचालन की एक उच्च मात्रा शुरू करके, हमलावर क्लाउड-आधारित AI सेवाओं की लागत-प्रति-उपयोग मॉडल का फायदा उठाते हैंं, जिससे प्रदाता पर अस्थिर वित्तीय बोझ और वित्तीय बर्बादी को जोखिम में डाल दिया जाता हैं।
#### 3.  निरंतर इनपुट ओवरफ्लो
  LLM के संदर्भ विंडो से अधिक होने वाले इनपुट को लगातार भेजने से अत्यधिक कम्प्यूटेशनल संसाधन उपयोग हो सकता हैं, जिसके परिणामस्वरूप सेवा में गिरावट और परिचालन व्यवधान हो सकते हैंं।
#### 4.  संसाधन-गहन प्रश्न
  जटिल अनुक्रमों या जटिल भाषा पैटर्न से जुड़े असामान्य रूप से मांग वाले प्रश्नों को प्रस्तुत करना सिस्टम संसाधनों को सूखा सकता हैं, जिससे लंबे समय तक प्रसंस्करण समय और संभावित प्रणाली विफलताएं हो सकती हैंं।
#### 5.  एपीआई के माध्यम से मॉडल निष्कर्षण
  हमलावर एक आंशिक मॉडल को दोहराने या एक छाया मॉडल बनाने के लिए पर्याप्त आउटपुट एकत्र करने के लिए सावधानीपूर्वक तैयार किए गए इनपुट और Prompt इंजेक्शन तकनीकों का उपयोग करके मॉडल एपीआई को query कर सकते हैंं। यह न केवल बौद्धिक संपदा चोरी के जोखिम पैदा करता हैं, बल्कि मूल मॉडल की अखंडता को भी कम करता हैं।
#### 6.  कार्यात्मक मॉडल प्रतिकृति
  सिंथेटिक प्रशिक्षण डेटा उत्पन्न करने के लिए लक्ष्य मॉडल का उपयोग करना हमलावरों को एक अन्य मूलभूत मॉडल को ठीक करने की अनुमति दे सकता हैं, जिससे एक कार्यात्मक समकक्ष बन सकता हैं। यह पारंपरिक query-आधारित निष्कर्षण विधियों को बढ़ाता हैं, जो proprietary मॉडल और प्रौद्योगिकियों के लिए महत्वपूर्ण जोखिम प्रस्तुत करता हैं।
#### 7.  साइड-चैनल हमले
  दुर्भावनापूर्ण हमलावर साइड-चैनल हमलों को निष्पादित करने, मॉडल वेट और वास्तुशिल्प जानकारी की कटाई के लिए LLM के इनपुट फ़िल्टरिंग तकनीकों का exploit कर सकते हैंं। यह मॉडल की सुरक्षा से compromise कर सकता हैं और आगे के exploit का कारण बन सकता हैं।

### रोकथाम एवं बचाव के लिये रणनीतियाँ

#### 1.  इनपुट सत्यापन
  यह सुनिश्चित करने के लिए सख्त इनपुट सत्यापन को लागू करें कि इनपुट उचित आकार की सीमा से अधिक न हों।
#### 2.  लॉग्स और लॉगप्रोब के एक्सपोज़र को सीमित करें
  API प्रतिक्रियाओं में `logit_bias` और` logprobs` के जोखिम को प्रतिबंधित या ऑब्जेक्ट करें। विस्तृत संभावनाओं का खुलासा किए बिना केवल आवश्यक जानकारी प्रदान करें।
#### 3.  दर सीमित
  किसी एकल स्रोत इकाई के अनुरोधों की संख्या को प्रतिबंधित करने के लिए दर सीमित और user कोटा को लागू करें एक निश्चित समय अवधि में कर सकते हैंं।
#### 4.  संसाधन आवंटन प्रबंधन
  किसी एकल user को रोकने के लिए या अत्यधिक संसाधनों का सेवन करने से अनुरोध करने के लिए गतिशील रूप से संसाधन आवंटन की निगरानी और प्रबंधन करें।
#### 5.  टाइमआउट और थ्रॉटलिंग
  लंबे समय तक संसाधन की खपत को रोकने के लिए संसाधन-गहन संचालन के लिए टाइमआउट और थ्रॉटल प्रोसेसिंग सेट करें।
#### 6.sandbox तकनीक
  नेटवर्क संसाधनों, आंतरिक सेवाओं और एपीआई तक LLM की पहुंच को प्रतिबंधित करें।
  - यह सभी सामान्य परिदृश्यों के लिए विशेष रूप से महत्वपूर्ण हैं क्योंकि यह इनसाइडर जोखिमों और खतरों को शामिल करता हैं। इसके अलावा, यह LLM एप्लिकेशन को डेटा और संसाधनों के लिए पहुंच की सीमा को नियंत्रित करता हैं, जिससे साइड-चैनल हमलों को कम करने या रोकने के लिए एक महत्वपूर्ण नियंत्रण तंत्र के रूप में काम होता हैं।
#### 7.  व्यापक लॉगिंग, निगरानी और विसंगति का पता लगाना
  संसाधन के उपयोग की लगातार निगरानी करें और संसाधन की खपत के असामान्य पैटर्न का पता लगाने और प्रतिक्रिया देने के लिए लॉगिंग को लागू करें।
#### 8.  वॉटरमार्किंग
  LLM आउटपुट के अनधिकृत उपयोग को एम्बेड करने और पता लगाने के लिए वॉटरमार्किंग फ्रेमवर्क को लागू करें।
#### 9.  ग्रेसफुल डिग्रेडेशन
  पूरी तरह से विफलता के बजाय आंशिक कार्यक्षमता बनाए रखने के लिए, भारी भार के तहत सुशोभित रूप से नीचा दिखाने के लिए सिस्टम को डिजाइन करें।
#### 10.  कतारबद्ध कार्यों को सीमित करें और मजबूत रूप से स्केल करें
  डायनामिक स्केलिंग और लोड बैलेंसिंग को शामिल करते हुए, अलग -अलग मांगों को संभालने और सुसंगत सिस्टम प्रदर्शन सुनिश्चित करने के लिए लोड बैलेंस को शामिल करते हुए, कतारबद्ध कार्यों और कुल कार्यों की संख्या पर प्रतिबंध लागू करें।
#### 11.  प्रतिकूल मजबूती प्रशिक्षण
  प्रतिकूल प्रश्नों और निष्कर्षण प्रयासों का पता लगाने और कम करने के लिए ट्रेन मॉडल।
#### 12.  ग्लिच टोकन फ़िल्टरिंग
  मॉडल के संदर्भ विंडो में जोड़ने से पहले ज्ञात गड़बड़ टोकन और स्कैन आउटपुट की सूची बनाएं।
#### 13.  एक्सेस कंट्रोल
  LLM मॉडल repository और प्रशिक्षण वातावरण तक अनधिकृत पहुंच को सीमित करने के लिए भूमिका-आधारित एक्सेस कंट्रोल (आरबीएसी) और कम से कम विशेषाधिकार के सिद्धांत सहित मजबूत पहुंच नियंत्रणों को लागू करें।
#### 14.  केंद्रीकृत ML मॉडल इन्वेंटरी
  उत्पादन में उपयोग किए जाने वाले मॉडल के लिए एक केंद्रीकृत ML मॉडल इन्वेंटरी या रजिस्ट्री का उपयोग करें, उचित शासन और पहुंच नियंत्रण सुनिश्चित करें।
#### 15.  स्वचालित MLOPS deployment
  बुनियादी ढांचे के भीतर पहुंच और deployment नियंत्रणों को कसने के लिए शासन, ट्रैकिंग और अनुमोदन वर्कफ़्लो के साथ स्वचालित MLOPS deployment को लागू करें।

### उदाहरण स्वरूप हमले के परिदृश्य

#### परिदृश्य#1: अनियंत्रित इनपुट आकार
  एक हमलावर एक LLM एप्लिकेशन के लिए एक असामान्य रूप से बड़ा इनपुट प्रस्तुत करता हैं जो texts डेटा को संसाधित करता हैं, जिसके परिणामस्वरूप अत्यधिक मेमोरी उपयोग और सीपीयू लोड होता हैं, संभवतः सिस्टम को दुर्घटनाग्रस्त हो जाता हैं या सेवा को काफी धीमा कर देता हैं।
#### परिदृश्य#2: बार -बार अनुरोध
  एक हमलावर LLM एपीआई के लिए अनुरोधों की एक उच्च मात्रा प्रसारित करता हैं, जिससे कम्प्यूटेशनल संसाधनों की अत्यधिक खपत होती हैं और सेवा को वैध userओं के लिए अनुपलब्ध बना दिया जाता हैं।
#### परिदृश्य#3: संसाधन-गहन प्रश्न
  एक हमलावर शिल्प विशिष्ट इनपुट्स को LLM की सबसे कम्प्यूटेशनल रूप से महंगी प्रक्रियाओं को ट्रिगर करने के लिए डिज़ाइन किया गया हैं, जिससे लंबे समय तक सीपीयू उपयोग और संभावित प्रणाली की विफलता होती हैं।
#### परिदृश्य#4: वॉलेट का इनकार (डॉव)
  एक हमलावर क्लाउड-आधारित AI सेवाओं के पे-प्रति-उपयोग मॉडल का फायदा उठाने के लिए अत्यधिक संचालन उत्पन्न करता हैं, जिससे सेवा प्रदाता के लिए अस्थिर लागत होती हैं।
#### परिदृश्य#5: कार्यात्मक मॉडल प्रतिकृति
  एक हमलावर सिंथेटिक प्रशिक्षण डेटा उत्पन्न करने के लिए LLM के एपीआई का उपयोग करता हैं और एक और मॉडल को ठीक करने के लिए, एक कार्यात्मक समकक्ष और पारंपरिक मॉडल निष्कर्षण सीमाओं को दरकिनार करता हैं।
#### परिदृश्य#6: सिस्टम इनपुट फ़िल्टरिंग को बायपास करना
  एक दुर्भावनापूर्ण हमलावर इनपुट फ़िल्टरिंग तकनीकों और LLM के प्रस्ताव को साइड-चैनल हमला करने और उनके नियंत्रण के तहत एक रिमोट नियंत्रित संसाधन के लिए मॉडल की जानकारी प्राप्त करने के लिए LLM के प्रस्ताव को बायपास करता हैं।

### संबंधित लिंक
1. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)
2. [arXiv:2403.06634 Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634) **arXiv**
3. [Runaway LLaMA | How Meta's LLaMA NLP model leaked](https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/): **Deep Learning Blog**
4. [You wouldn't download an AI, Extracting AI models from mobile apps](https://altayakkus.substack.com/p/you-wouldnt-download-an-ai): **Substack blog**
5. [A Comprehensive Defense Framework Against Model Extraction Attacks](https://ieeexplore.ieee.org/document/10080996): **IEEE**
6. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html): **Stanford Center on Research for Foundation Models (CRFM)**
7. [How Watermarking Can Help Mitigate The Potential Risks Of LLMs?](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html): **KD Nuggets**
8. [Securing AI Model Weights Preventing Theft and Misuse of Frontier Models](https://www.rand.org/content/dam/rand/pubs/research_reports/RRA2800/RRA2849-1/RAND_RRA2849-1.pdf)
9. [Sponge Examples: Energy-Latency Attacks on Neural Networks: Arxiv White Paper](https://arxiv.org/abs/2006.03463) **arXiv**
10. [Sourcegraph Security Incident on API Limits Manipulation and DoS Attack](https://about.sourcegraph.com/blog/security-update-august-2023) **Sourcegraph**

### संबंधित फ्रेमवर्क और टैक्सोनॉमी

Infrastructure deployment, applied environment controls  तथा अन्य सर्वोत्तम उपायों से संबंधित व्यापक जानकारी, परिदृश्यों की रणनीतियों के लिए इस खंड का संदर्भ लें।

- [MITRE CWE-400: Uncontrolled Resource Consumption](https://cwe.mitre.org/data/definitions/400.html) **MITRE Common Weakness Enumeration**
- [AML.TA0000 ML Model Access: Mitre ATLAS](https://atlas.mitre.org/tactics/AML.TA0000) & [AML.T0024 Exfiltration via ML Inference API](https://atlas.mitre.org/techniques/AML.T0024) **MITRE ATLAS**
- [AML.T0029 - Denial of ML Service](https://atlas.mitre.org/techniques/AML.T0029) **MITRE ATLAS**
- [AML.T0034 - Cost Harvesting](https://atlas.mitre.org/techniques/AML.T0034) **MITRE ATLAS**
- [AML.T0025 - Exfiltration via Cyber Means](https://atlas.mitre.org/techniques/AML.T0025) **MITRE ATLAS**
- [OWASP Machine Learning Security Top Ten - ML05:2023 Model Theft](https://owasp.org/www-project-machine-learning-security-top-10/docs/ML05_2023-Model_Theft.html) **OWASP ML Top 10**
- [API4:2023 - Unrestricted Resource Consumption](https://owasp.org/API-Security/editions/2023/en/0xa4-unrestricted-resource-consumption/) **OWASP Web Application Top 10**
- [OWASP Resource Management](https://owasp.org/www-project-secure-coding-practices-quick-reference-guide/) **OWASP Secure Coding Practices**
