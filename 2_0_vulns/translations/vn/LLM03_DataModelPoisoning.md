## LLM03: Data and Model Poisoning

### Mô tả

Điểm khởi đầu của bất kỳ phương pháp học máy nào là dữ liệu đào tạo, đơn giản là “văn bản thô”. Để có khả năng cao (ví dụ: có kiến ​​thức về ngôn ngữ và thế giới), văn bản này phải bao gồm nhiều lĩnh vực, thể loại và ngôn ngữ. Một mô hình ngôn ngữ lớn sử dụng mạng nơ-ron sâu để tạo ra đầu ra dựa trên các mẫu học được từ dữ liệu đào tạo. Do đó, các Mô hình ngôn ngữ lớn (LLM) phụ thuộc rất nhiều vào lượng lớn dữ liệu đào tạo đa dạng để tạo ra đầu ra thành công.

Đầu độc dữ liệu đề cập đến việc thao túng dữ liệu tiền đào tạo hoặc dữ liệu liên quan đến quá trình tinh chỉnh hoặc nhúng để đưa vào các lỗ hổng (tất cả đều có các vectơ tấn công riêng biệt và đôi khi được chia sẻ), cửa hậu hoặc thành kiến ​​có thể gây tổn hại đến tính bảo mật, hiệu quả hoặc hành vi đạo đức của mô hình. Thông tin bị đầu độc có thể được đưa ra cho người dùng hoặc tạo ra các rủi ro khác như suy giảm hiệu suất, khai thác phần mềm phía sau và tổn hại đến danh tiếng. Ngay cả khi người dùng không tin tưởng vào đầu ra AI có vấn đề, các rủi ro vẫn còn, bao gồm khả năng mô hình bị suy giảm và khả năng gây hại cho danh tiếng thương hiệu.

- Dữ liệu tiền đào tạo đề cập đến quá trình đào tạo mô hình dựa trên một tác vụ hoặc tập dữ liệu.
- Tinh chỉnh liên quan đến việc lấy một mô hình hiện có đã được đào tạo và điều chỉnh nó cho phù hợp hơn hoặc tập trung hơn bằng bằng cách đào tạo sử dụng một tập dữ liệu được quản lý. Tập dữ liệu này thường bao gồm các ví dụ về đầu vào và đầu ra mong muốn tương ứng.
- Quá trình nhúng là quá trình chuyển đổi số hóa dữ liệu phân loại (thường là văn bản) có thể được sử dụng để đào tạo mô hình ngôn ngữ. Quá trình nhúng liên quan đến việc biểu diễn các từ hoặc cụm từ từ dữ liệu văn bản dưới dạng vectơ trong không gian vectơ liên tục. Các vectơ thường được tạo bằng cách đưa dữ liệu văn bản vào mạng nơ-ron đã được đào tạo trên một tập văn bản lớn.

Tấn công các giai đoạn của vòng đời phát triển mô hình là bắt buộc phải hiểu để xác định nơi dữ liệu bị đầu độc có thể xảy ra và từ nguồn nào, tùy thuộc vào bản chất của cuộc tấn công và mục tiêu tấn công. Đầu độc dữ liệu được coi là một cuộc tấn công toàn vẹn vì việc giả mạo dữ liệu đào tạo sẽ ảnh hưởng đến khả năng đưa ra dự đoán chính xác của mô hình. Dĩ nhiên, các nguồn dữ liệu bên ngoài có rủi ro cao hơn vì người tạo mô hình không kiểm soát được dữ liệu hoặc không có mức độ tin cậy cao cũng như khó xác định nội dung không chứa thành kiến, thông tin sai lệch hoặc nội dung không phù hợp. Đầu độc dữ liệu có thể làm giảm hiệu suất của mô hình, đưa vào nội dung có thành kiến ​​hoặc có hại và thậm chí khai thác các hệ thống phía dưới. Những rủi ro này đặc biệt cao đối với các nguồn dữ liệu bên ngoài, có thể chứa nội dung chưa được xác minh hoặc độc hại.

Các mô hình thường được phân phối dưới dạng hiện vật thông qua các kho lưu trữ mô hình được chia sẻ hoặc các nền tảng nguồn mở, khiến chúng dễ bị tổn thương do các lỗ hổng được kế thừa. Ngoài ra, vì các mô hình được triển khai dưới dạng phần mềm và tích hợp với cơ sở hạ tầng, chúng có thể gây ra các rủi ro như cửa hậu và vi-rút máy tính khi các môi trường này được nhúng vào.

Cho dù là nhà phát triển, khách hàng hay người dùng chung của LLM, điều quan trọng là phải hiểu các rủi ro liên quan đến việc tương tác với các mô hình không độc quyền. Các lỗ hổng này có thể ảnh hưởng đến tính hợp pháp của đầu ra mô hình do các quy trình đào tạo của chúng. Đặc biệt, các nhà phát triển có thể phải đối mặt với rủi ro từ các cuộc tấn công trực tiếp hoặc gián tiếp vào dữ liệu nội bộ hoặc của bên thứ ba được sử dụng để tinh chỉnh và nhúng, điều này cuối cùng có thể ảnh hưởng đến tất cả người dùng LLM.

### Các ví dụ phổ biến về lỗ hổng

1.  Những kẻ xấu cố tình đưa dữ liệu không chính xác hoặc có hại vào bộ dữ liệu đào tạo của mô hình. Điều này có thể đạt được thông qua các kỹ thuật như [Split-View Data Poisoning - Đầu độc dữ liệu chia đôi chế độ xem ](https://github.com/GangGreenTemperTatum/speaking/blob/main/dc604/hacker-summer-camp-23/Ads%20_%20Poisoning%20Web%20Training%20Datasets%20_%20Flow%20Diagram%20-%20Exploit%201%20Split-View%20Data%20Poisoning.jpeg) or [Frontrunning Poisoning - Đầu độc dẫn đầu](https://github.com/GangGreenTemperTatum/speaking/blob/main/dc604/hacker-summer-camp-23/Ads%20_%20Poisoning%20Web%20Training%20Datasets%20_%20Flow%20Diagram%20-%20Exploit%202%20Frontrunning%20Data%20Poisoning.jpeg). 
    - Mô hình nạn nhân được đào tạo bằng thông tin sai lệch được phản ánh trong kết quả của lời nhắc AI tạo ra cho người dùng.
2. Kẻ xấu có thể thực hiện việc tiêm trực tiếp nội dung sai lệch, thiên vị hoặc có hại vào quá trình đào tạo của mô hình và trả về trong các kết quả đầu ra tiếp theo.
3. Người dùng vô tình đưa vào thông tin nhạy cảm hoặc độc quyền trong quá trình tương tác với mô hình, điều này có thể được phản ánh trong các kết quả đầu ra tiếp theo.
4. Một mô hình được đào tạo bằng cách sử dụng dữ liệu chưa được xác minh về nguồn gốc, xuất xứ hoặc nội dung trong bất kỳ ví dụ nào ở giai đoạn đào tạo, điều này có thể dẫn đến kết quả sai nếu dữ liệu bị tiêm nhiễm hoặc không chính xác.
5. Việc truy cập tài nguyên không hạn chế hoặc thử nghiệm không đầy đủ có thể khiến mô hình thu thập dữ liệu không an toàn, dẫn đến kết quả đầu ra bị sai lệch hoặc có hại.
   - Một kịch bản ví dụ có thể xảy ra trong quá trình tinh chỉnh, trong đó các lệnh gọi suy luận từ các máy khách (client) LLM có thể cố ý hoặc vô ý đưa thông tin bí mật vào kho dữ liệu của mô hình. Dữ liệu nhạy cảm này sau đó có thể bị tiết lộ cho một máy khách khác thông qua các đầu ra được tạo ra.
   - Một ví dụ khác là trong quá trình thu thập dữ liệu từ xa trên web từ các nguồn chưa được xác minh để hỗ trợ lấy dữ liệu dùng cho mục đích đào tạo hoặc tinh chỉnh các yếu tố của vòng đời mô hình.

### Chiến lược phòng ngừa và giảm thiểu

1. Duy trì hồ sơ chi tiết về nguồn gốc và chuyển đổi dữ liệu. Sử dụng các công cụ như "ML-BOM" (Machine Learning Bill of Materials) IE, OWASP CycloneDX để theo dõi chuỗi cung ứng dữ liệu. Nếu kế thừa các mô hình của bên thứ ba, hãy cân nhắc nghiên cứu thẻ mô hình của nhà phát triển để minh bạch xung quanh giai đoạn thu thập dữ liệu mô hình và đào tạo cũng như các trường hợp sử dụng có liên quan.
2. Xác minh tính hợp pháp chính xác của các nguồn dữ liệu mục tiêu và dữ liệu chứa trong cả giai đoạn tiền đào tạo, tinh chỉnh và nhúng. Phát triển công cụ để cho phép theo dõi dữ liệu đào tạo của mô hình, nguồn gốc của dữ liệu và mối liên kết của chúng và hợp tác với các nhà cung cấp bảo mật có uy tín để phát triển các giao thức bổ sung chống lại việc đầu độc dữ liệu và nội dung độc hại.
3. Yêu cầu về việc lưu trữ, bảo vệ dữ liệu, đảm bảo rằng chỉ những đối tác an toàn, đáng tin cậy mới được tích hợp vào chuỗi cung ứng. Xác thực đầu ra của mô hình so với các nguồn dữ liệu bên ngoài đáng tin cậy để phát hiện sự không nhất quán hoặc dấu hiệu đầu độc.
4. Xác minh trường hợp sử dụng của bạn cho LLM và ứng dụng mà nó sẽ tích hợp vào. Tạo các mô hình khác nhau thông qua dữ liệu đào tạo riêng biệt hoặc tinh chỉnh cho các trường hợp sử dụng khác nhau để tạo ra đầu ra AI tạo ra chi tiết và chính xác hơn theo trường hợp sử dụng đã xác định.
5. Đảm bảo có đủ biện pháp kiểm soát cơ sở hạ tầng để ngăn mô hình thu thập các nguồn dữ liệu không mong muốn có thể cản trở quá trình đào tạo.
6. Sử dụng bộ lọc đầu vào và bộ phân loại nghiêm ngặt cho dữ liệu đào tạo cụ thể hoặc lọc và phân loại các loại nguồn dữ liệu để kiểm soát khối lượng dữ liệu bị làm giả. Làm sạch dữ liệu, với các kỹ thuật như phát hiện giá trị ngoại lệ thống kê và phương pháp phát hiện dị thường để phát hiện và loại bỏ dữ liệu đối nghịch khỏi khả năng được đưa vào quy trình tinh chỉnh.
7. Sử dụng Kiểm soát phiên bản dữ liệu (DVC - Use Data Version Control) để xác định và theo dõi chặt chẽ một phần của tập dữ liệu có thể đã bị thao túng, xóa hoặc thêm vào dẫn đến đầu độc. Kiểm soát phiên bản rất quan trọng không chỉ trong phát triển phần mềm mà còn trong phát triển các mô hình ML, trong đó nó liên quan đến việc theo dõi và quản lý các thay đổi trong cả mã nguồn và các hiện vật như tập dữ liệu và mô hình. Trong ML, tập dữ liệu đóng vai trò là hiện vật đầu vào cho các quy trình đào tạo, trong khi mô hình là hiện vật đầu ra, khiến việc quản lý phiên bản của chúng trở nên cần thiết để duy trì tính toàn vẹn và khả năng tái tạo của quy trình phát triển.
8. Sử dụng cơ sở dữ liệu vector để lưu trữ và quản lý thông tin do người dùng cung cấp, có thể giúp ngăn ngừa việc đầu độc người dùng khác và cho phép điều chỉnh trong quá trình sản xuất mà không cần phải đào tạo lại toàn bộ mô hình.
9. Vận hành các chiến dịch của đội đỏ (Redteam) để kiểm tra khả năng bảo vệ mô hình và môi trường chống lại việc đầu độc dữ liệu. Việc luyện tập chống lại tấn công thông qua các kỹ thuật như học tập liên kết và ràng buộc có thể giảm thiểu tác động của các ngoại lệ hoặc huấn luyện đối nghịch để trở nên mạnh mẽ trước những nhiễu loạn của việc đầu độc dữ liệu huấn luyện.
10.  Kiểm tra và phát hiện, bằng cách đo lường tổn thất trong giai đoạn đào tạo và phân tích các mô hình đã đào tạo để phát hiện các dấu hiệu của cuộc tấn công đầu độc bằng cách phân tích hành vi của mô hình trên các đầu vào thử nghiệm cụ thể.
   - Giám sát và cảnh báo về số lượng phản hồi sai lệch vượt quá ngưỡng.
   - Sử dụng vòng lặp của con người để xem xét phản hồi và kiểm tra.
   - Triển khai LLM chuyên dụng để đánh giá chuẩn mực so với các phản hồi không mong muốn và đào tạo LLM bằng cách sử dụng [reinforcement learning techniques - kỹ thuật học tăng cường](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy).
11. Trong quá trình suy luận, việc kết hợp phương pháp Tạo tăng cường truy xuất (RAG) với các kỹ thuật nền tảng của các thực thể dữ liệu đáng tin cậy có thể giúp giảm nguy cơ sai lệch và các kết quả không chính xác. Điều này rất quan trọng để tránh rủi ro khi các dữ liệu được đưa vào mô hình. Việc kết hợp này cung cấp các nguồn thông tin và định nghĩa kiến thức thực tế, chính xác, và phù hợp với ngôn ngữ, từ đó giúp cải thiện độ tin cậy và chính xác của các kết quả suy luận từ mô hình.

### Ví dụ về các kịch bản tấn công

1. Đầu ra gây hiểu lầm: Kẻ tấn công thao túng dữ liệu đào tạo hoặc sử dụng kỹ thuật tiêm nhanh để làm sai lệch đầu ra của LLM. Kết quả là, mô hình tạo ra các phản hồi gây hiểu lầm hoặc thiên vị, có khả năng định hình ý kiến ​​của người dùng, phát tán thông tin sai lệch hoặc thậm chí kích động các hành động có hại như ngôn từ kích động thù địch.
2. Tiêm dữ liệu độc hại: Nếu không lọc và vệ sinh dữ liệu đúng cách, người dùng có ác ý có thể đưa dữ liệu độc hại vào bộ dữ liệu đào tạo. Dữ liệu này có thể khiến mô hình học và truyền bá các thành kiến ​​có hại hoặc thông tin sai lệch, sau đó có thể được truyền bá cho người dùng khác thông qua các đầu ra được tạo ra.
3. Làm giả có chủ đích: Một tác nhân hoặc đối thủ cạnh tranh độc hại cố tình tạo và nhập các tài liệu giả mạo hoặc có hại vào dữ liệu đào tạo của mô hình. Mô hình, thiếu cơ chế kiểm tra đầy đủ, kết hợp thông tin không chính xác này, dẫn đến đầu ra phản ánh những điểm không chính xác này và có khả năng gây hại hoặc gây hiểu lầm cho người dùng.
4. Tấn công tiêm nhanh: Việc lọc không đầy đủ cho phép kẻ tấn công chèn dữ liệu có hại hoặc gây hiểu lầm vào mô hình thông qua tiêm nhanh. Cuộc tấn công này tận dụng dữ liệu đầu vào của người dùng mà mô hình vô tình kết hợp vào dữ liệu đào tạo của nó, dẫn đến việc phát tán các đầu ra bị xâm phạm hoặc thiên vị cho những người dùng tiếp theo.
### Liên kết tham khảo

1. [How data poisoning attacks corrupt machine learning models](https://www.csoonline.com/article/3613932/how-data-poisoning-attacks-corrupt-machine-learning-models.html): **CSO Online**
2. [MITRE ATLAS (framework) Tay Poisoning](https://atlas.mitre.org/studies/AML.CS0009/): **MITRE ATLAS**
3. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/): **Mithril Security**
4. [Poisoning Language Models During Instruction](https://arxiv.org/abs/2305.00944): **Arxiv White Paper 2305.00944**
5.  [Poisoning Web-Scale Training Datasets - Nicholas Carlini | Stanford MLSys #75](https://www.youtube.com/watch?v=h9jf1ikcGyk): **Stanford MLSys Seminars YouTube Video**
6.  [ML Model Repositories: The Next Big Supply Chain Attack Target](https://www.darkreading.com/cloud-security/ml-model-repositories-next-big-supply-chain-attack-target) **OffSecML**
7.  [Data Scientists Targeted by Malicious Hugging Face ML Models with Silent Backdoor](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) **JFrog**
8.  [Backdoor Attacks on Language Models](https://towardsdatascience.com/backdoor-attacks-on-language-models-can-we-trust-our-models-weights-73108f9dcb1f): **Towards Data Science**
9.  [Can you trust ChatGPT’s package recommendations?](https://vulcan.io/blog/ai-hallucinations-package-risk) **VULCAN**