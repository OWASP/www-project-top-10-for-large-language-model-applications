## LLM06: 2025 اختیارات بیش‌ از حد

### توضیحات

به یک سامانه مبتنی بر مدل زبانی بزرگ (LLM) اغلب درجه‌ای از اختیار توسط توسعه‌دهنده‌اش برای انجام اقداماتی در پاسخ به یک پرامپت داده می‌شود - یعنی توانایی فراخوانی توابع یا تعامل با سامانه‌های دیگر از طریق افزونه‌ها (که گاهی توسط عرضه‌کنندگان مختلف به عنوان ابزار، مهارت یا پلاگین نامیده می‌شود). تصمیم‌گیری در مورد اینکه کدام افزونه فراخوانی شود، ممکن است به یک عامل (Agent) مدل زبانی بزرگ (LLM) نیز واگذار شود تا به صورت پویا بر اساس پرامپت ورودی یا خروجی مدل زبانی بزرگ (LLM) تعیین شود. سامانه‌های مبتنی بر عامل (Agent) معمولاً با استفاده از خروجی فراخوانی‌های قبلی به طور مکرر یک مدل زبانی بزرگ (LLM) را فراخوانی می‌کنند، تا فراخوانی‌های بعدی را زمینه‌سازی و هدایت کنند.

اختیارات بیش از حد، یک آسیب‌پذیری‌ است که باعث می‌شود اقدامات مخربی در پاسخ به خروجی‌های غیرمنتظره، مبهم یا دستکاری شده از یک مدل زبانی بزرگ (LLM) انجام شود، صرف نظر از این‌ که چه چیزی باعث اختلال در عملکرد مدل زبانی بزرگ (LLM) شده است. محرک‌های رایج عبارتند از:

* توهم/خیال‌پردازی ناشی از پرامپت‌های بی‌خطر با طراحی ضعیف یا صرفاً یک مدل با عملکرد ضعیف;
* تزریق مستقیم/غیرمستقیم پرامپت یک کاربر مخرب، فراخوانی قبلی یک افزونه مخرب/به‌ خطر افتاده، یا یک عامل همکار مخرب/به خطر افتاده (در سامانه‌های چندعاملی/مشارکتی).

ریشه اصلی اختیار بیش از حد معمولاً یک یا چند مورد از موارد زیر است:

* عملکرد بیش از حد;
* مجوز‌‎های بیش از حد;
* خودمختاری بیش از حد;

اختیار بیش از حد می‌تواند منجر به طیف گسترده‌ای از اثرات شامل محرمانگی، صحت و دسترسی‌پذیری شود و به این بستگی دارد که یک برنامه مبتنی بر مدل زبانی بزرگ (LLM) قادر به تعامل با چه سامانه‌هایی است.

توجه: «اختیارات بیش از حد» با «مدیریت ناامن خروجی» که مربوط به بررسی ناکافی خروجی‌‍‌های مدل‌های زبانی بزرگ (LLM) می‌‎شود، متفاوت است.

### نمونه‌های رایج از مخاطرات امنیتی

#### ۱. عملکرد بیش از حد

  عامل مدل زبانی بزرگ (LLM Agent) به افزونه‌هایی دسترسی دارد که شامل توابعی هستند که برای عملکرد مد نظر سامانه مورد نیاز نیستند. برای مثال، توسعه‌دهنده باید به عامل مدل زبانی بزرگ (LLM Agent) توانایی خواندن اسناد از مخزن را بدهد، اما افزونه شخص ثالثی که انتخاب می‌کند، علاوه بر آن شامل توانایی تغییر و حذف اسناد نیز می‌باشد.

#### ۲. عملکرد بیش از حد

  ممکن است یک افزونه در مرحله توسعه، آزمایش شده و به دلیل وجود یک جایگزین بهتر کنار گذاشته‌ شده باشد، اما افزونه اصلی همچنان برای عامل مدل زبانی بزرگ (LLM Agent) در دسترسی باقی مانده باشد.

#### ۳. عملکرد بیش از حد

  افزونه مدل زبانی بزرگ (LLM Plugin) با قابلیت‌های فراوان و بی انتها، نمی‌تواند دستورالعمل‌های ورودی را برای فرمان‌هایی که خارج از عملکرد مورد نظر برنامه هستند، به درستی پالایش کند. به عنوان مثال، افزونه‌ای که برای اجرای یک فرمان خاص می‌باشد نمی‌تواند به درستی از اجرای دستورات شل دیگر جلوگیری کند.

#### ۴. مجوزهای بیش از حد

  افزونه مدل زبانی بزرگ (LLM) دارای مجوزهای دسترسی در سامانه‌هایی پایین‌دستی است که برای عملکرد مد نظر برنامه مورد نیاز نیستند. به عنوان مثال، افزونه‌ای که برای خواندن داده‌ها در نظر گرفته شده است، با استفاده از هویتی به کارساز پایگاه داده متصل می‌شود که نه تنها مجوز SELECT دارد، بلکه مجوزهای INSERT ،UPDATE و  DELETE نیز دارد.

#### ۵. مجوزهای بیش از حد

  افزونه مدل زبانی بزرگ (LLM) که برای انجام عملیات در نقش یک کاربر خاص طراحی شده است، با استفاده از یک هویت عمومی با سطح دسترسی بالا به سامانه‌های پایین‌دستی دسترسی پیدا می‌کند. برای مثال، افزونه‌ای برای خواندن فضای ذخیره‌سازی اسناد کاربر فعلی، با یک حساب کاربری دارای سطح دسترسی بالا که به فایل‌های متعلق به همه کاربران دسترسی دارد، به مخزن اسناد متصل می‌شود.

#### ۶. خودمختاری بیش از حد

  برنامه یا افزونه مبتنی بر LLM نمی‌تواند اقدامات پرمخاطره را به‌طور مستقل تأیید و تصویب کند. برای مثال، افزونه‌ای که به کاربر اجازه می‌دهد اسناد خود را حذف کند، بدون هیچ‌گونه تأییدی از سوی کاربر، عملیات حذف را انجام می‌دهد.

### راهبردهای پیشگیری و کاهش مخاطره

اقدامات زیر می‌تواند از اختیارات بیش از حد جلوگیری کند:

#### ۱. افزونه‌ها را به حداقل برسانید

  اجازه دهید عامل‌های مدل‌های زبانی بزرگ (LLM Agents) به فراخوانی حداقلِ افزونه‌های ضروری دسترسی داشته باشند. برای مثال، اگر یک سامانه مبتنی بر مدل زبانی بزرگ (LLM) نیازی به دریافت محتوای یک URL ندارد، نباید چنین افزونه‌ای به عامل‌ مدل‌های زبانی بزرگ (LLM Agent) ارائه شود.

#### ۲. عملکرد افزونه‌ها را به حداقل برسانید

  توابعی که در افزونه‌های مدل‌های زبانی بزرگ (LLM) پیاده‌سازی می‌شوند را به حداقلِ ضروری محدود کنید. برای مثال، یک افزونه که برای خلاصه‌سازی رایانامه‌ها به صندوق پستی اینترنتی کاربر دسترسی پیدا می‌کند، تنها باید به قابلیت خواندن رایانامه‌ها نیاز داشته باشد، بنابراین افزونه نباید شامل عملکردهای دیگری مانند حذف یا ارسال پیام‌ها باشد.

#### ۳. از افزونه‌های با قابلیت‌های نامحدود اجتناب کنید

  تا حد امکان از استفاده از افزونه‌های با قابلیت‌های نامحدود (به عنوان مثال، افزونه دارای مجوز اجرای دستور واسط خط فرمان (shell)، بارگیری URL و ...) اجتناب کنید و از افزونه‌هایی با عملکردهای ریزدانه‌تر استفاده کنید. به عنوان مثال، برنامه مبتنی بر مدل زبانی بزرگ (LLM) ممکن است به نوشتن خروجی در یک فایل نیاز داشته باشد. اگر این کار با استفاده از یک افزونه برای اجرای یک تابع واسط خط فرمان (shell) پیاده‌سازی شود، دامنه اقدامات نامطلوب بسیار زیاد است (هر دستور واسط خط فرمان (shell) دیگری می‌تواند اجرا شود). جایگزین امن‌تر این است که یک افزونه خاص‌منظوره برای نوشتن فایل ایجاد کنید که فقط همان عملکرد خاص را پیاده‌سازی کند.

#### ۴. دسترسی‌های افزونه را به حداقل برسانید

  دسترسی‌هایی را که افزونه‌های مدل زبانی بزرگ (LLM) به سایر سامانه‌ها دارند، به حداقلِ ضروری محدود کنید تا دامنه اقدامات نامطلوب محدود شود. برای مثال، یک عامل مدل زبانی بزرگ (LLM) که از یک پایگاه داده محصول برای ارائه توصیه‌های خرید به مشتری استفاده می‌کند، ممکن است فقط به دسترسی خواندن به جدول «products» نیاز داشته باشد؛ این عامل نباید به جداول دیگر دسترسی داشته باشد یا توانایی درج، به‌روزرسانی یا حذف رکوردها را داشته باشد. این امر باید با اعمال دسترسی‌های مناسب پایگاه داده برای هویتی که افزونه مدل زبانی بزرگ (LLM) برای اتصال به پایگاه داده استفاده می‌کند، انجام شود.

#### ۵. افزونه‌ها را در نقش کاربر اجرا کنید

  مجوز و محدوده امنیتی کاربر را ردیابی کنید تا اطمینان حاصل شود که اقداماتی که به نمایندگی از یک کاربر انجام می‌شوند، در سامانه‌های پایین‌دستی در نقش آن کاربر خاص و با حداقل امتیازات لازم اجرا می‌شوند. برای مثال، یک افزونه مدل زبانی بزرگ (LLM) که مخزن کد کاربر را می‌خواند، باید از کاربر بخواهد تا از طریق OAuth و با حداقل دامنه مورد نیاز احراز هویت انجام دهد.

#### ۶. تأیید کاربر را الزامی کنید

  از کنترل "انسان در حلقه" (human-in-the-loop) استفاده کنید تا نیاز باشد یک انسان اقدامات با تأثیر بالا را قبل از اجرا تأیید کند. این مورد ممکن است در یک سامانه پایین‌دستی (خارج از محدوده برنامه LLM) یا در خود افزونه LLM پیاده‌سازی شود. برای مثال، یک برنامه مبتنی بر مدل زبانی بزرگ (LLM) که محتوای رسانه‌های اجتماعی را به نمایندگی از یک کاربر ایجاد و پست می‌کند، باید یک روال تأیید کاربر را در افزونه‌ای که عملیات «POST» را پیاده‌سازی می‌کند، بگنجاند.

#### ۷. میانجی‌‍‌گری کامل

  به جای اتکا به مدل زبانی بزرگ (LLM) برای تصمیم‌گیری در مورد مجاز بودن یا نبودن یک عمل، مجوزها را در سامانه‌های پایین‌دستی پیاده‌سازی کنید. اصل میانجی‌گری کامل را اعمال کنید تا تمام درخواست‌های ارسالی به سامانه‌های پایین‌دستی از طریق افزونه‌ها، در برابر سیاست‌های امنیتی اعتبارسنجی شوند.

#### ۸. ورودی‌ها و خروجی‌های مدل زبانی بزرگ (LLM) را پاک‌سازی کنید

  از به‌روش‌های کدنویسی امن، مانند اعمال توصیه‌های OWASP در ASVS (استاندارد وارسی امنیت برنامه کاربردی)، با تمرکز ویژه بر پاک‌سازی ورودی‌ها پیروی کنید. از آزمون‌های ایستای امنیت برنامه کاربردی (SAST) و آزمون‌های پویا و تعاملی برنامه  (DAST، IAST) در گردش‌کار توسعه (Development Pipelines) استفاده کنید.

گزینه‌های زیر از اختیارات بیش از حد جلوگیری نمی‌کنند، اما می‌توانند میزان آسیب‌های وارده را محدود کنند:

* فعالیت افزونه‌های مدل زبانی بزرگ (LLM) و سامانه‌های پایین‌دستی را رویدادنگاری و پایش کنید تا مشخص شود اقدامات نامطلوب در کجا اتفاق می‌افتند و بر اساس آن واکنش نشان دهید.
* محدودیت نرخ را اعمال کنید تا تعداد اقدامات نامطلوبی که می‌توانند در یک دوره زمانی معین انجام شوند، کاهش یابد و فرصت کشف اقدامات نامطلوب از طریق پایش، قبل از وقوع آسیب‌های قابل توجه، افزایش یابد.

### نمونه فرانامه‌های حمله

به یک برنامه دستیار شخصی مبتنی بر مدل زبانی بزرگ (LLM)، از طریق یک افزونه، دسترسی به صندوق پستی فرد داده می‌شود تا محتوای ایمیل‌های دریافتی را خلاصه کند. افزونه برای اجرای این عملکرد، به توانایی خواندن پیام‌ها نیاز دارد، با این حال افزونه‌ای که توسعه‌دهنده سامانه انتخاب کرده است، شامل توابعی برای ارسال پیام نیز است. علاوه بر این، برنامه در برابر یک حمله تزریق غیرمستقیم پرامپت آسیب‌پذیر است، به طوری که رایانامه دریافتیِ مخرب، مدل زبانی بزرگ (LLM) را فریب می‌دهد تا به عامل دستور دهد که صندوق ورودی کاربر را برای یافتن اطلاعات حساس پویش کرده و آن را به آدرس ایمیل مهاجم ارسال کند. می‌توان با روش‌های زیر از این امر جلوگیری کرد:

* حذف عملکردهای اضافی با استفاده از افزونه‌ای که فقط قابلیت‌های خواندن رایانامه را دارد,
* حذف دسترسی‌های بیش از حد با احراز هویت در خدمت رایانامه کاربر از طریق یک نشست OAuth با دامنه فقط مجوز خواندن (read-only) و/یا
* حذف خودمختاری بیش از حد با ملزم کردن کاربر به بررسی دستی و فشردن دکمه «send» برای هر رایانامه‌ی پیش‌نویس شده توسط افزونه مدل زبانی بزرگ (LLM).

همچنین، می‌توان با اعمال محدودیت نرخ بر روی واسط ارسال رایانامه، آسیب‌های وارده را کاهش داد.

### پیوند‌های مرجع

1. [Slack AI data exfil from private channels](https://promptarmor.substack.com/p/slack-ai-data-exfiltration-from-private): **PromptArmor**
2. [Rogue Agents: Stop AI From Misusing Your APIs](https://www.twilio.com/en-us/blog/rogue-ai-agents-secure-your-apis): **Twilio**
3. [Embrace the Red: Confused Deputy Problem](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./): **Embrace The Red**
4. [NeMo-Guardrails: Interface guidelines](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/security/guidelines.md): **NVIDIA Github**
5. [Simon Willison: Dual LLM Pattern](https://simonwillison.net/2023/Apr/25/dual-llm-pattern/): **Simon Willison**
