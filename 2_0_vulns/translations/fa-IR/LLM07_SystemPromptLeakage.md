## LLM07:2025 نشت پرامپت سیستم

### توضیحات

آسیب‌پذیری نشت پرامپت سیستم در مدل‌های زبانی بزرگ (LLM) به این مخاطره اشاره دارد که پرامپت‌ها یا دستورالعمل‌های سیستمی که برای هدایت رفتار مدل استفاده می‌شوند، می‌توانند حاوی اطلاعات حساسی باشند که نباید فاش شوند. پرامپت‌های سیستم برای هدایت خروجی مدل بر اساس الزامات برنامه طراحی شده‌اند، اما ممکن است ناخواسته حاوی اطلاعات محرمانه باشند. در صورت افشای این اطلاعات، می‌توان از آن‌ها برای تسهیل سایر حملات استفاده کرد.

باید توجه داشت که پرامپت سیستم را نباید محرمانه تلقی کرد و از آن به عنوان یک کنترل امنیتی استفاده نمود. بنابراین، اطلاعات حساسی مانند اعتبارنامه‌ها، رشته‌های اتصال (connection strings) و غیره نباید در متن پرامپت سیستم گنجانده شوند.

به طور مشابه، اگر پرامپت سیستم حاوی اطلاعاتی نظیر توصیف نقش‌ها و مجوزهای مختلف یا داده‌های حساس مانند رشته‌های اتصال یا گذرواژه باشد، در حالی که افشای چنین اطلاعاتی ممکن است مفید باشد، مخاطره امنیتی اصلی این نیست که این اطلاعات افشا شده‌اند، بلکه این است که برنامه به جای انجام مدیریت صحیح نشست و بررسی‌های مجازشماری، این موارد را به LLM واگذار کرده است و داده‌های حساس در مکانی ذخیره شده‌اند که نباید شوند.

به طور خلاصه: افشای پرامپت سیستم به خودی خود مخاطره واقعی را ایجاد نمی‌کند—مخاطره امنیتی در اجزای زیربنایی نهفته است، که می‌تواند شامل افشای اطلاعات حساس، دورزدن ایمنی‌بندهای (Guardrails) سامانه، جداسازی نادرست اختیارات دسترسی و غیره باشد. حتی اگر کلمات دقیقا فاش نشوند، مهاجمانی که با سامانه تعامل دارند تقریباً به طور قطع می‌توانند بسیاری از ایمنی‌بندها و محدودیت‌های قالب‌بندی موجود در زبان پرامپت سیستم را در حین استفاده از برنامه، ارسال عبارات به مدل و مشاهده نتایج، شناسایی کنند.

### نمونه‌های رایج از مخاطرات امنیتی

#### ۱. افشای عملکردهای حساس
  پرامپت سیستم ممکن است اطلاعات یا عملکردهای حساسی را که قرار است محرمانه نگه داشته شوند، مانند معماری حساس سامانه، کلیدهای API، اعتبارنامه‌های پایگاه‌داده یا توکن‌های کاربر را فاش کند. این اطلاعات می‌توانند توسط مهاجمان استخراج شده و برای دسترسی غیرمجاز به برنامه مورد سوء استفاده قرار گیرند. به عنوان مثال، پرامپتی که نوع پایگاه‌داده به کار رفته در یک ابزار را فاش کند، می‌تواند به مهاجم این امکان را بدهد که آن را مورد هدف حمله SQL injection قرار دهد.
#### ۲. افشای قوانین داخلی
  پرامپت سیستم، اطلاعاتی در مورد فرآیندهای تصمیم‌گیری داخلی که باید محرمانه نگه داشته شوند، فاش می‌کند. این اطلاعات به مهاجمان اجازه می‌دهد تا در مورد نحوه عملکرد برنامه شناخت کسب کنند که می‌تواند به آن‌ها کمک کند تا از ضعف‌ها بهره‌برداری کرده یا کنترل‌های برنامه را دور بزنند. به عنوان مثال، یک برنامک بانکی که دارای ربات گفتگو (chatbot) است، ممکن است پیام سیستم خود را طوری تنظیم کرده باشد که اطلاعاتی مانند زیر را افشا کند:
  
>«حد مجاز تراکنش برای یک کاربر روزانه ۵۰۰۰ دلار است. مبلغ کل وام برای یک کاربر ۱۰,۰۰۰ دلار است.»
    
  این اطلاعات به مهاجمان اجازه می‌دهد کنترل‌های امنیتی برنامک را دور بزنند، مانند انجام تراکنش‌های بیشتر از حد مجاز یا دور زدن مبلغ کل وام.
  
#### ۳. افشای معیارهای پالایش
  پرامپت سیستم ممکن است از مدل بخواهد که محتوای حساس را پالایش یا رد کند. برای مثال، یک مدل ممکن است پرامپت سیستمی مانند زیر داشته باشد:
  
>«اگر کاربری اطلاعاتی در مورد کاربر دیگری درخواست کرد، همیشه با عبارت 'متاسفم، نمی‌توانم به این درخواست کمکی ارائه دهم' پاسخ دهید.»
>
#### ۴. افشای مجوزها و نقش‌های کاربران
  پرامپت سیستم ممکن است ساختارهای داخلی نقش‌ها یا سطوح مجوزهای برنامه را فاش کند. برای مثال، پرامپت سیستم ممکن است این مورد را فاش کند:
  
>«نقش کاربر مدیر دسترسی کامل برای تغییر سوابق کاربران را می‌دهد.»

  اگر مهاجمان از اینگونه مجوزهای مبتنی بر نقش آگاه شوند، ممکن است اقدام به حملات افزایش سطح دسترسی (Privilege Escalation) کنند.

### راهبردهای پیشگیری و کاهش مخاطره

#### ۱. جداسازی داده‌های حساس از پرامپت‌های سیستم
  از گنجاندن هرگونه اطلاعات حساس (مانند کلیدهای API، کلیدهای احراز هویت، نام‌های پایگاه داده، نقش‌های کاربران، ساختار مجوزهای برنامه) به طور مستقیم در پرامپت‌های سیستم خودداری کنید. در عوض، چنین اطلاعاتی را به سامانه‌هایی که مدل به‌طور مستقیم به آن‌ها دسترسی ندارد، منتقل کنید.
#### ۲. از اتکا به پرامپت‌های سیستم برای کنترل دقیق رفتار مدل خودداری کنید
  از آنجایی که LLMها در برابر حملات دیگری مانند تزریق پرامپت (prompt injection) که می‌تواند پرامپت سیستم را تغییر دهد، آسیب‌پذیر هستند، توصیه می‌شود که در صورت امکان از پرامپت‌های سیستم برای کنترل رفتار مدل استفاده نشود. در عوض، برای اطمینان از این رفتار، به سامانه‌های خارج از LLM تکیه کنید. به عنوان مثال، شناسایی و جلوگیری از محتوای زیان‌بار باید در سامانه‌های مستقل بیرونی انجام شود.
#### ۳. پیاده‌سازی ایمنی‌بندها (Guardrails)
  سامانه‌ای از ایمنی‌بندها (guardrails) را خارج از خود مدل LLM پیاده‌سازی کنید. در حالی که آموزش رفتارهایی ویژه به مدل، مانند آموزش اینکه مدل پرامپت سیستم خودش را فاش نکند، می‌تواند موثر باشد، اما تضمینی وجود ندارد که مدل همیشه به این امر پایبند بماند. سامانه مستقلی که بتواند خروجی را بررسی کرده و تعیین کند که آیا مدل با انتظارات مطابقت دارد یا خیر، به فرمان‌های پرامپت سیستم ترجیح داده می‌شود.
#### ۴. اطمینان حاصل کنید که کنترل‌های امنیتی به طور مستقل از LLM اعمال شوند.
  کنترل‌های حیاتی مانند جداسازی اختیارات دسترسی، بررسی مرزهای مجازشماری (authorization) و موارد مشابه نباید چه از طریق پرامپت سیستم چه به روش‌های دیگری به LLM واگذار شوند. این کنترل‌ها باید به شیوه‌ای قطعیت‌پذیر و قابل ممیزی اعمال شوند و LLMها (در حال حاضر) برای این کار مناسب نیستند. در مواردی که یک عامل (agent) وظایفی را انجام می‌دهد، اگر آن وظایف نیاز به سطوح مختلف دسترسی داشته باشند، باید از چندین عامل استفاده شود که هر کدام با حداقل اختیارات (least privileges) لازم برای انجام وظایف مورد نظر پیکربندی شده‌اند.

### نمونه‌هایی از فرانامه‌های حمله

#### فرانامه #۱
   یک LLM دارای پرامپت سیستم است که حاوی مجموعه‌ای از اعتبارنامه‌ها برای دسترسی به ابزاری است که از آن استفاده می‌کند. پرامپت سیستم درز پیدا کرده و به دست یک مهاجم می‌افتد بنابراین وی قادر به استفاده از این اعتبارنامه‌ها برای اهداف مخرب دیگر خود خواهد بود.
#### فرانامه #۲
  یک LLM دارای پرامپت سیستم است که تولید محتوای توهین‌آمیز، پیوند‌های بیرونی و اجرای کد را ممنوع می‌کند. مهاجمی این پرامپت سیستم را استخراج کرده و سپس با استفاده از حمله تزریق پرامپت (prompt injection) این دستورالعمل‌ها را دور زده و حمله اجرای کد از راه دور را زمینه‌سازی می‌کند.

### پیوند‌های مرجع

1. [SYSTEM PROMPT LEAK](https://x.com/elder_plinius/status/1801393358964994062): Pliny the prompter
2. [Prompt Leak](https://www.prompt.security/vulnerabilities/prompt-leak): Prompt Security
3. [chatgpt_system_prompt](https://github.com/LouisShark/chatgpt_system_prompt): LouisShark
4. [leaked-system-prompts](https://github.com/jujumilk3/leaked-system-prompts): Jujumilk3
5. [OpenAI Advanced Voice Mode System Prompt](https://x.com/Green_terminals/status/1839141326329360579): Green_Terminals

### چارچوب‌ها و طبقه‌بندی‌های مرتبط

برای کسب اطلاعات جامع، فرانامه‌ها، راهبردهای مربوط به استقرار زیرساخت، کنترل‌های محیطی کاربردی و سایر به‌روش‌ها، به این بخش مراجعه کنید.

- [AML.T0051.000 - LLM Prompt Injection: Direct (Meta Prompt Extraction)](https://atlas.mitre.org/techniques/AML.T0051.000) **MITRE ATLAS**
