## LLM09:2025 کژاطلاعات (Misinformation)

### توضیحات

کژاطلاعات ناشی از مدل‌های زبانی بزرگ (LLMs) یک آسیب‌پذیری اساسی برای برنامه‌هایی است که به این مدل‌ها وابسته هستند. کژاطلاعات زمانی رخ می‌دهد که LLMها اطلاعات غلط یا گمراه‌کننده‌ای تولید می‌کنند که ظاهراً معتبر هستند. این آسیب‌پذیری می‌تواند منجر به نقض امنیت، آسیب به اعتبار و مسئولیت‌های حقوقی شود.

یکی از علل اصلی کژاطلاعات، توهم (hallucination) است؛ زمانی که مدل زبانی بزرگ محتوایی تولید می‌کند که به نظر دقیق می‌رسد، اما ساختگی است. توهم‌ها زمانی رخ می‌دهند که مدل‌های زبانی بزرگ برای پر کردن خلأهای موجود در داده‌های آموزشی خود از الگوهای آماری استفاده می‌کنند، بدون اینکه واقعاً محتوا را درک کنند. در نتیجه، مدل ممکن است پاسخ‌هایی ارائه دهد که به نظر درست می‌آیند، اما اساساً بی‌پایه و اساس هستند. در حالی که توهمات منبع اصلی اطلاعات نادرست هستند، تنها علت آن نیستند؛ سوگیری‌های ناشی از داده‌های آموزشی و اطلاعات ناقص نیز می‌توانند به این مشکل دامن بزنند.

مسئله مرتبط دیگری که وجود دارد، وابستگی بیش از حد است. وابستگی بیش از حد زمانی رخ می‌دهد که کاربران بیش از حد به محتوای تولید شده توسط LLM اعتماد می‌کنند و دقت و صحت آن را بررسی نمی‌کنند. این وابستگی بیش از حد تاثیر کژاطلاعات را تشدید می‌کند، زیرا کاربران ممکن است داده‌های غلط را بدون بررسی کافی در تصمیمات یا فرایندهای حیاتی خود وارد کنند.

### نمونه‌های رایج از مخاطرات امنیتی

#### ۱. اشتباهات واقعی
  این مدل گزاره‌های نادرستی تولید می‌کند که منجر به تصمیم‌گیری کاربران بر اساس اطلاعات اشتباه می‌شود. برای مثال، چت‌بات شرکت "Air Canada" به مسافران کژاطلاعات ارائه داد که منجر به اختلالات عملیاتی و مشکلات حقوقی شد. در نتیجه، این شرکت هواپیمایی تحت پیگرد قانونی قرار گرفت.
  (پیوند مرجع: [BBC](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know))
#### ۲. ادعاهای بی‌پایه و اساس
  این مدل ادعاهای بی‌اساسی را تولید می‌کند که می‌تواند به‌ویژه در زمینه‌های حساسی مانند مراقبت‌های بهداشتی یا دادرسی‌های حقوقی، زیان‌بار باشد. برای مثال، "ChatGPT" پرونده‌های حقوقی جعلی و ساختگی ایجاد کرد که منجر به مشکلات قابل‌توجهی در دادگاه شد.
  (پیوند مرجع: [LegalDive](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/))
#### ۳. معرفی نادرست تخصص
  مدل این توهم را ایجاد می‌کند که مباحث پیچیده را درک می‌کند و کاربران را نسبت به سطح تخصص خود گمراه می‌کند. برای مثال، مشخص شده است که چت‌بات‌ها ظرافت مسائل مربوط به سلامتی را به‌‌شکل نادرستی جلوه می‌دهند و در جایی که عدم قطعیتی وجود ندارد، عدم قطعیت را القا می‌کنند، که این باعث شد کاربران تصور کنند درمان‌های تایید‌نشده هنوز محل بحث هستند.
  (پیوند مرجع: [KFF](https://www.kff.org/health-misinformation-monitor/volume-05/))
#### ۴. تولید کد ناامن
  مدل، کتابخانه‌های ناامن یا ناموجود را پیشنهاد می‌کند که می‌توانند هنگام ادغام در سامانه‌های نرم‌افزاری، آسیب‌پذیری‌هایی ایجاد کنند. به عنوان مثال، مدل‌های زبان بزرگ (LLM) استفاده از کتابخانه‌های ناامن شخص ثالث را پیشنهاد می‌دهند که اگر بدون اعتبارسنجی به آن‌ها اعتماد شود، می‌تواند منجر به مخاطرات امنیتی شود.
  (پیوند مرجع: [Lasso](https://www.lasso.security/blog/ai-package-hallucinations))

### راهبردهای پیشگیری و کاهش مخاطره

#### ۱. روش Retrieval-Augmented Generation (RAG)
  از«Retrieval-Augmented Generation» در حین تولید پاسخ برای افزایش اتکاپذیری خروجی‌های مدل به کمک بازیابی اطلاعات مرتبط و تاییدشده از پایگاه‌های داده خارجیِ قابل‌اعتماد، استفاده کنید. این امر به کاهش مخاطره توهم‌ها و کژاطلاعات کمک می‌کند.
#### ۲. تنظیم دقیق مدل
  برای بهبود کیفیت خروجی، مدل را با تنظیم دقیق یا بازنمود‌های برداری (embeddings) ارتقا دهید. روش‌هایی مانند تنظیم کارآمد پارامتر ("PET") و زنجیره تفکر ("chain-of-thought prompting") می‌توانند به کاهش رخداد کژاطلاعات کمک کنند.
#### ۳. راستی‌آزمایی متقابل و نظارت انسانی
  کاربران را تشویق کنید تا خروجی‌های مدل‌های زبان بزرگ (LLM) را با منابع معتبر خارجی مقایسه کنند تا از صحت اطلاعات مطمئن شوند. فرآیندهای نظارت انسانی و راستی‌آزمایی اطلاعات را به‌ویژه برای اطلاعات حساس یا حیاتی، پیاده‌سازی کنید. اطمینان حاصل کنید که بازبینی‌کنندگان انسانی به درستی آموزش دیده‌اند تا از اتکای بیش‌ازحد به محتوای تولیدشده توسط هوش مصنوعی جلوگیری شود.
#### ۴. سازوکارهای اعتبارسنجی خودکار
  ابزارها و فرآیندهایی را پیاده‌سازی کنید تا به طور خودکار خروجی‌های کلیدی را اعتبارسنجی کنند، به ویژه خروجی‌های حاصل از محیط‌های حساس و پرمخاطره.
#### ۵. انتقال مخاطره
  مخاطرات و آسیب‌های احتمالی مرتبط با محتوای تولیدشده توسط LLM را شناسایی کنید، سپس این مخاطرات و محدودیت‌ها، از جمله احتمال وجود کژاطلاعات را به‌وضوح به کاربران منتقل کنید.
#### ۶. شیوه‌های کدنویسی امن
  استانداردهای کدنویسی امن را به‌کار بگیرید تا از ادغام آسیب‌پذیری‌های ناشی از پیشنهادات نادرست کد جلوگیری شود.
#### ۷. طراحی رابط کاربری
  رابط‌های کاربری و  API ها را به گونه‌ای طراحی کنید که استفاده مسئولانه از LLM ها را تقویت کنند، مانند ادغام پالایش‌گرهای محتوا، برچسب‌گذاری واضح محتوای تولیدشده توسط هوش مصنوعی و اطلاع‌رسانی به کاربران در مورد محدودیت‌های اتکاپذیری و دقت. به‌طور مشخص در مورد محدودیت‌های حوزه کاربرد مورد نظر، اطلاع‌رسانی کنید.
#### ۸. آموزش و تربیت
  آموزش‌های جامعی برای کاربران در مورد محدودیت‌های LLM ها، اهمیت راستی‌آزمایی مستقل محتوای تولیدشده و لزوم تفکر انتقادی ارائه دهید. در زمینه‌های خاص، آموزش‌های تخصصی مربوط به آن حوزه را ارائه کنید تا اطمینان حاصل شود که کاربران می‌توانند به‌طور موثر خروجی‌های LLM را در زمینه تخصصی خود ارزیابی کنند.

### نمونه‌هایی از فرانامه‌های حمله

#### فرانامه #۱
  مهاجمان با دستیارهای کدنویسی پرطرفدار تلاش می‌کنند تا نام‌های بسته‌های رایج که به اشتباه پیشنهاد می‌شوند را شناسایی کنند. هنگامی که این کتابخانه‌های پرکاربرد، اما غیرموجود را شناسایی کردند، بسته‌های مخربی را با آن نام‌ها در مخازنِ پُرکاربرد منتشر می‌کنند. توسعه‌دهندگان، با اتکا به پیشنهادات دستیار کدنویسی، ناآگاهانه این بسته‌های ازپیش‌آماده‌شده را در نرم‌افزار خود ادغام می‌کنند. در نتیجه، مهاجمان دسترسی غیرمجاز به سامانه‌ها را پیدا می‌کنند، کد مخرب تزریق می‌کنند یا درب‌های پشتی ایجاد می‌کنند که منجر به رخنه‌های امنیتی قابل‌توجه و به خطر افتادن داده‌های کاربر می‌شود.
#### فرانامه #۲
  شرکتی یک چت‌بات برای تشخیص پزشکی،  بدون اطمینان از دقت کافی آن را ارائه می‌دهد. این چت‌بات اطلاعات نادرستی ارائه می‌دهد که منجر به عواقب زیان‌باری برای بیماران می‌شود. در نتیجه، شرکت به‌دلیل خسارات وارده تحت پیگرد قانونی قرار می‌گیرد. در این حالت، فروپاشی ایمنی و امنیتی نیازی به یک مهاجم خرابکار نداشته، بلکه از نظارت و اتکاپذیری ناکافی سامانه LLM ناشی شده است. در این فرانامه، شرکت بدون آنکه مهاجم فعالی وجود داشته باشد در معرض مخاطرات اعتباری و مالی قرار می‌گیرد.

### پیوند‌های مرجع

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/): **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know): **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/): **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations): **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655): **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/): **Microsoft**

### چارچوب‌ها و طبقه‌بندی‌های مرتبط

برای کسب اطلاعات جامع، فرانامه‌ها، راهبردهای مربوط به استقرار زیرساخت، کنترل‌های محیطی کاربردی و سایر به‌روش‌ها، به این بخش مراجعه کنید.

- [AML.T0048.002 - Societal Harm](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
