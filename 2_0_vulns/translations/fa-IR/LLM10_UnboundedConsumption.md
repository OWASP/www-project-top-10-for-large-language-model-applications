## LLM10:2025 مصرف بی حد و مرز

### توضیحات

مصرف بی‌حد و مرز به فرآیندی اشاره دارد که در آن یک مدل زبانی بزرگ (LLM) بر اساس پرس‌وجوها یا پرامپت‌های ورودی، خروجی‌هایی تولید می‌کند. استنتاج یک عملکرد حیاتی مدل‌های زبانی بزرگ (LLM) است که شامل به کارگیری الگوها و دانش آموخته شده، برای تولید پاسخ‌ها یا پیش‌بینی‌های مرتبط است.

حملاتی که با هدف اختلال در خدمت، تخلیه منابع مالی هدف یا حتی سرقت مالکیت فکری با شبیه‌سازی رفتار یک مدل طراحی شده‌اند، همه برای موفقیت به یک نوع مشترک از آسیب‌پذیری‌های امنیتی متکی هستند. مصرف بی‌حد و مرز زمانی رخ می‌دهد که یک برنامه مدل زبانی بزرگ (LLM) به کاربران اجازه می‌دهد تا استنتاج‌های بیش از حد و کنترل نشده‌ای انجام دهند که منجر به خطراتی مانند منع خدمت (DoS)، زیان‌های اقتصادی، سرقت مدل، و کاهش کیفیت خدمات می‌شود. نیازهای محاسباتی بالای مدل‌های زبانی بزرگ (LLM)، به ویژه در محیط‌های ابری، آنها را در برابر بهره‌برداری از منابع و استفاده غیرمجاز آسیب‌پذیر می‌کند.

### نمونه‌های رایج از مخاطرات امنیتی

#### ۱. طغیان ورودی با طول متغیر
  مهاجمان می‌توانند مدل زبانی بزرگ (LLM) را با ورودی‌های متعدد با طول‌های متغیر سرریز کنند و از ناکارآمدی‌های پردازشی سوءاستفاده کنند. این امر می‌تواند منجر به اتمام منابع شده و به طور بالقوه سامانه را از پاسخگویی باز دارد و به طور قابل توجهی بر در دسترس پذیری خدمت تأثیر بگذارد.
#### ۲. آسیب‌پذیری Denial of Wallet (DoW)
  مهاجمان با ایجاد حجم بالایی از عملیات، از مدل هزینه-به-ازای-مصرف (cost-per-use) خدمات هوش مصنوعی ابری سوءاستفاده می‌کنند، که منجر به تحمیل بار مالی غیرقابل تحمل بر ارائه‌دهنده و مواجهه او با خطر ورشکستگی می‌شود.
#### ۳. سرریز مستمر ورودی
  ارسال مداوم ورودی‌هایی که از پنجره محتوای (context window) مدل زبانی بزرگ (LLM) فراتر می‌روند، می‌تواند منجر به استفاده بیش از حد از منابع محاسباتی شود و در نتیجه باعث کاهش کیفیت خدمات و اختلالات عملیاتی گردد.
#### ۴. پرس‌وجوهای با مصرف بالای منابع
  ارسال پرس‌وجوهای سنگین غیرمعمول از نظر ظرفیت منابع که شامل توالی‌های پیچیده یا الگوهای زبانی دشوار هستند، می‌تواند منابع سامانه را به اتمام برساند و منجر به زمان‌های پردازش طولانی و خرابی‌های احتمالی سامانه شود.
#### ۵. استخراج مدل از طریق واسط برنامه نویسی کاربردی (API)
  مهاجمان ممکن است با استفاده از ورودی‌های با دقت ساخته شده و روش‌های تزریق پرامپت، از API مدل پرس‌وجو کنند تا خروجی‌های کافی برای تکثیر جرئی مدل یا ایجاد مدل بدل (Shadow Model) جمع‌آوری کنند که نه تنها خطرات سرقت مالکیت فکری را به همراه دارد، بلکه صحت مدل اصلی را نیز تضعیف می‌کند.
#### ۶. تکثیر عملکردی مدل
  استفاده از مدل هدف برای تولید داده‌های آموزشی مصنوعی می‌تواند به مهاجمان اجازه دهد تا یک مدل پایه‌ای دیگر را تنظیم دقیق (fine-tune) کنند و یک معادل کارا ایجاد نمایند. این کار، روش‌های سنتی استخراج مبتنی بر پرس‌وجو را دور می‌زند و خطرات قابل توجهی برای مدل‌ها و فناوری‌های اختصاصی ایجاد می‌کند.
#### ۷. حملات کانال جانبی (Side-Channel)
  مهاجمان مخرب ممکن است از روش‌های پالایش ورودی مدل زبانی بزرگ (LLM) برای اجرای حملات کانال جانبی، جمع‌آوری وزن‌های مدل و اطلاعات معماری آن، سوءاستفاده کنند. این امر می‌تواند امنیت مدل را به خطر بیندازد و منجر به سوءاستفاده‌های بیشتر شود.

### راهبردهای پیشگیری و کاهش مخاطره

#### ۱. اعتبارسنجی ورودی
  اعتبارسنجی ورودی سختگیرانه‌ای را برای اطمینان از اینکه ورودی‌ها از محدودیت‌های معقول اندازه تجاوز نمی‌کنند، پیاده‌سازی کنید.
#### ۲. محدود کردن نمایش Logitها و Logprobها
  نمایش `logit_bias` و `logprobs` در پاسخ‌های API را محدود یا مبهم کنید. فقط اطلاعات لازم را بدون افشای جزئی احتمالات ارائه دهید.
#### ۳. محدودسازی نرخ درخواست
  محدودسازی نرخ درخواست و سهمیه‌های کاربری (user quotas) برای محدود کردن تعداد درخواست‌هایی که یک منبع موجودیت واحد می‌تواند در یک دوره زمانی مشخص ارسال کند اعمال کنید.
#### ۴. مدیریت تخصیص منابع
  تخصیص منابع را به صورت پویا پایش و مدیریت کنید تا از مصرف بیش از حد منابع توسط یک کاربر یا درخواست واحد جلوگیری شود.
#### ۵. وقفه‌ها (Timeouts) و کندسازی (Throttling)
  برای عملیات‌های پرمصرفِ منابع، وقفه‌ها و محدودیت‌ سرعت پردازش را تنظیم کنید تا از مصرف طولانی‌مدت منابع جلوگیری شود.
#### ۶. روش‌های جعبه شنی (Sandbox)
  دسترسی مدل زبانی بزرگ (LLM) را به منابع شبکه، خدمات داخلی و APIها محدود کنید.
این امر به ویژه برای همه فرانامه‌های رایج حائز اهمیت است زیرا خطرات و تهدیدات داخلی را در بر می گیرد. علاوه بر این، میزان دسترسی برنامه مدل زبانی بزرگ (LLM) به داده ها و منابع را کنترل می کند، در نتیجه به عنوان یک سازوکار کنترلی حیاتی برای کاهش یا جلوگیری از حملات کانال جانبی عمل می کند.
#### ۷. بطور جامع رویدادنگاری، پایش و تشخیص ناهنجاری را انجام دهید.
  به طور مداوم مصرف منابع را پایش کنید و برای شناسایی الگوهای غیرعادی مصرف منابع و پاسخ به آنها، رویدادنگاری را پیاده سازی کنید.
#### ۸. ته‌نقش‌گذاری (Watermarking)
  چارچوب‌های ته‌نقش‌گذاری را برای تعبیه و شناسایی استفاده غیرمجاز از خروجی‌های مدل زبانی بزرگ (LLM) پیاده‌سازی کنید.
#### ۹. تنزل مطبوع (Graceful Degradation)
  سامانه را به گونه‌ای طراحی کنید که در زمان بار سنگین به تدریج تنزل یابد و به جای از کار افتادن کامل، عملکرد بخشی از خود را حفظ کند.
#### ۱۰. محدود کردن اقدامات در صف و مقیاس‌پذیری قوی
  محدودیت‌هایی را برای تعداد اقدامات در صف و کل اقدامات اعمال کنید، در حالی که مقیاس‌پذیری پویا و توازن بار را برای مدیریت نیازمندی‌‎های متغیر و تضمین عملکرد پایدار سامانه در نظر می‌گیرید.
#### ۱۱. آموزش مقاوم‌سازی در برابر حملات خصمانه
  مدل‌ها را برای شناسایی و کاهش پرس‌وجوهای خصمانه و تلاش‌های استخراج آموزش دهید.
#### ۱۲. پالایش نشان‌های معیوب (Glitch Token Filtering)
  لیست‌هایی از نشان‌های معیوب شناخته شده ایجاد کنید و قبل از افزودن خروجی به پنجره محتوای مدل، آن را پایش کنید.
#### ۱۳. کنترل‌های دسترسی
  کنترل‌های دسترسی قوی، از جمله کنترل دسترسی مبتنی بر نقش (RBAC) و اصل حداقل امتیاز را برای محدود کردن دسترسی غیرمجاز به مخازن مدل زبانی بزرگ (LLM) و محیط‌های آموزش پیاده‌سازی کنید.
#### ۱۴. فهرست متمرکز مدل‌های یادگیری ماشین
  از یک فهرست ثبت متمرکز مدل یادگیری ماشین برای مدل‌های مورد استفاده در محیط عملیاتی استفاده کنید و از حاکمیت و کنترل دسترسی مناسب اطمینان حاصل کنید.
#### ۱۵. استقرار خودکار عملیات یادگیری ماشین (MLOps)
  استقرار خودکار MLOps را با پیاده‌سازی نظارت، ردیابی و تاییدیه‌ی گردش‌کار برای تقویت کنترل‌های دسترسی و استقرار در زیرساخت انجام دهید.

### نمونه فرانامه‌های حمله

#### فرانامه #۱: اندازه ورودی کنترل نشده
  مهاجم یک ورودی بسیار بزرگ به یک برنامه مدل زبانی بزرگ (LLM) که داده‌های متنی را پردازش می کند ارسال می کند، که منجر به استفاده بیش از حد از حافظه و بار پردازشگر می شود و به طور بالقوه باعث خرابی سامانه یا کاهش قابل توجه سرعت خدمت می‌شود.
#### فرانامه #۲: درخواست‌های مکرر
  مهاجم حجم زیادی از درخواست‌ها را به API مدل زبانی بزرگ (API) ارسال می‌کند و باعث مصرف بیش از حد منابع محاسباتی می‌شود و خدمت را برای کاربران مجاز غیرقابل دسترس می‌کند.
#### فرانامه #۳: پرس‌وجوهای با مصرف بالای منابع
  مهاجم ورودی‌های خاصی را می‌سازد که برای فعال کردن پرمصرف‌ترین فرآیندهای محاسباتی مدل زبانی بزرگ (LLM) طراحی شده‌اند و منجر به استفاده طولانی‌مدت از پردازشگر و خرابی احتمالی سامانه می‌شوند.
#### فرانامه #۴: Denial of Wallet (DoW)
  مهاجم عملیات‌های بیش از حدی را برای سوء‌استفاده از مدل پرداخت به ازای مصرف (pay-per-use) از خدمات هوش مصنوعی ابری ایجاد می‌کند و باعث هزینه‌های غیرقابل تحمل برای ارائه‌دهنده خدمات می‌شود.
#### فرانامه #۵: تکثیر عملکردی مدل
  مهاجم از API مدل زبانی بزرگ (LLM) برای تولید داده‌های آموزشی مصنوعی و تنظیم دقیق (fine-tune) یک مدل دیگر استفاده می‌کند و یک معادل کارا از مدل اصلی ایجاد می‌کند و محدودیت‌های استخراج سنتی مدل را دور می‌زند.
#### فرانامه #۶: دور زدن پالایش ورودی سامانه
  مهاجم خرابکار، روش‌های پالایش ورودی و تنظیمات اولیه مدل زبانی بزرگ (LLM) را دور می‌زند تا یک حمله کانال جانبی انجام دهد و اطلاعات مدل را بازیابی و به یک منبع کنترل‌شده از راه دور تحت کنترل خود ارسال کند.

### پیوند‌های مرجع

1. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)
2. [arXiv:2403.06634 Stealing Part of a Production Language Model](https://arxiv.org/abs/2403.06634) **arXiv**
3. [Runaway LLaMA | How Meta's LLaMA NLP model leaked](https://www.deeplearning.ai/the-batch/how-metas-llama-nlp-model-leaked/): **Deep Learning Blog**
4. [I Know What You See:](https://arxiv.org/pdf/1803.05847.pdf): **Arxiv White Paper**
5. [A Comprehensive Defense Framework Against Model Extraction Attacks](https://ieeexplore.ieee.org/document/10080996): **IEEE**
6. [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html): **Stanford Center on Research for Foundation Models (CRFM)**
7. [How Watermarking Can Help Mitigate The Potential Risks Of LLMs?](https://www.kdnuggets.com/2023/03/watermarking-help-mitigate-potential-risks-llms.html): **KD Nuggets**
8. [Securing AI Model Weights Preventing Theft and Misuse of Frontier Models](https://www.rand.org/content/dam/rand/pubs/research_reports/RRA2800/RRA2849-1/RAND_RRA2849-1.pdf)
9. [Sponge Examples: Energy-Latency Attacks on Neural Networks: Arxiv White Paper](https://arxiv.org/abs/2006.03463) **arXiv**
10. [Sourcegraph Security Incident on API Limits Manipulation and DoS Attack](https://about.sourcegraph.com/blog/security-update-august-2023) **Sourcegraph**

### چارچوب‌ها و طبقه‌بندی‌های مرتبط

برای کسب اطلاعات جامع، فرانامه‌ها، راهبردهای مربوط به استقرار زیرساخت، کنترل‌های محیطی کاربردی و سایر به‌روش‌ها، به این بخش مراجعه کنید.

- [MITRE CWE-400: Uncontrolled Resource Consumption](https://cwe.mitre.org/data/definitions/400.html) **MITRE Common Weakness Enumeration**
- [AML.TA0000 ML Model Access: Mitre ATLAS](https://atlas.mitre.org/tactics/AML.TA0000) & [AML.T0024 Exfiltration via ML Inference API](https://atlas.mitre.org/techniques/AML.T0024) **MITRE ATLAS**
- [AML.T0029 - Denial of ML Service](https://atlas.mitre.org/techniques/AML.T0029) **MITRE ATLAS**
- [AML.T0034 - Cost Harvesting](https://atlas.mitre.org/techniques/AML.T0034) **MITRE ATLAS**
- [AML.T0025 - Exfiltration via Cyber Means](https://atlas.mitre.org/techniques/AML.T0025) **MITRE ATLAS**
- [OWASP Machine Learning Security Top Ten - ML05:2023 Model Theft](https://owasp.org/www-project-machine-learning-security-top-10/docs/ML05_2023-Model_Theft.html) **OWASP ML Top 10**
- [API4:2023 - Unrestricted Resource Consumption](https://owasp.org/API-Security/editions/2023/en/0xa4-unrestricted-resource-consumption/) **OWASP Web Application Top 10**
- [OWASP Resource Management](https://owasp.org/www-project-secure-coding-practices-quick-reference-guide/) **OWASP Secure Coding Practices**
