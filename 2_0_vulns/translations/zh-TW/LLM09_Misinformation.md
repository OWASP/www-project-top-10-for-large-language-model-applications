## LLM09:2025 錯誤資訊

### 描述

**錯誤資訊** (Misinformation) 是指 LLM (大型語言模型) 產生錯誤或具誤導性的內容，且此內容表面上看似可信。依賴 LLM 的應用程式若遭此漏洞影響，可能導致安全缺口、商譽受損以及法律訴訟。

錯誤資訊的成因之一是幻覺 (hallucination)：模型在訓練數據不足時，會依據統計模式填補空白，生成看似正確但實際全無依據的回應。此外，訓練數據本身的偏差與資訊不完整也會導致錯誤資訊。

與此相關的問題是過度依賴 (overreliance)，指使用者過度信任 LLM 的內容而未核實其正確性。在過度依賴下，使用者易將錯誤資訊納入關鍵決策或流程中，擴大錯誤資訊的影響。

### 常見風險實例

#### 1. 事實錯誤
模型產生錯誤的陳述，使使用者根據錯誤資訊做出決策。例如，加拿大航空 (Air Canada) 曾因其聊天機器人提供錯誤訊息給旅客而陷入法律糾紛。
(參考連結：[BBC](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know))

#### 2. 無依據主張
模型提出毫無根據的斷言，於敏感領域 (如醫療、法律) 特別有害。舉例來說，ChatGPT 曾捏造不存在的法律案例，導致法庭中產生重大問題。
(參考連結：[LegalDive](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/))

#### 3. 專業度誤導
模型顯示對複雜議題具備專業知識的假象，誤導使用者相信其權威性。例如，聊天機器人可能誤導使用者對健康議題的認知，造成不恰當的醫療建議。
(參考連結：[KFF](https://www.kff.org/health-misinformation-monitor/volume-05/))

#### 4. 不安全的程式碼產出
模型可能建議不安全或不存在的程式庫，若使用者未經查證便整合至系統中，將帶來安全風險。
(參考連結：[Lasso](https://www.lasso.security/blog/ai-package-hallucinations))

### 預防與緩解策略

#### 1. 使用檢索增強生成 (RAG)
透過檢索增強生成 (RAG) 從可信任的外部資料庫擷取相關與已驗證的資訊，以減低幻覺與錯誤資訊的風險。

#### 2. 模型微調
透過微調或嵌入 (embeddings) 改善模型輸出品質。採用參數高效調整 (Parameter-Efficient Tuning, PET) 與連鎖思維提示 (chain-of-thought prompting) 等技術，降低產生錯誤資訊的機率。

#### 3. 交叉驗證與人工審查
鼓勵使用者從可信外部來源交叉比對 LLM 輸出資訊的正確性。對關鍵或敏感資訊實施人工審核，並確保審查者不盲目依賴 LLM 的回應。

#### 4. 自動驗證機制
為高風險環境建立自動化驗證工具與流程，以確保關鍵輸出正確無誤。

#### 5. 風險溝通
明確向使用者傳達 LLM 產生錯誤資訊的風險與限制，使其瞭解內容可能不完全可信。

#### 6. 安全程式碼慣例
建立安全程式碼開發慣例，以免因不正確的程式碼建議而導入易受攻擊的程式碼。

#### 7. 使用者介面設計
在 APIs 與使用者介面中明確標示 AI 產生的內容及其限制，使使用者意識到模型的可靠性問題與適用範圍。

#### 8. 訓練與教育
為使用者提供全面的培訓，使其瞭解 LLM 的限制及獨立驗證資訊之重要性。針對特定領域提供專業訓練，確保使用者能有效評估 LLM 輸出。

### 攻擊情境範例

#### 情境 #1
攻擊者透過常見的程式碼助手實驗出經常被模型「幻覺」建議的不存在套件名稱，並在套件庫中上架惡意套件。開發者盲目信任模型建議而引用這些惡意套件，造成後門或惡意程式碼注入。

#### 情境 #2
一家公司提供醫療診斷的聊天機器人，未確保其輸出正確性。該聊天機器人提供不良資訊導致病患受到傷害，公司因此面臨法律訴訟。在此情境中，即使無惡意攻擊者，單純缺乏監管與可靠性已足以造成名譽和財務損失。

### 參考連結

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/) **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know) **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/) **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786) **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/) **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/) **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations) **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655) **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/) **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination) **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/) **Microsoft**

### 相關框架與分類法

請參考此區，以取得關於基礎架構部署、應用環境控管與其他最佳實務的完整資訊與範例策略。

- [AML.T0048.002 - Societal Harm](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
