## LLM09:2025 Εσφαλμένη Πληροφόρηση  (Misinformation)

### Περιγραφή

Η εσφαλμένη πληροφόρηση από τα LLM αποτελεί βασική ευπάθεια για τις εφαρμογές που βασίζονται σε αυτά τα μοντέλα. Η παραπληροφόρηση συμβαίνει όταν τα LLM παράγουν ψευδείς ή παραπλανητικές πληροφορίες που φαίνονται αξιόπιστες. Αυτή η ευπάθεια μπορεί να οδηγήσει σε παραβιάσεις της ασφάλειας, βλάβη της εταιρικής φήμης και νομική ευθύνη.

Μία από τις κύριες αιτίες της παραπληροφόρησης είναι η ψευδαίσθηση - όταν το LLM παράγει περιεχόμενο που φαίνεται ακριβές αλλά είναι επινοημένο. Οι ψευδαισθήσεις συμβαίνουν όταν τα LLMs συμπληρώνουν κενά στα δεδομένα εκπαίδευσής τους χρησιμοποιώντας στατιστικά μοτίβα, χωρίς να κατανοούν πραγματικά το περιεχόμενο. Ως αποτέλεσμα, το μοντέλο μπορεί να παράγει απαντήσεις που ακούγονται σωστές αλλά είναι εντελώς αβάσιμες. Αν και οι ψευδαισθήσεις αποτελούν σημαντική πηγή παραπληροφόρησης, δεν είναι η μόνη αιτία- οι προκαταλήψεις που εισάγονται από τα δεδομένα εκπαίδευσης και οι ελλιπείς πληροφορίες μπορούν επίσης να συνεισφέρουν.

Ένα συναφές ζήτημα είναι η υπερβολική εμπιστοσύνη. Η υπερβολική εμπιστοσύνη εμφανίζεται όταν οι χρήστες εμπιστεύονται υπερβολικά το περιεχόμενο που παράγεται από το LLM, χωρίς να επαληθεύουν την ακρίβειά του. Αυτή η υπερβολική εμπιστοσύνη επιδεινώνει τον αντίκτυπο της παραπληροφόρησης, καθώς οι χρήστες μπορεί να ενσωματώσουν λανθασμένα δεδομένα σε κρίσιμες αποφάσεις ή διαδικασίες χωρίς επαρκή έλεγχο.

### Συνήθη Παραδείγματα Κινδύνου

#### 1. Ανακρίβειες γεγονότων

  Το μοντέλο παράγει λανθασμένες δηλώσεις, οδηγώντας τους χρήστες να λαμβάνουν αποφάσεις με βάση λανθασμένες πληροφορίες. Για παράδειγμα, το chatbot της Air Canada παρείχε λανθασμένες πληροφορίες στους ταξιδιώτες, οδηγώντας σε διακοπές λειτουργίας και νομικές επιπλοκές. Ως αποτέλεσμα, η αεροπορική εταιρεία υπέστη επιτυχή μήνυση.
  (Σύνδεσμος αναφοράς: [BBC](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know))

#### 2. Ανυπόστατοι ισχυρισμοί

  Το μοντέλο παράγει αβάσιμους ισχυρισμούς, οι οποίοι μπορεί να είναι ιδιαίτερα επιζήμιοι σε ευαίσθητα περιβάλλοντα όπως η υγειονομική περίθαλψη ή οι νομικές διαδικασίες. Για παράδειγμα, το ChatGPT κατασκεύασε ψεύτικες νομικές υποθέσεις, οδηγώντας σε σημαντικά ζητήματα στο δικαστήριο.
  (Σύνδεσμος αναφοράς: [LegalDive](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/))

#### 3. Παραπλανητική παρουσίαση εμπειρογνωμοσύνης

  Το μοντέλο δίνει την ψευδαίσθηση της κατανόησης πολύπλοκων θεμάτων, παραπλανώντας τους χρήστες όσον αφορά το επίπεδο τεχνογνωσίας του. Για παράδειγμα, έχει διαπιστωθεί ότι τα chatbots παραποιούν την πολυπλοκότητα των θεμάτων που σχετίζονται με την υγεία, υποδηλώνοντας αβεβαιότητα εκεί που δεν υπάρχει, γεγονός που παραπλάνησε τους χρήστες και τους έκανε να πιστέψουν ότι μη τεκμηριωμένες θεραπείες ήταν ακόμη υπό εξέταση.
  (Σύνδεσμος αναφοράς: [KFF](https://www.kff.org/health-misinformation-monitor/volume-05/))

#### 4. Μη ασφαλής παραγωγή κώδικα

  Το μοντέλο υποδεικνύει μη ασφαλείς ή ανύπαρκτες βιβλιοθήκες κώδικα, οι οποίες μπορούν να εισάγουν ευπάθειες όταν ενσωματώνονται σε συστήματα λογισμικού. Για παράδειγμα, τα LLM προτείνουν τη χρήση μη ασφαλών βιβλιοθηκών τρίτων, οι οποίες, αν γίνουν αντικείμενο εμπιστοσύνης χωρίς επαλήθευση, οδηγούν σε κινδύνους ασφάλειας.
  (Σύνδεσμος αναφοράς: [Lasso](https://www.lasso.security/blog/ai-package-hallucinations))

### Στρατηγικές Πρόληψης και Αντιμετώπισης

#### 1. Παραγωγή επαυξημένης ανάκτησης (RAG)

  Χρησιμοποιήστε την παραγωγή επαυξημένης ανάκτησης για να ενισχύσετε την αξιοπιστία των αποτελεσμάτων του μοντέλου, ανακτώντας σχετικές και επαληθευμένες πληροφορίες από αξιόπιστες εξωτερικές βάσεις δεδομένων κατά τη διάρκεια της δημιουργίας των απαντήσεων. Αυτό συμβάλλει στον μετριασμό του κινδύνου ψευδαισθήσεων και παραπληροφόρησης.

#### 2. Βελτιστοποίηση μοντέλου

  Βελτιώστε το μοντέλο με μικρορύθμιση ή ενσωμάτωση για να βελτιώσετε την ποιότητα εξόδου. Τεχνικές όπως ο αποδοτικός συντονισμός παραμέτρων (PET) και η προτροπή αλυσίδας σκέψης μπορούν να βοηθήσουν στη μείωση της συχνότητας παραπληροφόρησης.

#### 3. Διασταυρούμενη επαλήθευση και ανθρώπινη εποπτεία

  Ενθαρρύνετε τους χρήστες να διασταυρώνουν τα αποτελέσματα του LLM με αξιόπιστες εξωτερικές πηγές για να διασφαλίσουν την ακρίβεια των πληροφοριών. Εφαρμόστε διαδικασίες ανθρώπινης επίβλεψης και ελέγχου των γεγονότων, ιδίως για κρίσιμες ή ευαίσθητες πληροφορίες. Βεβαιωθείτε ότι οι αξιολογητές είναι κατάλληλα εκπαιδευμένοι ώστε να αποφεύγεται η υπερβολική εξάρτηση από το περιεχόμενο που παράγεται με τεχνητή νοημοσύνη.

#### 4. Μηχανισμοί αυτόματης επικύρωσης

  Implement tools and processes to automatically validate key outputs, especially output from high-stakes environments.

#### 5. Γνωστοποίηση κινδύνου

  Προσδιορίστε τους κινδύνους και τις πιθανές βλάβες που σχετίζονται με το περιεχόμενο που παράγεται από το LLM και, στη συνέχεια, γνωστοποιήστε με σαφήνεια αυτούς τους κινδύνους και τους περιορισμούς στους χρήστες, συμπεριλαμβανομένης της πιθανότητας παραπληροφόρησης.

#### 6. Πρακτικές ασφαλούς προγραμματισμού

  Καθιέρωση πρακτικών ασφαλούς προγραμματισμού για την αποτροπή της ενσωμάτωσης ευπαθειών λόγω λανθασμένων προτάσεων κώδικα.

#### 7. Σχεδιασμός διεπαφής χρήστη

  Σχεδιάστε API και διεπαφές χρήστη που ενθαρρύνουν την υπεύθυνη χρήση των LLM, όπως η ενσωμάτωση φίλτρων περιεχομένου, η σαφής επισήμανση του περιεχομένου που παράγεται από τεχνητή νοημοσύνη και η ενημέρωση των χρηστών σχετικά με τους περιορισμούς αξιοπιστίας και ακρίβειας. Να είστε συγκεκριμένοι σχετικά με τους προβλεπόμενους περιορισμούς του πεδίου χρήσης.

#### 8. Εκπαίδευση και κατάρτιση

  Παροχή ολοκληρωμένης κατάρτισης στους χρήστες σχετικά με τους περιορισμούς των LLM, τη σημασία της ανεξάρτητης επαλήθευσης του παραγόμενου περιεχομένου και την ανάγκη για κριτική σκέψη. Σε συγκεκριμένα πλαίσια, να προσφέρετε εκπαίδευση για συγκεκριμένους τομείς, ώστε να διασφαλιστεί ότι οι χρήστες μπορούν να αξιολογούν αποτελεσματικά τα αποτελέσματα των LLM εντός του τομέα της ειδικότητάς τους.

### Παραδείγματα Σεναρίων Επίθεσης

#### Σενάριο #1

  Οι επιτιθέμενοι πειραματίζονται με δημοφιλείς βοηθούς προγραμματισμού για να βρουν συνήθη ψευδεπίγραφα ονόματα πακέτων. Μόλις εντοπίσουν αυτές τις συχνά προτεινόμενες αλλά ανύπαρκτες βιβλιοθήκες, δημοσιεύουν κακόβουλα πακέτα με αυτά τα ονόματα σε ευρέως χρησιμοποιούμενα αποθετήρια. Οι προγραμματιστές, βασιζόμενοι στις προτάσεις του βοηθού προγραμματισμού, ενσωματώνουν εν αγνοία τους αυτά τα δηλητηριασμένα πακέτα στο λογισμικό τους. Ως αποτέλεσμα, οι επιτιθέμενοι αποκτούν μη εξουσιοδοτημένη πρόσβαση, εισάγουν κακόβουλο κώδικα ή δημιουργούν κερκόπορτες, οδηγώντας σε σημαντικές παραβιάσεις ασφαλείας και θέτοντας σε κίνδυνο τα δεδομένα των χρηστών.

#### Σενάριο #2

  Μια εταιρεία παρέχει ένα chatbot για ιατρική διάγνωση χωρίς να διασφαλίζει επαρκή ακρίβεια. Το chatbot παρέχει ελλιπείς πληροφορίες, οδηγώντας σε επιβλαβείς συνέπειες για τους ασθενείς. Ως αποτέλεσμα, η εταιρεία μηνύεται επιτυχώς για αποζημίωση. Σε αυτή την περίπτωση, η κατάρρευση της ασφάλειας και της προστασίας δεν απαιτούσε κακόβουλο εισβολέα, αλλά προέκυψε από την ανεπαρκή εποπτεία και αξιοπιστία του συστήματος LLM. Σε αυτό το σενάριο, δεν απαιτείται να υπάρξει πραγματικός επιτιθέμενος για να κινδυνεύσει η εταιρεία από ζημία φήμης και οικονομική ζημία.

### Σύνδεσμοι Αναφοράς

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/): **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know): **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/): **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations): **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655): **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/): **Microsoft**

### Σχετικά πλαίσια και ταξινομήσεις

Ανατρέξτε σε αυτή την ενότητα για αναλυτικές πληροφορίες, στρατηγικές σεναρίων σχετικά με την ανάπτυξη υποδομών, εφαρμοσμένους ελέγχους περιβάλλοντος και άλλες βέλτιστες πρακτικές.

- [AML.T0048.002 - Societal Harm](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
