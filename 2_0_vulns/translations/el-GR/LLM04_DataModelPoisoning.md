## LLM04:2025 Δηλητηρίαση Μοντέλου  και Δεδομένων (Data and Model Poisoning)

### Περιγραφή

Η δηλητηρίαση δεδομένων συμβαίνει όταν τα δεδομένα προ-εκπαίδευσης, βελτιστοποίησης ή ενσωμάτωσης χειραγωγούνται για την εισαγωγή ευπαθειών, κερκόπορτας ή προκαταλήψεων. Αυτή η χειραγώγηση μπορεί να θέσει σε κίνδυνο την ασφάλεια του μοντέλου, τις επιδόσεις ή την ηθική συμπεριφορά, οδηγώντας σε επιβλαβείς εξόδους ή μειωμένες δυνατότητες. Οι συνήθεις κίνδυνοι περιλαμβάνουν υποβαθμισμένες επιδόσεις μοντέλων, μεροληπτικό ή τοξικό περιεχόμενο και εκμετάλλευση δευτερευόντων συστημάτων.

Η δηλητηρίαση δεδομένων μπορεί να στοχεύει σε διάφορα στάδια του κύκλου ζωής του LLM, συμπεριλαμβανομένης της προ-εκπαίδευσης (μάθηση από γενικά δεδομένα), της βελτιστοποίησης (προσαρμογή των μοντέλων σε συγκεκριμένες εργασίες) και της ενσωμάτωσης (μετατροπή κειμένου σε αριθμητικά διανύσματα). Η κατανόηση αυτών των σταδίων βοηθά στον εντοπισμό της προέλευσης των τρωτών σημείων. Η δηλητηρίαση δεδομένων θεωρείται επίθεση ακεραιότητας, καθώς η αλλοίωση των δεδομένων εκπαίδευσης επηρεάζει την ικανότητα του μοντέλου να κάνει ακριβείς προβλέψεις. Οι κίνδυνοι είναι ιδιαίτερα υψηλοί με εξωτερικές πηγές δεδομένων, οι οποίες μπορεί να περιέχουν μη επαληθευμένο ή κακόβουλο περιεχόμενο.

Επιπλέον, τα μοντέλα που διανέμονται μέσω κοινόχρηστων αποθετηρίων ή πλατφορμών ανοικτού κώδικα μπορεί να ενέχουν κινδύνους πέραν της δηλητηρίασης δεδομένων, όπως κακόβουλο λογισμικό που ενσωματώνεται μέσω τεχνικών όπως το κακόβουλο «Pickling», το οποίο μπορεί να εκτελέσει επιβλαβή κώδικα κατά τη φόρτωση του μοντέλου. Επίσης, λάβετε υπόψη ότι η δηλητηρίαση μπορεί να επιτρέψει την υλοποίηση μιας κερκόπορτας. Τέτοιες κερκόπορτες μπορεί να αφήνουν τη συμπεριφορά του μοντέλου ανέγγιχτη έως ότου ένα συγκεκριμένο έναυσμα προκαλέσει την αλλαγή της. Αυτό μπορεί να δυσχεράνει τον έλεγχο και την ανίχνευση τέτοιων αλλαγών, δημιουργώντας στην πραγματικότητα την ευκαιρία για ένα μοντέλο να γίνει πράκτορας εν υπνώσει.

### Συνήθη Παραδείγματα Ευπάθειας

1. Κακόβουλοι δρώντες εισάγουν επιβλαβή δεδομένα κατά τη διάρκεια της εκπαίδευσης, οδηγώντας σε μεροληπτικές εξόδους. Τεχνικές όπως το «Split-View Data Poisoning» ή το «Frontrunning Poisoning» εκμεταλλεύονται τη δυναμική εκπαίδευσης του μοντέλου για να το επιτύχουν αυτό.
  (Σύνδεσμος Αναφοράς: [Split-View Data Poisoning](https://github.com/GangGreenTemperTatum/speaking/blob/main/dc604/hacker-summer-camp-23/Ads%20_%20Poisoning%20Web%20Training%20Datasets%20_%20Flow%20Diagram%20-%20Exploit%201%20Split-View%20Data%20Poisoning.jpeg))
  (Σύνδεσμος Αναφοράς: [Frontrunning Poisoning](https://github.com/GangGreenTemperTatum/speaking/blob/main/dc604/hacker-summer-camp-23/Ads%20_%20Poisoning%20Web%20Training%20Datasets%20_%20Flow%20Diagram%20-%20Exploit%202%20Frontrunning%20Data%20Poisoning.jpeg))
2. Οι επιτιθέμενοι μπορούν να εισάγουν επιβλαβές περιεχόμενο απευθείας στη διαδικασία εκπαίδευσης, θέτοντας σε κίνδυνο την ποιότητα των αποτελεσμάτων του μοντέλου.
3. Οι χρήστες εισάγουν εν αγνοία τους ευαίσθητες ή ιδιόκτητες πληροφορίες κατά τη διάρκεια των αλληλεπιδράσεων, οι οποίες θα μπορούσαν να εκτεθούν σε επακόλουθες εξόδους.
4. Τα μη επαληθευμένα δεδομένα εκπαίδευσης αυξάνουν τον κίνδυνο μεροληπτικών ή εσφαλμένων αποτελεσμάτων.
5. Η έλλειψη περιορισμών πρόσβασης σε πόρους μπορεί να επιτρέψει την εισαγωγή μη ασφαλών δεδομένων, με αποτέλεσμα μεροληπτικά αποτελέσματα.

### Στρατηγικές Πρόληψης και Αντιμετώπισης

1. Παρακολουθήστε την προέλευση και τους μετασχηματισμούς δεδομένων χρησιμοποιώντας εργαλεία όπως το OWASP CycloneDX ή το ML-BOM και αξιοποιήστε εργαλεία όπως το Dyana για την εκτέλεση δυναμικής ανάλυσης λογισμικού τρίτων. Επαληθεύστε τη νομιμότητα των δεδομένων σε όλα τα στάδια ανάπτυξης μοντέλων.
(Σύνδεσμος Αναφοράς: [Dyana](https://github.com/dreadnode/dyana))
2. Ελέγξτε αυστηρά τους προμηθευτές δεδομένων και επικυρώστε τα αποτελέσματα του μοντέλου με αξιόπιστες πηγές για να εντοπίσετε σημάδια δηλητηρίασης.
3. Εφαρμόστε αυστηρό sandboxing για να περιορίσετε την έκθεση του μοντέλου σε μη επαληθευμένες πηγές δεδομένων. Χρησιμοποιήστε τεχνικές ανίχνευσης ανωμαλιών για να φιλτράρετε τα ανταγωνιστικά δεδομένα.
4. Προσαρμόστε τα μοντέλα για διαφορετικές περιπτώσεις χρήσης χρησιμοποιώντας συγκεκριμένα σύνολα δεδομένων για βελτιστοποίηση. Αυτό συμβάλλει στην παραγωγή ακριβέστερων αποτελεσμάτων με βάση τους καθορισμένους στόχους.
5. Εξασφαλίστε επαρκείς ελέγχους υποδομής για να αποτρέψετε την πρόσβαση του μοντέλου σε ανεπιθύμητες πηγές δεδομένων.
6. Χρησιμοποιήστε τον έλεγχο έκδοσης δεδομένων (DVC) για την παρακολούθηση των αλλαγών στα σύνολα δεδομένων και τον εντοπισμό χειραγώγησης. Η διαχείριση εκδόσεων είναι ζωτικής σημασίας για τη διατήρηση της ακεραιότητας του μοντέλου.
7. Αποθηκεύστε τις πληροφορίες που παρέχει ο χρήστης σε μια διανυσματική βάση δεδομένων, επιτρέποντας προσαρμογές χωρίς επανεκπαίδευση ολόκληρου του μοντέλου.
8. Δοκιμάστε την ανθεκτικότητα του μοντέλου με εκστρατείες της «ερυθράς ομάδας» και αντίπαλες τεχνικές, όπως η ομοσπονδιακή μάθηση, για να ελαχιστοποιήσετε τον αντίκτυπο των διαταραχών των δεδομένων.
9. Παρακολουθήστε την απώλεια εκπαίδευσης και αναλύστε τη συμπεριφορά του μοντέλου για ενδείξεις δηλητηρίασης. Χρησιμοποιήστε κατώτατα όρια για τον εντοπισμό αποκλίνουσας εξόδου.
10. Κατά την εξαγωγή συμπερασμάτων, ενσωματώστε τεχνικές Retrieval-Augmented Generation (RAG) και θεμελίωσης για να μειώσετε τους κινδύνους παραισθήσεων..

### Παραδείγματα Σεναρίων Επίθεσης

#### Σενάριο #1

  Ένας επιτιθέμενος μεροληπτεί στις εξόδους του μοντέλου χειραγωγώντας τα δεδομένα εκπαίδευσης ή χρησιμοποιώντας τεχνικές άμεσης έγχυσης, διαδίδοντας παραπληροφόρηση.

#### Σενάριο #2

  Τα τοξικά δεδομένα χωρίς κατάλληλο φιλτράρισμα μπορεί να οδηγήσουν σε επιβλαβείς ή προκατειλημμένες εξόδους, διαδίδοντας επικίνδυνες πληροφορίες.

#### Σενάριο # 3

  Ένας κακόβουλος δρών ή ανταγωνιστής δημιουργεί παραποιημένα έγγραφα για την εκπαίδευση, με αποτέλεσμα οι έξοδοι του μοντέλου να αντικατοπτρίζουν αυτές τις ανακρίβειες.

#### Σενάριο #4

  Το ανεπαρκές φιλτράρισμα επιτρέπει σε έναν εισβολέα να εισάγει παραπλανητικά δεδομένα μέσω έγχυσης προτροπής, οδηγώντας σε υπονομευμένες εξόδους.

#### Σενάριο #5

  Ένας επιτιθέμενος χρησιμοποιεί τεχνικές δηλητηρίασης για να εισάγει ένα ενεργοποιητή κερκόπορτας στο μοντέλο. Αυτό μπορεί να σας αφήσει εκτεθειμένους σε παράκαμψη ελέγχου ταυτότητας, διαρροή δεδομένων ή κρυφή εκτέλεση εντολών.

### Σύνδεσμοι Αναφοράς

1. [How data poisoning attacks corrupt machine learning models](https://www.csoonline.com/article/3613932/how-data-poisoning-attacks-corrupt-machine-learning-models.html): **CSO Online**
2. [MITRE ATLAS (framework) Tay Poisoning](https://atlas.mitre.org/studies/AML.CS0009/): **MITRE ATLAS**
3. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news/): **Mithril Security**
4. [Poisoning Language Models During Instruction](https://arxiv.org/abs/2305.00944): **Arxiv White Paper 2305.00944**
5. [Poisoning Web-Scale Training Datasets - Nicholas Carlini | Stanford MLSys #75](https://www.youtube.com/watch?v=h9jf1ikcGyk): **Stanford MLSys Seminars YouTube Video**
6. [ML Model Repositories: The Next Big Supply Chain Attack Target](https://www.darkreading.com/cloud-security/ml-model-repositories-next-big-supply-chain-attack-target) **OffSecML**
7. [Data Scientists Targeted by Malicious Hugging Face ML Models with Silent Backdoor](https://jfrog.com/blog/data-scientists-targeted-by-malicious-hugging-face-ml-models-with-silent-backdoor/) **JFrog**
8. [Backdoor Attacks on Language Models](https://towardsdatascience.com/backdoor-attacks-on-language-models-can-we-trust-our-models-weights-73108f9dcb1f): **Towards Data Science**
9. [Never a dill moment: Exploiting machine learning pickle files](https://blog.trailofbits.com/2021/03/15/never-a-dill-moment-exploiting-machine-learning-pickle-files/) **TrailofBits**
10. [arXiv:2401.05566 Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://www.anthropic.com/news/sleeper-agents-training-deceptive-llms-that-persist-through-safety-training) **Anthropic (arXiv)**
11. [Backdoor Attacks on AI Models](https://www.cobalt.io/blog/backdoor-attacks-on-ai-models) **Cobalt**

### Σχετικά Πλαίσια και Ταξινομήσεις

Ανατρέξτε σε αυτή την ενότητα για αναλυτικές πληροφορίες, στρατηγικές σεναρίων σχετικά με την ανάπτυξη υποδομών, εφαρμοσμένους περιβαλλοντικούς ελέγχους και άλλες βέλτιστες πρακτικές.

- [AML.T0018 | Backdoor ML Model](https://atlas.mitre.org/techniques/AML.T0018) **MITRE ATLAS**
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework): Strategies for ensuring AI integrity. **NIST**
