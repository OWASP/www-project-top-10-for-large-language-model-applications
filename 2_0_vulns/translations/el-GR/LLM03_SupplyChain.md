## LLM03:2025 Εφοδιαστική Αλυσίδα  (Supply Chain)

### Περιγραφή

Οι αλυσίδες εφοδιασμού LLM είναι ευάλωτες σε διάφορες ευπάθειες, οι οποίες μπορούν να επηρεάσουν την ακεραιότητα των δεδομένων εκπαίδευσης, τα μοντέλα και τις πλατφόρμες ανάπτυξης. Αυτοί οι κίνδυνοι μπορεί να οδηγήσουν σε μεροληπτικές εξόδους, παραβιάσεις της ασφάλειας ή αποτυχίες του συστήματος. Ενώ οι παραδοσιακές ευπάθειες λογισμικού επικεντρώνονται σε θέματα όπως τα ελαττώματα κώδικα και οι εξαρτήσεις, στη μηχανική μάθηση οι κίνδυνοι επεκτείνονται και σε προ-εκπαιδευμένα μοντέλα και δεδομένα τρίτων.

Αυτά τα εξωτερικά στοιχεία μπορούν να αλλοιωθούν μέσω επιθέσεων παραποίησης ή «poisoning» (δηλητηρίασης).

Η δημιουργία LLM είναι μια εξειδικευμένη εργασία που συχνά εξαρτάται από μοντέλα τρίτων. Η άνοδος των LLM ανοικτής πρόσβασης και των νέων μεθόδων βελτιστοποίησης όπως η «LoRA» (Low-Rank Adaptation) και η «PEFT» (Parameter-Efficient Fine-Tuning), ιδίως σε πλατφόρμες όπως η Hugging Face, εισάγουν νέους κινδύνους για την εφοδιαστική αλυσίδα. Τέλος, η εμφάνιση των LLM στη συσκευή αυξάνει την επιφάνεια επίθεσης και τους κινδύνους της εφοδιαστικής αλυσίδας για τις εφαρμογές LLM.

Ορισμένοι από τους κινδύνους που εξετάζονται εδώ εξετάζονται επίσης στην ενότητα «LLM04 Δηλητηρίαση δεδομένων και μοντέλων». Το παρόν λήμμα επικεντρώνεται στην πτυχή των κινδύνων της αλυσίδας εφοδιασμού.
Ένα απλό μοντέλο απειλής μπορεί να βρεθεί [εδώ](https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png).

### Συνήθη Παραδείγματα Κινδύνων

#### 1. Παραδοσιακές Ευπάθειες Πακέτων Τρίτων

  Όπως ξεπερασμένα ή απαρχαιωμένα δομικά στοιχεία, τα οποία οι επιτιθέμενοι μπορούν να εκμεταλλευτούν για να θέσουν σε κίνδυνο εφαρμογές LLM. Αυτό είναι παρόμοιο με το «A06:2021 - Vulnerable and Outdated Components» με αυξημένους κινδύνους όταν τα δομικά στοιχεία χρησιμοποιούνται κατά τη διάρκεια της ανάπτυξης μοντέλων ή της βελτιστοποίησης.
  (Σύνδεσμος αναφοράς: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))

#### 2. Κίνδυνοι Αδειοδότησης

  Η ανάπτυξη της τεχνητής νοημοσύνης συχνά περιλαμβάνει ποικίλες άδειες χρήσης λογισμικού και συνόλων δεδομένων, γεγονός που δημιουργεί κινδύνους εάν δεν γίνεται σωστή διαχείριση. Διαφορετικές άδειες ανοικτού κώδικα και ιδιόκτητες άδειες επιβάλλουν διαφορετικές νομικές απαιτήσεις. Οι άδειες χρήσης συνόλων δεδομένων μπορεί να περιορίζουν τη χρήση, τη διανομή ή την εμπορική εκμετάλλευση.

#### 3. Παρωχημένα ή Απαξιωμένα Μοντέλα

  Η χρήση παρωχημένων ή απαξιωμένων μοντέλων που δεν συντηρούνται πλέον οδηγεί σε θέματα ασφάλειας.

#### 4. Ευάλωτο Προεκπαιδευμένο Μοντέλο

  Τα μοντέλα είναι δυαδικά μαύρα κουτιά και σε αντίθεση με τον ανοιχτό κώδικα, η στατική επιθεώρηση μπορεί να προσφέρει ελάχιστες εγγυήσεις ασφαλείας. Τα ευάλωτα προ-εκπαιδευμένα μοντέλα μπορεί να περιέχουν κρυφές προκαταλήψεις, κερκόπορτες ή άλλα κακόβουλα χαρακτηριστικά που δεν έχουν εντοπιστεί μέσω των αξιολογήσεων ασφαλείας του αποθετηρίου μοντέλων. Τα ευάλωτα μοντέλα μπορούν να δημιουργηθούν τόσο από δηλητηριασμένα σύνολα δεδομένων όσο και από την άμεση αλλοίωση του μοντέλου με τη χρήση τεχνικών όπως η ROME, γνωστή και ως «λοβοτομή».

#### 5. Αδύναμη Διακρίβωση Μοντέλου

  Επί του παρόντος δεν υπάρχουν ισχυρές διασφαλίσεις προέλευσης σε δημοσιευμένα μοντέλα. Οι κάρτες μοντέλων και η σχετική τεκμηρίωση παρέχουν πληροφορίες για το μοντέλο και βασίζονται στους χρήστες, αλλά δεν προσφέρουν εγγυήσεις σχετικά με την προέλευση του μοντέλου. Ένας επιτιθέμενος μπορεί να παραβιάσει τον λογαριασμό προμηθευτή σε ένα αποθετήριο μοντέλων ή να δημιουργήσει έναν παρόμοιο και να τον συνδυάσει με τεχνικές κοινωνικής μηχανικής για να παραβιάσει την αλυσίδα εφοδιασμού μιας εφαρμογής LLM.

#### 6. Ευάλωτοι Προσαρμογείς LoRA

  Η LoRA είναι μια δημοφιλής τεχνική βελτιστοποίησης που ενισχύει την σπονδυλωτή δομή, επιτρέποντας την προσθήκη προ-εκπαιδευμένων στρωμάτων σε ένα υπάρχον LLM. Η μέθοδος αυξάνει την αποδοτικότητα, αλλά δημιουργεί νέους κινδύνους, όπου ένας κακόβουλος προσαρμογέας LorA θέτει σε κίνδυνο την ακεραιότητα και την ασφάλεια του προ-εκπαιδευμένου βασικού μοντέλου. Αυτό μπορεί να συμβεί τόσο σε συνεργατικά περιβάλλοντα συγχώνευσης μοντέλων, αλλά και αξιοποιώντας την υποστήριξη του LoRA από δημοφιλείς πλατφόρμες ανάπτυξης συμπερασμάτων, όπως το vLMM και το OpenLLM, όπου οι προσαρμογείς μπορούν να μεταφορτωθούν και να εφαρμοστούν σε ένα υλοποιημένο μοντέλο.

#### 7. Αξιοποίηση Συνεργατικών Διαδικασιών Ανάπτυξης

  Η συνεργατική συγχώνευση μοντέλων και οι υπηρεσίες επεξεργασίας μοντέλων (π.χ. μετατροπές) που φιλοξενούνται σε κοινόχρηστα περιβάλλοντα μπορούν να αξιοποιηθούν για την εισαγωγή ευπαθειών σε κοινόχρηστα μοντέλα. Η συγχώνευση μοντέλων είναι πολύ δημοφιλής στο Hugging Face με τα συγχωνευμένα μοντέλα να βρίσκονται στην κορυφή του πίνακα κατάταξης του OpenLLM και μπορεί να αξιοποιηθεί για την παράκαμψη των επιθεωρήσεων. Ομοίως, υπηρεσίες όπως το ρομπότ συνομιλίας έχει αποδειχθεί ότι είναι ευάλωτες στη χειραγώγηση και εισάγουν κακόβουλο κώδικα στα μοντέλα.

#### 8. Τρωτότητες Αλυσίδας Εφοδιασμού Μοντέλων LLM επί Συσκευών

  Τα μοντέλα LLM επί της συσκευής αυξάνουν την επιφάνεια επίθεσης αλυσίδας εφοδιασμού με παραβιασμένες κατασκευασμένες διεργασίες και την εκμετάλλευση ευπαθειών του λειτουργικού συστήματος της συσκευής ή του υλικολογισμικού για να θέσουν σε κίνδυνο τα μοντέλα. Οι επιτιθέμενοι μπορούν να πραγματοποιήσουν αντίστροφη μηχανική και να ανακατασκευάσουν εφαρμογές με αλλοιωμένα μοντέλα.

#### 9. Ασαφείς Όροι και Προϋποθέσεις και Πολιτικές Απορρήτου Δεδομένων

  Οι ασαφείς όροι και πολιτικές απορρήτου των φορέων εκμετάλλευσης των μοντέλων οδηγούν στη χρήση ευαίσθητων δεδομένων της εφαρμογής για την εκπαίδευση των μοντέλων και στην επακόλουθη έκθεση ευαίσθητων πληροφοριών. Αυτό μπορεί επίσης να ισχύει για τους κινδύνους από τη χρήση υλικού που προστατεύεται από πνευματικά δικαιώματα από τον προμηθευτή του μοντέλου.

### Στρατηγικές Πρόληψης και Αντιμετώπισης

1. Ελέγξτε προσεκτικά τις πηγές και τους προμηθευτές δεδομένων, συμπεριλαμβανομένων των όρων και προϋποθέσεων και των πολιτικών απορρήτου τους, χρησιμοποιώντας μόνο αξιόπιστους προμηθευτές. Να επανεξετάζετε και να ελέγχετε τακτικά την ασφάλεια και την πρόσβαση των προμηθευτών, διασφαλίζοντας ότι δεν υπάρχουν αλλαγές στη θεώρηση ασφάλειάς τους ή στους όρους και τις προϋποθέσεις χρήσης.
2. Κατανοήστε και εφαρμόστε τα μέτρα αντιμετώπισης που περιέχονται στο «A06:2021 - Vulnerable and Outdated Components» του OWASP Τοπ 10. Αυτό περιλαμβάνει τη σάρωση ευπαθειών, τη διαχείριση και την επιδιόρθωση στοιχείων. Για περιβάλλοντα ανάπτυξης με πρόσβαση σε ευαίσθητα δεδομένα, εφαρμόστε αυτούς τους ελέγχους και σε αυτά τα περιβάλλοντα.
  (Σύνδεσμος Αναφοράς: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
3. Εφαρμόστε ολοκληρωμένο AI Red Teaming και αξιολογήσεις κατά την επιλογή ενός μοντέλου τρίτου μέρους. Η αποκωδικοποίηση εμπιστοσύνης είναι ένα παράδειγμα ενός αξιόπιστου δείκτη αναφοράς AI για LLM, αλλά τα μοντέλα μπορούν να ρυθμιστούν ώστε να περάσουν τα δημοσιευμένα κριτήρια αναφοράς. Χρησιμοποιήστε εκτεταμένο AI Red Teaming για την αξιολόγηση του μοντέλου, ιδίως στις περιπτώσεις χρήσης για τις οποίες σχεδιάζετε να χρησιμοποιήσετε το μοντέλο.
4. Διατηρήστε μια ενημερωμένη καταγραφή των συστατικών με τη χρήση ενός Κατάλογο Υλικών Λογισμικού (SBOM) για να διασφαλίσετε ότι έχετε μια ενημερωμένη, ακριβή και πιστοποιημένη καταγραφή, αποτρέποντας την αλλοίωση των πακέτων που αναπτύσσονται. Οι SBOM μπορούν να χρησιμοποιηθούν για τον γρήγορο εντοπισμό και προειδοποίηση για νέες, ευπάθειες «ημέρας μηδέν». Τα AI BOMs και τα ML SBOMs είναι ένας αναδυόμενος τομέας και θα πρέπει να αξιολογήσετε τις επιλογές ξεκινώντας από το OWASP CycloneDX.
5. Για να μετριάσετε τους κινδύνους αδειοδότησης ΤΝ, δημιουργήστε έναν κατάλογο όλων των τύπων αδειών που εμπλέκονται με τη χρήση BOM και διεξάγετε τακτικούς ελέγχους σε όλο το λογισμικό, τα εργαλεία και τα σύνολα δεδομένων, διασφαλίζοντας τη συμμόρφωση και τη διαφάνεια μέσω των BOM. Χρησιμοποιήστε αυτοματοποιημένα εργαλεία διαχείρισης αδειών για παρακολούθηση σε πραγματικό χρόνο και εκπαιδεύστε τις ομάδες στα μοντέλα αδειοδότησης. Διατηρείτε λεπτομερή τεκμηρίωση αδειοδότησης σε BOMs και αξιοποιείτε εργαλεία όπως το Dyana για την εκτέλεση δυναμικής ανάλυσης λογισμικού τρίτων.
(Σύνδεσμος Αναφοράς: [Dyana](https://github.com/dreadnode/dyana))
6. Χρησιμοποιήστε μόνο μοντέλα από επαληθεύσιμες πηγές και χρησιμοποιήστε ελέγχους ακεραιότητας μοντέλων τρίτων με υπογραφή και κατακερματισμούς αρχείων για να αντισταθμίσετε την έλλειψη ισχυρής διασφάλισης της προέλευσης των μοντέλων. Ομοίως, χρησιμοποιήστε υπογραφή κώδικα για κώδικα που παρέχεται εξωτερικά.
7. Εφαρμόστε αυστηρές πρακτικές παρακολούθησης και ελέγχου για συνεργατικά περιβάλλοντα ανάπτυξης μοντέλων για την πρόληψη και τον γρήγορο εντοπισμό οποιασδήποτε κατάχρησης. Το «HuggingFace SF_Convertbot Scanner» είναι ένα παράδειγμα αυτοματοποιημένων σεναρίων προς χρήση.
  (Σύνδεσμος Αναφοράς: [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163))
8. Η ανίχνευση ανωμαλιών και οι ανταγωνιστικές δοκιμές ανθεκτικότητας σε παρεχόμενα μοντέλα και δεδομένα μπορούν να βοηθήσουν στον εντοπισμό παραποίησης και δηλητηρίασης, όπως συζητείται στο «LLM04 Data and Model Poisoning»- ιδανικά, αυτό θα πρέπει να αποτελεί μέρος των διαδικασιών MLOps και LLM- ωστόσο, αυτές είναι αναδυόμενες τεχνικές και μπορεί να είναι ευκολότερο να εφαρμοστούν ως μέρος ασκήσεων ερυθράς ομάδας.
9. Εφαρμόστε μια πολιτική επιδιορθώσεων για τον μετριασμό των ευάλωτων ή ξεπερασμένων στοιχείων. Βεβαιωθείτε ότι η εφαρμογή βασίζεται σε μια συντηρημένη έκδοση των API και του υποκείμενου μοντέλου.
10. Κρυπτογραφήστε τα μοντέλα που αναπτύσσονται στην πλευρά της ΤΝ με ελέγχους ακεραιότητας και χρησιμοποιήστε API πιστοποίησης του προμηθευτή για να αποτρέψετε τις παραποιημένες εφαρμογές και τα μοντέλα και να τερματίσετε τις εφαρμογές μη αναγνωρισμένου υλικολογισμικού.

### Ενδεικτικά Σενάρια Επίθεσης

#### Σενάριο #1: Ευάλωτη Βιβλιοθήκη Python

  Ένας επιτιθέμενος εκμεταλλεύεται μια ευάλωτη βιβλιοθήκη Python για να θέσει σε κίνδυνο μια εφαρμογή LLM. Αυτό συνέβη στην πρώτη παραβίαση δεδομένων του Open AI.  Οι επιθέσεις στο μητρώο πακέτων PyPi εξαπάτησαν τους προγραμματιστές μοντέλων να κατεβάσουν μια παραβιασμένη εξάρτηση PyTorch με κακόβουλο λογισμικό σε ένα περιβάλλον ανάπτυξης μοντέλων.  Ένα πιο εξελιγμένο παράδειγμα αυτού του τύπου επίθεσης είναι η επίθεση Shadow Ray στο πλαίσιο Ray AI που χρησιμοποιείται από πολλούς προμηθευτές για τη διαχείριση της υποδομής AI.  Σε αυτή την επίθεση, πέντε ευπάθειες πιστεύεται ότι αξιοποιήθηκαν ευρέως και επηρέασαν πολλούς διακομιστές.

#### Σενάριο #2: Άμεση Παραβίαση

  Άμεση παραβίαση και δημοσίευση ενός μοντέλου για τη διάδοση παραπληροφόρησης. Αυτή είναι μια πραγματική επίθεση με το PoisonGPT να παρακάμπτει τα χαρακτηριστικά ασφαλείας του Hugging Face αλλάζοντας άμεσα τις παραμέτρους του μοντέλου.

#### Σενάριο #3: Προσαρμογή Δημοφιλούς Μοντέλου

  Ένας επιτιθέμενος ρυθμίζει ένα δημοφιλές μοντέλο ανοικτής πρόσβασης για να αφαιρέσει βασικά χαρακτηριστικά ασφαλείας και να επιτύχει υψηλές επιδόσεις σε έναν συγκεκριμένο τομέα (ασφάλιση). Το μοντέλο είναι ρυθμισμένο έτσι ώστε να σημειώνει υψηλή βαθμολογία στα κριτήρια ασφαλείας, αλλά έχει πολύ στοχευμένες ενεργοποιήσεις. Το αναπτύσσουν στο Hugging Face για να το χρησιμοποιήσουν τα θύματα εκμεταλλευόμενοι την εμπιστοσύνη τους στις διαβεβαιώσεις αναφοράς.

#### Σενάριο #4: Προ-εκπαιδευμένα μοντέλα

  Ένα σύστημα LLM αναπτύσσει προ-εκπαιδευμένα μοντέλα από ένα ευρέως χρησιμοποιούμενο αποθετήριο χωρίς ενδελεχή επαλήθευση. Ένα παραβιασμένο μοντέλο εισάγει κακόβουλο κώδικα, προκαλώντας μεροληπτικές εξόδους σε ορισμένα πλαίσια και οδηγώντας σε επιβλαβή ή χειραγωγημένα αποτελέσματα

#### Σενάριο #5: Υπονομευμένος Προμηθευτής Τρίτου Μέρους

  Ένας εκτεθειμένος τρίτος προμηθευτής παρέχει έναν ευάλωτο προσαρμογέα LorA που συγχωνεύεται σε ένα LLM χρησιμοποιώντας συγχώνευση μοντέλων στο Hugging Face.

#### Σενάριο #6: Παρείσφρηση Προμηθευτή

  Ένας εισβολέας διεισδύει σε έναν προμηθευτή και θέτει σε κίνδυνο την παραγωγή ενός προσαρμογέα LoRA (Low-Rank Adaptation) που προορίζεται για ενσωμάτωση με ένα LLM σε συσκευή που αναπτύσσεται χρησιμοποιώντας πλαίσια όπως το vLLM ή το OpenLLM. Ο παραβιασμένος προσαρμογέας LoRA τροποποιείται διακριτικά ώστε να περιλαμβάνει κρυφές ευπάθειες και κακόβουλο κώδικα. Μόλις αυτός ο προσαρμογέας συγχωνευτεί με το LLM, παρέχει στον εισβολέα ένα κρυφό σημείο εισόδου στο σύστημα. Ο κακόβουλος κώδικας μπορεί να ενεργοποιηθεί κατά τη διάρκεια των λειτουργιών του μοντέλου, επιτρέποντας στον εισβολέα να χειραγωγήσει τις εξόδους του LLM.

#### Σενάριο #7: Επιθέσεις CloudBorne και CloudJacking

  Αυτές οι επιθέσεις στοχεύουν σε υποδομές νέφους, αξιοποιώντας κοινόχρηστους πόρους και ευπάθειες στα επίπεδα εικονικοποίησης. Το CloudBorne περιλαμβάνει την εκμετάλλευση ευπαθειών υλικολογισμικού σε κοινόχρηστα περιβάλλοντα νέφους, θέτοντας σε κίνδυνο τους φυσικούς διακομιστές που φιλοξενούν εικονικές περιπτώσεις. Το CloudJacking αναφέρεται στον κακόβουλο έλεγχο ή την κακή χρήση των στιγμιοτύπων νέφους, οδηγώντας δυνητικά σε μη εξουσιοδοτημένη πρόσβαση σε κρίσιμες πλατφόρμες ανάπτυξης LLM. Και οι δύο επιθέσεις αντιπροσωπεύουν σημαντικούς κινδύνους για τις αλυσίδες εφοδιασμού που εξαρτώνται από μοντέλα μηχανικής μάθησης που βασίζονται στο υπολογιστικό νέφος, καθώς τα παραβιασμένα περιβάλλοντα θα μπορούσαν να εκθέσουν ευαίσθητα δεδομένα ή να διευκολύνουν περαιτέρω επιθέσεις.

#### Σενάριο #8: LeftOvers (CVE-2023-4969)

  Αξιοποίηση της ευπαθειας «LeftOvers» της τοπικής μνήμης της GPU για την ανάκτηση ευαίσθητων δεδομένων. Ένας επιτιθέμενος μπορεί να χρησιμοποιήσει αυτή την επίθεση για να διαρρεύσει ευαίσθητα δεδομένα σε διακομιστές παραγωγής και σταθμούς εργασίας ή φορητούς υπολογιστές ανάπτυξης.

#### Σενάριο #9: WizardLM

  Μετά την απόσυρση του μοντέλου WizardLM, ένας επιτιθέμενος εκμεταλλεύεται το ενδιαφέρον για αυτό το μοντέλο και δημοσιεύει μια ψεύτικη έκδοση του μοντέλου με το ίδιο όνομα, η οποία όμως περιέχει κακόβουλο λογισμικό και backdoors.

#### Σενάριο #10: Υπηρεσία Συγχώνευσης Μοντέλων/Μετατροπής Μορφότυπων

  Ένας επιτιθέμενος εξαπολύει επίθεση μέσω μιας υπηρεσίας συγχώνευσης ή μετατροπής μορφής μοντέλωνγια να θέσει σε κίνδυνο ένα δημόσια διαθέσιμο μοντέλο πρόσβασης για να εισάγει κακόβουλο λογισμικό. Αυτή είναι μια πραγματική επίθεση που δημοσιεύθηκε από τον προμηθευτή HiddenLayer.

#### Σενάριο #11: Αντίστροφη Μηχανικής Εφαρμογής για Κινητές Συσκευές

  Ένας εισβολέας πραγματοποιεί αντίστροφη μηχανική μιας εφαρμογής για κινητά για να αντικαταστήσει το μοντέλο με μια παραποιημένη έκδοση που οδηγεί τον χρήστη σε ιστότοπους εξαπάτησης. Οι χρήστες ενθαρρύνονται να κατεβάσουν απευθείας την εφαρμογή μέσω τεχνικών κοινωνικής μηχανικής. Πρόκειται για μια «πραγματική επίθεση στην προγνωστική Τεχνητή Νοημοσύνη» που επηρέασε 116 εφαρμογές του Google Play, συμπεριλαμβανομένων δημοφιλών εφαρμογών ασφαλείας και κρίσιμων για την ασφάλεια εφαρμογών που χρησιμοποιούνται σε αναγνώριση μετρητών, γονικό έλεγχο, έλεγχο ταυτότητας προσώπου και χρηματοοικονομικές υπηρεσίες.
  (Σύνδεσμος Αναφοράς: [real attack on predictive AI](https://arxiv.org/abs/2006.08131))

#### Σενάριο #12: Δηλητηρίαση Συνόλου Δεδομένων

  Ένας επιτιθέμενος δηλητηριάζει δημόσια διαθέσιμα σύνολα δεδομένων για να βοηθήσει στη δημιουργία μιας κερκόπορτας κατά τη βελτιστοποίηση των μοντέλων. Η κερκόπορτα ευνοεί διακριτικά συγκεκριμένες εταιρείες σε διαφορετικές αγορές.

#### Σενάριο #13: Όροι και Προϋποθέσεις και Πολιτική Απορρήτου

  Ένας φορέας διαχείρισης LLM αλλάζει τους όρους και την πολιτική απορρήτου του ώστε να απαιτεί ρητή εξαίρεση από τη χρήση δεδομένων εφαρμογών για την εκπαίδευση μοντέλων, οδηγώντας στην απομνημόνευση ευαίσθητων δεδομένων.

### Σύνδεσμοι Αναφοράς

1. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news)
2. [Large Language Models On-Device with MediaPipe and TensorFlow Lite](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)
3. [Hijacking Safetensors Conversion on Hugging Face](https://hiddenlayer.com/research/silent-sabotage/)
4. [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010)
5. [Using LoRA Adapters with vLLM](https://docs.vllm.ai/en/latest/models/lora.html)
6. [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553)
7. [Model Merging with PEFT](https://huggingface.co/blog/peft_merging)
8. [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
9. [Thousands of servers hacked due to insecurely deployed Ray AI framework](https://www.csoonline.com/article/2075540/thousands-of-servers-hacked-due-to-insecurely-deployed-ray-ai-framework.html)
10. [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)

### Σχετικά Πλαίσια και Ταξινομήσεις

Ανατρέξτε σε αυτή την ενότητα για αναλυτικές πληροφορίες, στρατηγικές σεναρίων σχετικά με την ανάπτυξη υποδομών, εφαρμοσμένους ελέγχους περιβάλλοντος και άλλες βέλτιστες πρακτικές.

- [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010) -  **MITRE ATLAS**
