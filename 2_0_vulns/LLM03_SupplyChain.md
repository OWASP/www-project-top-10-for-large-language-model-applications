## LLM03:2025 Уязвимость цепочки поставки

### Описание

Цепочки поставок LLM подвержены различным уязвимостям, которые могут повлиять на целостность учебных данных, моделей и платформ для развертывания. Эти риски могут привести к искажению результатов, нарушению безопасности или сбоям в работе системы. В то время как традиционные уязвимости программного обеспечения сосредоточены на таких проблемах, как дефекты кода и зависимости, в ML риски также распространяются на сторонние предварительно обученные модели и данные.

Этими внешними элементами можно манипулировать с помощью атак с применением подмены и заражения данных.

Создание LLM - специализированная задача, которая часто зависит от сторонних моделей. Появление LLM в открытом доступе и новых методов тонкой настройки, таких как «LoRA» (Low-Rank Adaptation) и «PEFT» (Parameter-Efficient Fine-Tuning), особенно на таких платформах, как Hugging Face, создает новые риски для цепочки поставок. Наконец, появление LLM на устройствах увеличивает область атак и риски вмешательства в цепочки поставок для LLM-приложений.

Некоторые из обсуждаемых здесь рисков также рассматриваются в записи "LLM04 Отравление данных и модели". В данной записи основное внимание уделяется рискам, связанным с цепочками поставок.
Простую модель угроз можно найти [здесь](https://github.com/jsotiro/ThreatModels/blob/main/LLM%20Threats-LLM%20Supply%20Chain.png).

### Распространенные примеры рисков

#### 1. Традиционные уязвимости пакетов сторонних разработчиков
  Например, устаревшие или неактуальные компоненты, которые злоумышленники могут использовать для компрометации LLM-приложений. Это похоже на "A06:2021 – Vulnerable and Outdated Components" с повышенным риском, когда компоненты используются во время разработки или доработки модели.
  (Ссылка: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
#### 2. Лицензионные риски
  При разработке ИИ зачастую используются лицензии на программное обеспечение и наборы данных, что создает риски, если ими не управлять должным образом. Лицензии с открытым исходным кодом и проприетарные лицензии налагают различные юридические требования. Таким образом, лицензии на наборы данных могут ограничивать использование, распространение или коммерциализацию. 
#### 3. Устаревшие или не рекомендуемые модели
  Использование устаревших или нерекомендуемых моделей, которые больше не поддерживаются, приводит к проблемам безопасности.
#### 4. Уязвимые предварительно обученные модели
  Модели - это двоичные «черные ящики», и, в отличие от открытого исходного кода, статическая проверка может дать мало гарантий безопасности. Уязвимые предварительно обученные модели могут содержать скрытые предубеждения, бэкдоры или другие вредоносные функции, которые не были выявлены в ходе оценки безопасности репозитория моделей. Уязвимые модели могут быть созданы как с помощью отравленных наборов данных, так и путем прямой фальсификации моделей с помощью таких методов, как ROME, также известный как лоботомизация.
#### 5. Недостаточная уверенность в модели
  В настоящее время в опубликованных моделях нет надежных гарантий достоверности. Карточки моделей и сопутствующая документация предоставляют информацию о модели и полагаются на пользователей, но они не дают никаких гарантий происхождения модели. Злоумышленник может скомпрометировать учетную запись поставщика в репозитории моделей или создать аналогичную и, используя методы социальной инженерии, скомпрометировать цепочку поставок LLM-приложения.
#### 6. Уязвимые адаптеры LoRA
  LoRA - это популярная техника тонкой настройки, которая повышает модульность, позволяя добавлять предварительно обученные слои к существующей LLM. Этот метод повышает эффективность, но создает новые риски, когда злонамеренный адаптер LoRA нарушает целостность и безопасность предварительно обученной базовой модели. Это может произойти как в среде совместного объединения моделей, так и при использовании поддержки LoRA в популярных платформах для развертывания выводов, таких как vLMM и OpenLLM, где адаптеры могут быть загружены и применены к развернутой модели.
#### 7. Использование процессов совместной разработки
  Совместное объединение моделей и сервисы обработки моделей (например, преобразования), размещенные в общих средах, могут быть использованы для внедрения уязвимостей в общие модели. Слияние моделей очень популярно на Hugging Face: объединенные модели занимают первые места в рейтинге OpenLLM и могут быть использованы для обхода рецензий. Аналогично, было доказано, что такие сервисы, как talk bot, уязвимы для манипуляций и внедрения вредоносного кода в модели.
#### 8. Уязвимости цепочки поставок LLM-моделей на устройствах
  LLM-модели на устройствах увеличивают площадь атаки на цепочку поставок за счет компрометации производственных процессов и использования уязвимостей ОС устройства или встроенного ПО (firmware) для компрометации моделей. Злоумышленники могут проводить реверс-инжиниринг и переупаковывать приложения с поддельными моделями. 
#### 9. Неясные условия и положения и политика конфиденциальности данных
  Неясные условия и политика конфиденциальности данных операторов моделей приводят к использованию конфиденциальных данных приложения для обучения моделей и последующему раскрытию конфиденциальной информации. Это может также относиться к рискам, связанным с использованием материалов, защищенных авторским правом, поставщиком моделей.

### Стратегии предотвращения и смягчения последствий

1. Тщательно проверяйте источники данных и их поставщиков, включая условия использования и их политику конфиденциальности, а также используйте только проверенных поставщиков. Регулярно проверяйте и аудируйте безопасность и доступ поставщиков, не допуская изменений в их системе безопасности и правилах и условиях.
2. Понимание и применение мер защиты, описанных в документе OWASP Top Ten's "A06:2021 – Vulnerable and Outdated Components." Сюда входят компоненты сканирования уязвимостей, управления и исправления. В средах разработки с доступом к конфиденциальным данным применяйте эти средства контроля и в этих средах.
  (Ссылка: [A06:2021 – Vulnerable and Outdated Components](https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/))
3. При выборе сторонней модели применяйте комплексную проверку и оценку ИИ. Decoding Trust - это пример эталона ИИ, заслуживающего доверия, для LLM, но модели могут настраиваться таким образом, чтобы обойти опубликованные эталоны. Для оценки модели, особенно в тех случаях, для которых вы планируете использовать модель, используйте обширный AI Red Teaming. 
4. Поддерживайте актуальный перечень компонентов с использованием Software Bill of Materials (SBOM), чтобы обеспечить точность и актуальность информации, предотвращая вмешательство в развернутые пакеты. SBOM могут использоваться для быстрого обнаружения и уведомления о новых уязвимостях с нулевым днем. AI BOM и ML SBOM — развивающаяся область, и вам следует начать оценку вариантов с OWASP CycloneD.
5. Чтобы снизить риски лицензирования ИИ, создайте перечень всех типов лицензий с использованием спецификаций и проводите регулярный аудит всего программного обеспечения, инструментов и наборов данных, обеспечивая соответствие и прозрачность с помощью спецификаций. Используйте автоматизированные инструменты управления лицензиями для мониторинга в режиме реального времени и обучайте команды моделям лицензирования. Ведение подробной документации по лицензированию в спецификациях.
6. Используйте модели только из проверенных источников и применяйте сторонние проверки целостности моделей с помощью подписи и хэшей файлов, чтобы компенсировать отсутствие надежного происхождения моделей. Аналогично, используйте подпись кода для кода, поставляемого извне.
7. Внедрите строгие методы мониторинга и аудита для сред совместной разработки моделей, чтобы предотвратить и быстро обнаружить любые злоупотребления. «HuggingFace SF_Convertbot Scanner» - пример автоматизированных скриптов, которые можно использовать.
  (Ссылка: [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163))
8. Обнаружение аномалий и тесты на устойчивость моделей и данных, предоставляемых противником, могут помочь обнаружить фальсификацию и отравление, как обсуждается в "LLM04 Отравление данных и модели"; в идеале это должно быть частью конвейеров MLOps и LLM; однако это новые методы, и их может быть проще реализовать в рамках работы Red Team. Рекомендуем внедрить политику исправлений для снижения уязвимостей или устаревания компонентов. Убедитесь, что приложение опирается на поддерживаемую версию API и базовую модель.
10. Шифруйте модели, развернутые на AI edge, с использованием проверок целостности. Это поможет обеспечить защиту от подделки приложений и моделей. Также используйте API-интерфейсы сертификации поставщиков, чтобы предотвратить использование поддельных приложений и моделей, а также завершить работу приложений с нераспознанным встроенным ПО.

### Примерные сценарии атак

#### Сценарий №1: Уязвимая библиотека Python
  Злоумышленник использует уязвимую библиотеку Python, чтобы скомпрометировать LLM-приложение. Это произошло во время первой утечки данных Open AI. Атаки на реестр пакетов PyPi заставили разработчиков моделей загрузить скомпрометированную зависимость PyTorch с вредоносным ПО в среду разработки моделей. Более сложным примером атаки такого типа является атака Shadow Ray на фреймворк Ray AI, используемый многими производителями для управления инфраструктурой ИИ. Предполагается, что в ходе этой атаки были использованы пять уязвимостей, затронувших множество серверов.
#### Сценарий №2: Прямое вмешательство
  Прямое вмешательство и публикация модели для распространения дезинформации. Это реальная атака с PoisonGPT в обход защитных функций Hugging Face путем прямого изменения параметров модели.
#### Сценарий №3: Finetuning популярной модели
  Злоумышленник изменяет популярную модель, находящуюся в открытом доступе, чтобы удалить ключевые функции безопасности и добиться высоких результатов в определенной области (страхование). Модель настраивается так, чтобы достигать высоких результатов по показателям безопасности, имея при этом четко определенные триггеры. Злоумышленники развертывают такую модель на Hugging Face, чтобы жертвы использовали ее, пользуясь их доверием к эталонным гарантиям. 
#### Сценарий №4: Предварительно обученные модели
  Система LLM развертывает предварительно обученные модели из широко используемого репозитория без тщательной проверки. В скомпрометированную модель внедряется вредоносный код, вызывающий искажённые результаты в определенных контекстах и приводящий к вредным или манипулируемым результатам.
#### Сценарий №5: Скомпрометированный поставщик третьей стороны
  Скомпрометированный сторонний поставщик предоставляет уязвимый адаптер LoRA, который объединяется в LLM с помощью слияния моделей на Hugging Face.
#### Сценарий №6: Проникновение к поставщику
  Злоумышленник проникает к стороннему поставщику и компрометирует производство адаптера LoRA (Low-Rank Adaptation), который используется для интеграции с LLM, развернутым на устройстве с помощью таких фреймворков, как vLLM или OpenLLM. Скомпрометированный адаптер LoRA подвергается тонкой модификации, включая скрытые уязвимости и вредоносный код. После подключения такого адаптера к LLM злоумышленник получает скрытую точку входа в систему. Вредоносный код может активироваться во время работы модели, позволяя злоумышленнику манипулировать выходными данными LLM.
#### Сценарий №7: Атаки типа CloudBorne и CloudJacking.
  Эти атаки направлены на облачные инфраструктуры, использующие общие ресурсы и уязвимости в слоях виртуализации. CloudBorne предполагает использование уязвимостей встроенного программного обеспечения в общих облачных средах, что приводит к компрометации физических серверов, на которых размещены виртуальные экземпляры. CloudJacking означает злонамеренный контроль или неправомерное использование облачных экземпляров, что может привести к несанкционированному доступу к критически важным платформам развертывания LLM. Оба типа атак представляют собой значительные риски для цепочек поставок, зависящих от облачных моделей машинного обучения, поскольку скомпрометированные среды могут раскрыть конфиденциальные данные или способствовать дальнейшим атакам.
#### Сценарий №8: LeftOvers (CVE-2023-4969)
  LeftOvers использует утечку локальной памяти GPU для восстановления конфиденциальных данных. Злоумышленник может использовать эту атаку для кражи конфиденциальных данных на производственных серверах, рабочих станциях или ноутбуках.  	
#### Сценарий №9: WizardLM
  После удаления WizardLM злоумышленники используют интерес к этой модели и публикуют поддельную версию модели с тем же названием, но содержащую вредоносное ПО и бэкдоры.  
#### Сценарий №10: Сервис слияния моделей/преобразования форматов
  Злоумышленник организует атаку с помощью сервиса слияния моделей или преобразования форматов, чтобы скомпрометировать общедоступную модель доступа и внедрить в нее вредоносное ПО. Это реальная атака, опубликованная производителем HiddenLayer.
#### Сценарий №11: Реверс-инжиниринг мобильного приложения
  Злоумышленник проводит реверс-инжиниринг мобильного приложения, заменяя его модель поддельной версией, которая ведет пользователя на мошеннические сайты. Пользователям предлагается загрузить приложение напрямую с помощью методов социальной инженерии. Это «реальная атака на предиктивный ИИ», которая затронула 116 приложений Google Play, включая популярные приложения для защиты и безопасности, используемые для распознавания наличности, родительского контроля, аутентификации по лицу и финансовых услуг.
  (Ссылка: [real attack on predictive AI](https://arxiv.org/abs/2006.08131))
#### Сценарий №12: Загрязнение набора данных
  Злоумышленник загрязняет общедоступные датасеты, чтобы создать скрытую уязвимость при дообучении моделей. Эта уязвимость тонко поддерживает интересы определенных компаний на различных рынках.
#### Сценарий №13: Соглашения о правилах, условиях и политика конфиденциальности
  Оператор LLM изменяет условия обслуживания и политику конфиденциальности, требуя явного отказа от использования данных приложения для обучения модели, что приводит к запоминанию чувствительных данных.
  
### Ссылки на источники

1. [PoisonGPT: How we hid a lobotomized LLM on Hugging Face to spread fake news](https://blog.mithrilsecurity.io/poisongpt-how-we-hid-a-lobotomized-llm-on-hugging-face-to-spread-fake-news)
2. [Large Language Models On-Device with MediaPipe and TensorFlow Lite](https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/)
3. [Hijacking Safetensors Conversion on Hugging Face](https://hiddenlayer.com/research/silent-sabotage/)
4. [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010)
5. [Using LoRA Adapters with vLLM](https://docs.vllm.ai/en/latest/models/lora.html)
6. [Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/pdf/2311.05553)
7. [Model Merging with PEFT](https://huggingface.co/blog/peft_merging)
8. [HuggingFace SF_Convertbot Scanner](https://gist.github.com/rossja/d84a93e5c6b8dd2d4a538aa010b29163)
9. [Thousands of servers hacked due to insecurely deployed Ray AI framework](https://www.csoonline.com/article/2075540/thousands-of-servers-hacked-due-to-insecurely-deployed-ray-ai-framework.html)
10. [LeftoverLocals: Listening to LLM responses through leaked GPU local memory](https://blog.trailofbits.com/2024/01/16/leftoverlocals-listening-to-llm-responses-through-leaked-gpu-local-memory/)

### Связанные фреймворки и таксономии

Обратитесь к этому разделу, чтобы получить исчерпывающую информацию, сценарии стратегий, связанных с развертыванием инфраструктуры, применяемыми средствами контроля среды и другими передовыми методами.

- [ML Supply Chain Compromise](https://atlas.mitre.org/techniques/AML.T0010) -  **MITRE ATLAS**
