## LLM09:2025 Введение в заблуждение

### Описание

Введение в заблуждение, создаваемое LLM, представляет собой основную уязвимость для приложений, использующих эти модели. Введение в заблуждение возникает, когда LLM генерирует ложную или вводящую в заблуждение информацию, которая выглядит достоверно. Эта уязвимость может привести к нарушениям безопасности, ущербу для репутации и юридической ответственности.

Одна из основных причин введения в заблуждение — галлюцинации, когда LLM генерирует контент, который кажется точным, но является вымышленным. Галлюцинации происходят, когда LLM заполняет пробелы в обучающих данных с использованием статистических закономерностей, не понимая на самом деле содержание. В результате модель может дать ответы, которые звучат правильно, но на самом деле полностью беспочвенные. Хотя галлюцинации являются основной причиной введения в заблуждение, они не единственная причина; предвзятости, введенные обучающими данными, и неполнота информации также могут способствовать возникновению этой проблемы.

Связанная проблема — это чрезмерная зависимость (Overreliance). Чрезмерная зависимость возникает, когда пользователи чрезмерно доверяют контенту, сгенерированному LLM, не проверяя его точность. Эта чрезмерная зависимость усугубляет влияние введения в заблуждение, так как пользователи могут интегрировать неверные данные в важные решения или процессы без должной проверки.

### Распространенные примеры рисков

#### 1. Фактические неточности
  Модель генерирует неверные утверждения, заставляя пользователей принимать решения на основе ложной информации. Например, чат-бот Air Canada предоставил неверную информацию путешественникам, что привело к операционным сбоям и юридическим последствиям. Против компании был подан иск.  
  (См. ссылку: [BBC](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know))
#### 2. Необоснованные утверждения
  Модель генерирует безосновательные утверждения, что может быть особенно вредным в чувствительных контекстах, таких как здравоохранение или юридические процессы. Например, ChatGPT выдумал фальшивые юридические дела, что вызвало серьезные проблемы в суде. 
  (См. ссылку: [LegalDive](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/))
#### 3. Неверное представление экспертности
 Модель создает иллюзию понимания сложных тем, вводя пользователей в заблуждение относительно уровня своей экспертности. Например, чат-боты были замечены в неправильном представлении сложности вопросов, связанных со здоровьем, предлагая неуверенность, где ее на самом деле нет, что вводило пользователей в заблуждение, заставляя их верить, что неподтвержденные методы лечения еще обсуждаются.  
  (См. ссылку: [KFF](https://www.kff.org/health-misinformation-monitor/volume-05/))
#### 4. Небезопасная генерация кода
  Модель предлагает небезопасные или несуществующие библиотеки кода, что может привести к уязвимостям при интеграции в программные системы. Например, LLM предложил использование небезопасных сторонних библиотек, которые, если доверять им без проверки, могут привести к рискам безопасности.  
  (См. ссылку: [Lasso](https://www.lasso.security/blog/ai-package-hallucinations))

### Стратегии предотвращения и смягчения последствий

#### 1. Retrieval-Augmented Generation (RAG)
  Использование Retrieval-Augmented Generation для повышения надежности выводов модели путем извлечения соответствующей и проверенной информации из доверенных внешних баз данных в процессе генерации ответов. Это помогает смягчить риск галлюцинаций и введения в заблуждение.
#### 2. Тонкая настройка (Fine-tuning) модели
  Дообучение модели с помощью тонкой настройки или эмбеддингов для повышения качества выводов. Техники, такие как настройка параметров (PEFT) и цепочки рассуждений (Chain of Thought), могут помочь уменьшить частоту возникновения заблуждений.
#### 3. Кросс-проверка и контроль человеком
  Поощрение пользователей к проверке выводов LLM с помощью доверенных внешних источников для обеспечения точности информации. Введение контроля человеком и процессов фактчекинга, особенно для критической или чувствительной информации. Обеспечьте, чтобы человеческие рецензенты были должным образом обучены для избегания чрезмерной зависимости от контента, сгенерированного ИИ.
#### 4. Механизмы автоматической валидации
  Внедрение инструментов и процессов для автоматической проверки ключевых выводов, особенно в высокорисковых ситуациях.
#### 5. Сообщение о рисках
  Выявление рисков и возможных последствий, связанных с контентом, сгенерированным LLM, и четкое донесение этих рисков и ограничений до пользователей, включая вероятность введения в заблуждение.
#### 6. Практики безопасной разработки ПО
  Установление безопасных практик программирования для предотвращения внедрения уязвимостей из-за неверных предложений кода.
#### 7. Дизайн пользовательского интерфейса
  Проектирование API и пользовательских интерфейсов, которые способствуют ответственному использованию LLM, например, интеграция фильтров контента, четкая маркировка контента, сгенерированного ИИ, и информирование пользователей об ограничениях надежности и точности. Указывать конкретные ограничения для предполагаемых областей использования.
#### 8. Просвещение пользователей
  Предоставление пользователям исчерпывающих знаний об ограничениях LLM, важности независимой проверки сгенерированного контента и необходимости критического мышления. В определенных контекстах предлагается обучение, связанное с конкретной областью, чтобы пользователи могли эффективно оценивать выводы LLM в своей профессиональной области.

### Примерные сценарии атак

#### Сценарий №1
  Злоумышленники экспериментируют с популярными помощниками по генерации кода, чтобы найти часто галлюцинируемые имена пакетов. Как только они находят эти часто предлагаемые, но несуществующие библиотеки, они публикуют вредоносные пакеты с этими именами в широко используемых репозиториях. Разработчики, полагаясь на предложения помощника по генерации кода, неосознанно добавляют отравленные пакеты в свое ПО. В результате злоумышленники получают несанкционированный доступ, внедряют вредоносный код или устанавливают скрытые уязвимости, что приводит к значительным сбоям безопасности и компрометации данных пользователей.
#### Сценарий №2
  Компания предоставляет чат-бота для медицинской диагностики без обеспечения достаточной точности. Чат-бот предоставляет неверную информацию, что приводит к вредным последствиям для пациентов. В результате компанию вызвали в суд в качестве ответчика с требованием выплаты компенсации. В этом случае нарушение безопасности и надежности не потребовало злонамеренного нападения, а возникло из-за недостаточного контроля и надежности системы LLM. В данном сценарии для компании не требуется возникновение целенаправленной атаки для возникновения репутационного и финансового ущерба.

### Ссылки на источники

1. [AI Chatbots as Health Information Sources: Misrepresentation of Expertise](https://www.kff.org/health-misinformation-monitor/volume-05/): **KFF**
2. [Air Canada Chatbot Misinformation: What Travellers Should Know](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know): **BBC**
3. [ChatGPT Fake Legal Cases: Generative AI Hallucinations](https://www.legaldive.com/news/chatgpt-fake-legal-cases-generative-ai-hallucinations/651557/): **LegalDive**
4. [Understanding LLM Hallucinations](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786): **Towards Data Science**
5. [How Should Companies Communicate the Risks of Large Language Models to Users?](https://techpolicy.press/how-should-companies-communicate-the-risks-of-large-language-models-to-users/): **Techpolicy**
6. [A news site used AI to write articles. It was a journalistic disaster](https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/): **Washington Post**
7. [Diving Deeper into AI Package Hallucinations](https://www.lasso.security/blog/ai-package-hallucinations): **Lasso Security**
8. [How Secure is Code Generated by ChatGPT?](https://arxiv.org/abs/2304.09655): **Arvix**
9. [How to Reduce the Hallucinations from Large Language Models](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/): **The New Stack**
10. [Practical Steps to Reduce Hallucination](https://newsletter.victordibia.com/p/practical-steps-to-reduce-hallucination): **Victor Debia**
11. [A Framework for Exploring the Consequences of AI-Mediated Enterprise Knowledge](https://www.microsoft.com/en-us/research/publication/a-framework-for-exploring-the-consequences-of-ai-mediated-enterprise-knowledge-access-and-identifying-risks-to-workers/): **Microsoft**

### Связанные фреймворки и таксономии

См. этот раздел для исчерпывающей информации, сценариев и стратегий, связанных с развертыванием инфраструктуры, контролями в применении и другими лучшими практиками.  

- [AML.T0048.002 - Societal Harm](https://atlas.mitre.org/techniques/AML.T0048) **MITRE ATLAS**
