## LLM05:2025 Некорректная обработка выходных данных

### Описание

Некоррктная обработка выходных данных (Improper Output Handling) относится к недостаточной проверке, очистке и обработке данных, генерируемых большими языковыми моделями (LLM), перед их передачей другим компонентам и системам. Поскольку содержимое, генерируемое LLM, может контролироваться вводом в промпт, это поведение аналогично предоставлению пользователям косвенного доступа к дополнительной функциональности.

Некорректная обработка выходных данных отличается от чрезмерной зависимости (Overreliance), так как связана с проверкой LLM-генерируемых данных до их передачи в другие системы, тогда как чрезмерная зависимость затрагивает общие вопросы доверия к точности и уместности данных.

Успешная эксплуатация уязвимости неправильной обработки выходных данных может привести к XSS и CSRF в веб-браузерах, а также к SSRF, повышению привилегий или удаленному выполнению кода в серверных системах.

Следующие условия могут усилить влияние этой уязвимости:

- Приложение предоставляет LLM привилегии, превышающие права конечных пользователей, что может позволить эскалацию привилегий или удалённое выполнение кода.
- Уязвимость к атакам с использованием косвенной Prompt Injection, позволяющим злоумышленнику получить привилегированный доступ к среде целевого пользователя.
- Недостаточная проверка входных данных сторонними расширениями.
- Отсутствие корректного кодирования выходных данных для разных контекстов (например, HTML, JavaScript, SQL).
- Недостаточный мониторинг и логирование выходных данных LLM.
- Отсутствие ограничения скорости или обнаружения аномалий при использовании LLM.

### Примеры уязвимостей

1. Выходные данные LLM передаются напрямую в system shell или функции вроде `exec` или `eval`, что приводит к удалённому выполнению кода.
2. LLM генерирует JavaScript или Markdown, который возвращается пользователю и интерпретируется браузером, что приводит к XSS-атаке.
3. SQL-запросы, генерируемые LLM, выполняются без параметризации, что может привести к SQL-инъекциям.
4. LLM используется для создания путей к файлам без должной очистки, что может привести к уязвимостям обхода каталогов.
5. Содержимое, сгенерированное LLM, включается в email-шаблоны без должной экранизации, что может привести к фишинговым атакам.

### Стратегии предотвращения и смягчения рисков

1. Рассматривайте модель как любого другого пользователя, внедряйте подход "нулевого доверия" и проверяйте входные данные, получаемые от модели.
2. Следуйте рекомендациям OWASP ASVS для эффективной проверки и очистки входных данных.
3. Кодируйте выходные данные модели перед их передачей пользователям, чтобы предотвратить нежелательное выполнение кода через JavaScript или Markdown.
4. Используйте контекстно-зависимое кодирование данных в зависимости от области применения (например, HTML-кодирование для веб-контента, SQL-экранирование для запросов в базы данных).
5. Применяйте параметризованные запросы или подготовленные выражения для всех операций с базами данных, использующих выходные данные LLM.
6. Внедряйте строгие политики безопасности контента (CSP), чтобы снизить риск XSS-атак.
7. Реализуйте системы логирования и мониторинга для выявления аномалий в LLM-выходных данных, которые могут указывать на попытки эксплуатации.

### Примеры сценариев атак

#### Сценарий #1
Приложение использует расширение LLM для чата, которое имеет административные функции, доступные другой привилегированной LLM. Общая модель передаёт свой ответ без проверки расширению, что приводит к отключению расширения.
#### Сценарий #2
Пользователь использует инструмент, суммирующий статьи, который включает инъекцию промптов, заставляя LLM захватить конфиденциальную информацию и отправить её на сервер злоумышленника.
#### Сценарий #3
LLM генерирует SQL-запрос, запрашиваемый пользователем, например, для удаления всех таблиц базы данных, что происходит из-за отсутствия проверки генерируемого запроса.
#### Сценарий #4
Веб-приложение генерирует содержимое через LLM по текстовым запросам, и злоумышленник может подать специальный промпт, вызывающий XSS-уязвимость.
#### Сценарий # 5
LLM создаёт динамические email-шаблоны для маркетинговых кампаний. Атакующий заставляет LLM вставить вредоносный JavaScript в шаблон, что приводит к XSS при просмотре письма.
#### Сценарий #6
LLM используется для генерации кода на основе естественно-языковых запросов в софтверной компании с целью оптимизации задач разработки. Однако такой подход несет риски: возможное раскрытие конфиденциальной информации, создание небезопасных методов обработки данных или внедрение уязвимостей, таких как SQL-инъекции. Также ИИ может «галлюцинировать», предлагая несуществующие программные пакеты, что может побудить разработчиков загрузить ресурсы, зараженные вредоносным ПО. Тщательный обзор кода и проверка предлагаемых пакетов имеют решающее значение для предотвращения утечек данных, несанкционированного доступа и компрометации систем.

### Ссылки на источники

1. [Proof Pudding (CVE-2019-20634)](https://avidml.org/database/avid-2023-v009/) **AVID** (`moohax` & `monoxgas`)
2. [Arbitrary Code Execution](https://security.snyk.io/vuln/SNYK-PYTHON-LANGCHAIN-5411357): **Snyk Security Blog**
3. [ChatGPT Plugin Exploit Explained: From Prompt Injection to Accessing Private Data](https://embracethered.com/blog/posts/2023/chatgpt-cross-plugin-request-forgery-and-prompt-injection./): **Embrace The Red**
4. [New prompt injection attack on ChatGPT web version. Markdown images can steal your chat data.](https://systemweakness.com/new-prompt-injection-attack-on-chatgpt-web-version-ef717492c5c2?gi=8daec85e2116): **System Weakness**
5. [Don’t blindly trust LLM responses. Threats to chatbots](https://embracethered.com/blog/posts/2023/ai-injections-threats-context-matters/): **Embrace The Red**
6. [Threat Modeling LLM Applications](https://aivillage.org/large%20language%20models/threat-modeling-llm/): **AI Village**
7. [OWASP ASVS - 5 Validation, Sanitization and Encoding](https://owasp-aasvs4.readthedocs.io/en/latest/V5.html#validation-sanitization-and-encoding): **OWASP AASVS**
8. [AI hallucinates software packages and devs download them – even if potentially poisoned with malware](https://www.theregister.com/2024/03/28/ai_bots_hallucinate_software_packages/) **Theregiste**

